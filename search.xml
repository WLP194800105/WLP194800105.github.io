<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Kafka生产者API示例</title>
      <link href="/2022/06/18/Kafka%E7%94%9F%E4%BA%A7%E8%80%85API%E7%A4%BA%E4%BE%8B/"/>
      <url>/2022/06/18/Kafka%E7%94%9F%E4%BA%A7%E8%80%85API%E7%A4%BA%E4%BE%8B/</url>
      
        <content type="html"><![CDATA[<h4 id="Kafka生产者API示例"><a href="#Kafka生产者API示例" class="headerlink" title="Kafka生产者API示例"></a>Kafka生产者API示例</h4><h4 id="1、Kafka生产者API示例"><a href="#1、Kafka生产者API示例" class="headerlink" title="1、Kafka生产者API示例"></a>1、Kafka生产者API示例</h4><h5 id="1-1-生产者API示例"><a href="#1-1-生产者API示例" class="headerlink" title="1.1 生产者API示例"></a>1.1 生产者API示例</h5><p>一个正常的生产逻辑需要具备以下几个步骤</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(1)配置生产者客户端参数及创建相应的生产者实例</span><br><span class="line">(2)构建待发送的消息</span><br><span class="line">(3)发送消息</span><br><span class="line">(4)关闭生产者实例</span><br></pre></td></tr></table></figure><span id="more"></span><ul><li>示例代码（部分截取）</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">Properties</span> <span class="variable">props</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>(); </span><br><span class="line"></span><br><span class="line"><span class="comment">//设置 kafka 集群的地址</span></span><br><span class="line">props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;node1:9092,node2:9092,node3:9092&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//ack 模式,取值有 0,1,-1(all) , all 是最慢但最安全的，</span></span><br><span class="line">props.put(“acks”, “all”); </span><br><span class="line"></span><br><span class="line"><span class="comment">//失败重试次数-&gt;失败会自动重试（可恢复/不可恢复）--&gt;(有可能会造成数据的乱序)</span></span><br><span class="line">props.put(“retries”, <span class="number">3</span>); </span><br><span class="line"></span><br><span class="line"><span class="comment">//数据发送的批次大小提高效率/吞吐量太大会数据延迟</span></span><br><span class="line">props.put(“batch.size”, <span class="number">10</span>); </span><br><span class="line"></span><br><span class="line"><span class="comment">//消息在缓冲区保留的时间,超过设置的值就会被提交到服务端</span></span><br><span class="line">props.put(<span class="string">&quot;linger.ms&quot;</span>, <span class="number">10000</span>); </span><br><span class="line"></span><br><span class="line"><span class="comment">//数据发送请求的最大缓存数</span></span><br><span class="line">props.put(<span class="string">&quot;max.request.size&quot;</span>,<span class="number">10</span>); </span><br><span class="line"></span><br><span class="line"><span class="comment">//整个 Producer 用到总内存的大小,如果缓冲区满了会提交数据到服务端 buffer.memory 要大于 batch.size,否则会报申请内存不足的错误降低阻塞的可能性</span></span><br><span class="line">props.put(“buffer.memory”, <span class="number">10240</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//key-value序列化器</span></span><br><span class="line">props.put(<span class="string">&quot;key.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>); </span><br><span class="line"></span><br><span class="line"><span class="comment">//字符串最好</span></span><br><span class="line">props.put(“value.serializer”, “org.apache.kafka.common.serialization.StringSerializer”);</span><br></pre></td></tr></table></figure><ul><li>消息对象 ProducerRecord,它并不是单纯意义上的消息,它包含了多个属性,原本需要发送的与业务关的消息体只是其中的一个 value 属性 ,比“ Hello, rgzn!”只是 ProducerRecord 对象的一个属性。</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">ProducerRecord 类的定义如下:</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ProducerRecord</span>&lt;K, V&gt; &#123; </span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> String topic; </span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> Integer partition;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> Headers headers; </span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> K key; </span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> V value; </span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> Long timestamp;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="1-2-必要参数配置"><a href="#1-2-必要参数配置" class="headerlink" title="1.2 必要参数配置"></a>1.2 必要参数配置</h5><ul><li>在创建真正的生产者实例前需要配置相应的参数,比如需要连接的 Kafka 集群地址。在 Kafka 生产者客户端 KatkaProducer 中有 3 个参数是必填的。</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">* bootstrap.servers </span><br><span class="line">* key.serializer </span><br><span class="line">* value.serializer</span><br></pre></td></tr></table></figure><ul><li>为了防止参数名字符串书写错误,可以使用如下方式进行设置:</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">props.setProperty(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG,ProducerInterceptorPrefix.class.getName());</span><br><span class="line">props.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,<span class="string">&quot;node1:9092,node2:9092&quot;</span>); </span><br><span class="line">props.setProperty(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,StringSerializer.class.getName()); props.setProperty(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,StringSerializer.class.getName());</span><br></pre></td></tr></table></figure><h5 id="1-3-生产者API参数发送方式"><a href="#1-3-生产者API参数发送方式" class="headerlink" title="1.3  生产者API参数发送方式"></a>1.3  生产者API参数发送方式</h5><p>这个客户端经过了生产环境测试并且通常情况它比原来Scals客户端更加快速、功能更加齐全。你可以通过添加以下示例的Maven坐标到客户端依赖中来使用这个新的客户端（你可以修改版本号来使用新的发布版本）：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.kafka<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>kafka-clients<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>0.10.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h6 id="1-3-1-发后即忘-fire-and-forget"><a href="#1-3-1-发后即忘-fire-and-forget" class="headerlink" title="1.3.1  发后即忘( fire-and-forget)"></a>1.3.1  发后即忘( fire-and-forget)</h6><p>发后即忘,它只管往 Kafka 发送,并不关心消息是否正确到达。在大多数情况下,这种发送方式没有问题; 不过在某些时候(比如发生不可重试异常时)会造成消息的丢失。这种发送方式的性能最高,可靠性最差。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Future&lt;RecordMetadata&gt; send = producer.send(rcd);</span><br></pre></td></tr></table></figure><h6 id="1-3-2-同步发送-sync"><a href="#1-3-2-同步发送-sync" class="headerlink" title="1.3.2  同步发送(sync )"></a>1.3.2  同步发送(sync )</h6><p>0.8.x 前,有一个参数 producer.type&#x3D;sycn|asycn 来决定生产者的发送模式;现已失效(新版中,producer 在底层只有异步)</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">try &#123;</span><br><span class="line">producer.send(rcd).get(); </span><br><span class="line">&#125; catch (Exception e) &#123; </span><br><span class="line">e.printStackTrace();</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><p>在调用 <code>send</code> 方法后可以接着调用 <code>get()</code> 方法，<code>send</code> 方法的返回值是一个 Future\对象，RecordMetadata 里面包含了发送消息的主题、分区、偏移量等信息。改写后的代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        ProducerRecord&lt;String, String&gt; record = <span class="keyword">new</span> <span class="title class_">ProducerRecord</span>&lt;&gt;(topicName, <span class="string">&quot;k&quot;</span> + i, <span class="string">&quot;world&quot;</span> + i);</span><br><span class="line">        <span class="comment">/*同步发送消息*/</span></span><br><span class="line">        <span class="type">RecordMetadata</span> <span class="variable">metadata</span> <span class="operator">=</span> producer.send(record).get();</span><br><span class="line">        System.out.printf(<span class="string">&quot;topic=%s, partition=%d, offset=%s \n&quot;</span>,</span><br><span class="line">                metadata.topic(), metadata.partition(), metadata.offset());</span><br><span class="line">    &#125; <span class="keyword">catch</span> (InterruptedException | ExecutionException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>此时得到的输出如下：偏移量和调用次数有关，所有记录都分配到了 0 分区，这是因为在创建 <code>Hello-Kafka</code> 主题时候，使用 <code>--partitions</code> 指定其分区数为 1，即只有一个分区。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">topic=Hello-Kafka, partition=0, offset=40 </span><br><span class="line">topic=Hello-Kafka, partition=0, offset=41 </span><br><span class="line">topic=Hello-Kafka, partition=0, offset=42 </span><br><span class="line">topic=Hello-Kafka, partition=0, offset=43 </span><br><span class="line">topic=Hello-Kafka, partition=0, offset=44 </span><br><span class="line">topic=Hello-Kafka, partition=0, offset=45 </span><br><span class="line">topic=Hello-Kafka, partition=0, offset=46 </span><br><span class="line">topic=Hello-Kafka, partition=0, offset=47 </span><br><span class="line">topic=Hello-Kafka, partition=0, offset=48 </span><br><span class="line">topic=Hello-Kafka, partition=0, offset=49</span><br><span class="line"></span><br></pre></td></tr></table></figure><h6 id="1-3-3-异步发送-async"><a href="#1-3-3-异步发送-async" class="headerlink" title="1.3.3 异步发送(async )"></a>1.3.3 异步发送(async )</h6><p>回调函数会在 producer 收到 ack 时调用,为异步调用,该方法有两个参数,分别是 RecordMetadata 和Exception,如果 Exception 为 null,说明消息发送成功,如果 Exception 不为 null,说明消息发送失败。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">注意:消息发送失败会自动重试,不需要我们在回调函数中手动重试</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>通常我们并不关心发送成功的情况，更多关注的是失败的情况，因此 Kafka 提供了异步发送和回调函数。 代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line">    ProducerRecord&lt;String, String&gt; record = <span class="keyword">new</span> <span class="title class_">ProducerRecord</span>&lt;&gt;(topicName, <span class="string">&quot;k&quot;</span> + i, <span class="string">&quot;world&quot;</span> + i);</span><br><span class="line">    <span class="comment">/*异步发送消息，并监听回调*/</span></span><br><span class="line">    producer.send(record, <span class="keyword">new</span> <span class="title class_">Callback</span>() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onCompletion</span><span class="params">(RecordMetadata metadata, Exception exception)</span> &#123;</span><br><span class="line">            <span class="keyword">if</span> (exception != <span class="literal">null</span>) &#123;</span><br><span class="line">                System.out.println(<span class="string">&quot;进行异常处理&quot;</span>);</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                System.out.printf(<span class="string">&quot;topic=%s, partition=%d, offset=%s \n&quot;</span>,</span><br><span class="line">                        metadata.topic(), metadata.partition(), metadata.offset());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h5 id="1-4-生产者原理解析"><a href="#1-4-生产者原理解析" class="headerlink" title="1.4 生产者原理解析"></a>1.4 生产者原理解析</h5><p><img src="/.%5CKafka%E7%94%9F%E4%BA%A7%E8%80%85API%E7%A4%BA%E4%BE%8B%5C1654751205374.png" class="lazyload" data-srcset="/.%5CKafka%E7%94%9F%E4%BA%A7%E8%80%85API%E7%A4%BA%E4%BE%8B%5C1654751205374.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw=="></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">0 新建kafka生产实例，参数也是放在kafkaProducer里面</span><br><span class="line"></span><br><span class="line">1 Producerinterceptor拦截器，设置特定的规则对消息进行拦截，可以通过指定的消息</span><br><span class="line"></span><br><span class="line">2 Serializer序列化器，创建生产者对象时必须指定序列化器，作用就是将key和value转换为二进制</span><br><span class="line"></span><br><span class="line">3 Partitioner，topic中有分区，如何分发就是通过此处有规划的分发数据</span><br><span class="line"></span><br><span class="line">4.1 RecordAccumulator消息累加器，其中有多个分区，对于每个分区，都会单独维护主要用来缓存消息以便 Sender 线程可以批量发送, 进而减少网络传输的资源消耗以提升性能。</span><br><span class="line"></span><br><span class="line">4.2 RecordAccumulator 缓存的大小可以通过生产者客户端参数 buffer.memory 配 置, 默认值为 33554432B ,即 32M。</span><br><span class="line"></span><br><span class="line">4.3 如果生产者发送消息的速度超过发送到服务器的速度,则会导致生产者空间不足,这个时KafkaProducer.send()方法调用要么被阻塞,要么抛出异常,这个取决于参数max.block.ms 的配置,此参数的默认值为 60000,即 60 秒。（此配置可以理解为阻塞时间，在这个范围内不会抛出异常）</span><br><span class="line"></span><br><span class="line">4.4 主线程中发送过来的消息都会被迫加到 RecordAccumulator 的某个双端队列(Deque )中,RecordAccumulator 内部为每个分区都维护了一个双端队列,即Deque&lt;ProducerBatch&gt;。消息写入缓存时,追加到双端队列的尾部;</span><br><span class="line"></span><br><span class="line">4.5 Sender 读取消息时,从双端队列的头部读取。</span><br><span class="line"></span><br><span class="line">4.6 注意:ProducerBatch 是指一个消息批次; 与此同时,会将较小的 ProducerBatch凑成一个较ProducerBatch ,也可以减少网络请求的次数以提升整体的吞吐量。</span><br><span class="line"></span><br><span class="line">4.7 ProducerBatch 大小和 batch.size 参数也有着密切的关系。</span><br><span class="line"></span><br><span class="line">4.8 当一条消息(ProducerRecord ) 流入RecordAccumulator 时,会先寻找与消息分区所对应的双端队列(如果没有则新建),再从这个双端队列的尾部获取一个ProducerBatch (如果没有则新建),查看 ProducerBatch 中是否还可以写入这个ProducerRecord,如果可以写入,如果不可以则需要创建一个新的 Producer Batch。</span><br><span class="line"></span><br><span class="line">4.9 在新建ProducerBatch 时评估这条消息的大小是否超过 batch.size 参数大小, 如果不超过, 那么就以 batch.size 参数的大小来创建 ProducerBatch。</span><br><span class="line"></span><br><span class="line">4.10 如果生产者客户端需要向很多分区发送消息, 则可以将 buffer.memory 参数适当调大以增加整体的吞吐量</span><br><span class="line">6.1 Sender 从 RecordAccumulator 获取缓存的消息之后,会进一步将&lt;分区,Deque&lt;Producer Batch&gt;&gt;的形式转变成&lt;Node,List&lt;ProducerBatch&gt;的形式,其中 Node 表示 Kafka 集群 broker 节点。</span><br><span class="line"></span><br><span class="line">6.2 对于网络连接来说,生产者客户端是与具体 broker 节点建立的连接,也就是向具体的 broker 节点发送消息,而并不关心消息属于哪一个分区;</span><br><span class="line"></span><br><span class="line">6.3 而对于 KafkaProducer 的应用逻辑而言,我们只关注向哪个分区中发送哪些消息,所以在这里需要做一个应用逻辑层面到网络 I/O层面的转换。</span><br><span class="line"></span><br><span class="line">6.4 在转换成&lt;Node, List&lt;ProducerBatch&gt;&gt;的形式之后, Sender 会进一步封装成&lt;Node,Request&gt; 的形式, 这样就可以将 Request 请求发往各个 Node 了,这里的 Request 是 Kafka 各种协议请求;</span><br><span class="line"></span><br><span class="line">6.5 下一步直接就可以从Request 发送到 Selector 在转到 kafka集群</span><br><span class="line"></span><br><span class="line">7.1 缓存操作可以理解为当请求 从 sender 发送给 kafka 集群时候，sender 是不知道是否成功发送，即kafka 是否接收到消息，所以此功能是，当sender只要给 kafka 发送请求，此消息就同步InFlightRequests</span><br><span class="line"></span><br><span class="line">7.2 请求在从 sender 线程发往 Kafka 之前还会保存到InFlightRequests 中,InFlightRequests 保存对象的具体形式为Map&lt;Nodeld, Deque&lt;request&gt;&gt;,它的主要作用是缓存了已经发出去但还没有收到服务端响应的请求(Nodeld 是一个 String 类型,表示节点的 id 编号)。</span><br><span class="line"></span><br><span class="line">7.3 与此同时,InFlightRequests 还提供了许多管理类的方法,并且通过配置参数还可以限制每个连接(也就是客户端与 Node 之间的连接) 最多缓存的请求数。</span><br><span class="line"></span><br><span class="line">7.4 这个配置参数为 max.in.flight.request.per. connection ,默认值为 5,即每个连接最多只能缓存 5 个未响应的请求,超过该数值之后就不能再向这个连接发送更多的请求了,除非有缓存的请求收到了响应</span><br><span class="line">( Response )。</span><br><span class="line"></span><br><span class="line">8 提交到selector 准备发送</span><br><span class="line"></span><br><span class="line">9 发送到 kafka集群</span><br><span class="line"></span><br><span class="line">10 当 kafka 集群受到消息 ，集群响应，返回给selector</span><br><span class="line"></span><br><span class="line">11.1 selector 回复给 InFlightRequests</span><br><span class="line"></span><br><span class="line">11.2 如果没有受到响应， request 则会在 InFlightRequests 一直缓存</span><br><span class="line">11.3 通过比较 Deque&lt;Request&gt; 的 size 与这个参数的大小来判断对应的 Node中是否己经堆积了很多未响应的消息, 如果真是如此, 那么说明这个 Node 节点负载较大或网络连接有问题,再继其发送请求会增大请求超时的可能。</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="2、消费者API"><a href="#2、消费者API" class="headerlink" title="2、消费者API"></a>2、消费者API</h4><p>一个正常的消费逻辑需要具备以下几个步骤: </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">(1)配置消费者客户端参数</span><br><span class="line">(2)创建相应的消费者实例; </span><br><span class="line">(3)订阅主题; </span><br><span class="line">(4)拉取消息并消费; </span><br><span class="line">(5)提交消费位移 offset;</span><br><span class="line">(6)关闭消费者实例。</span><br><span class="line"></span><br></pre></td></tr></table></figure><ul><li>消费者API示例代码（部分截取）</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">Properties</span> <span class="variable">props</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>(); </span><br><span class="line"></span><br><span class="line"><span class="comment">// 定义 kakfa 服务的地址,不需要将所有 broker 指定上</span></span><br><span class="line">props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;node1:9092&quot;</span>); </span><br><span class="line"></span><br><span class="line"><span class="comment">// 指定 consumer group </span></span><br><span class="line">props.put(<span class="string">&quot;group.id&quot;</span>, <span class="string">&quot;g1&quot;</span>); </span><br><span class="line"></span><br><span class="line"><span class="comment">// 是否自动提交 offset </span></span><br><span class="line">props.put(<span class="string">&quot;enable.auto.commit&quot;</span>, <span class="string">&quot;true&quot;</span>); </span><br><span class="line"></span><br><span class="line"><span class="comment">// 自动提交 offset 的时间间隔</span></span><br><span class="line">props.put(<span class="string">&quot;auto.commit.interval.ms&quot;</span>, <span class="string">&quot;1000&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// key 的反序列化类</span></span><br><span class="line">props.put(<span class="string">&quot;key.deserializer&quot;</span>,<span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>); </span><br><span class="line"></span><br><span class="line"><span class="comment">// value 的反序列化类</span></span><br><span class="line">props.put(<span class="string">&quot;value.deserializer&quot;</span>,<span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>); </span><br><span class="line"></span><br><span class="line"><span class="comment">// 如果没有消费偏移量记录,则自动重设为起始 offset:latest, earliest, none</span></span><br><span class="line"><span class="comment">//Earliest目前状态下最前面的一条消息（日志在一定保存时间后会自动清空）</span></span><br><span class="line"><span class="comment">//none（上次记录的偏移量，如果没有，会抛异常） </span></span><br><span class="line">props.put(<span class="string">&quot;auto.offset.reset&quot;</span>,<span class="string">&quot;earliest&quot;</span>); </span><br><span class="line"></span><br><span class="line"><span class="comment">// 定义 consumer KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props); </span></span><br><span class="line"><span class="comment">// 消费者订阅的 topic, 可同时订阅多个</span></span><br><span class="line">consumer.subscribe(Arrays.asList(<span class="string">&quot;first&quot;</span>, <span class="string">&quot;test&quot;</span>,<span class="string">&quot;test1&quot;</span>)); </span><br><span class="line"></span><br></pre></td></tr></table></figure><h5 id="2-1-Kafka消费者可选属性"><a href="#2-1-Kafka消费者可选属性" class="headerlink" title="2.1 Kafka消费者可选属性"></a>2.1 Kafka消费者可选属性</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">1. fetch.min.byte</span><br><span class="line">消费者从服务器获取记录的最小字节数。如果可用的数据量小于设置值，broker 会等待有足够的可用数据时才会把它返回给消费者。</span><br><span class="line"></span><br><span class="line">2. fetch.max.wait.ms</span><br><span class="line">broker 返回给消费者数据的等待时间，默认是 500ms。</span><br><span class="line"></span><br><span class="line">3. max.partition.fetch.bytes</span><br><span class="line">该属性指定了服务器从每个分区返回给消费者的最大字节数，默认为 1MB。</span><br><span class="line"></span><br><span class="line">4. session.timeout.ms</span><br><span class="line">消费者在被认为死亡之前可以与服务器断开连接的时间，默认是 3s。</span><br><span class="line"></span><br><span class="line">5. auto.offset.reset</span><br><span class="line">该属性指定了消费者在读取一个没有偏移量的分区或者偏移量无效的情况下该作何处理：</span><br><span class="line"></span><br><span class="line">latest (默认值) ：在偏移量无效的情况下，消费者将从最新的记录开始读取数据（在消费者启动之后生成的最新记录）;</span><br><span class="line">earliest ：在偏移量无效的情况下，消费者将从起始位置读取分区的记录。</span><br><span class="line">6. enable.auto.commit</span><br><span class="line">是否自动提交偏移量，默认值是 true。为了避免出现重复消费和数据丢失，可以把它设置为 false。</span><br><span class="line"></span><br><span class="line">7. client.id</span><br><span class="line">客户端 id，服务器用来识别消息的来源。</span><br><span class="line"></span><br><span class="line">8. max.poll.records</span><br><span class="line">单次调用 poll() 方法能够返回的记录数量。</span><br><span class="line"></span><br><span class="line">9. receive.buffer.bytes &amp; send.buffer.byte</span><br><span class="line">这两个参数分别指定 TCP socket 接收和发送数据包缓冲区的大小，-1 代表使用操作系统的默认值。</span><br><span class="line"></span><br></pre></td></tr></table></figure><ul><li>必要参数配置</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">Properties</span> <span class="variable">props</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>(); props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG,StringDeserializer.class.getName());</span><br><span class="line"></span><br><span class="line">props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,StringDeserializer.class.getName());</span><br><span class="line"></span><br><span class="line">props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG,brokerList);</span><br><span class="line"></span><br><span class="line">props.put(ConsumerConfig.GROUP_ID_CONFIG,groupid);</span><br><span class="line"></span><br><span class="line">props.put(ConsumerConfig.CLIENT_ID_CONFIG,clientid);</span><br><span class="line"></span><br></pre></td></tr></table></figure><h5 id="2-2-subscribe-订阅主题"><a href="#2-2-subscribe-订阅主题" class="headerlink" title="2.2 subscribe 订阅主题"></a>2.2 subscribe 订阅主题</h5><ul><li><p>subscribe 有如下重载方法: </p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">subscribe</span><span class="params">(Collection&lt;String&gt; topics,ConsumerRebalanceListener listener)</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">subscribe</span><span class="params">(Collection&lt;String&gt; topics)</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">subscribe</span><span class="params">(Pattern pattern, ConsumerRebalanceListener listener)</span> </span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">subscribe</span><span class="params">(Pattern pattern)</span></span><br><span class="line"></span><br></pre></td></tr></table></figure></li><li><p>指定集合方式订阅主题</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">consumer.subscribe(Arrays.asList(topic1)); </span><br><span class="line">consumer <span class="title function_">subscribe</span><span class="params">(Arrays.asList(topic2)</span>);</span><br><span class="line"></span><br></pre></td></tr></table></figure></li><li><p>正则方式订阅主题</p><p>如果消费者采用的是正则表达式的方式(subscribe(Pattern))订阅, 在之后的过程中,如果有人又创建了新的主题,并且主题名字与正表达式相匹配,那么这个消费者就可以消费到新添加的主题中的消息。如果应用程序需要消费多个主题,并且可以处理不同的类型,那么这种订阅方式就很有效。</p><ul><li><p>正则表达式的方式订阅的示例如下</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">consumer.subscribe(Pattern.compile (<span class="string">&quot;topic.*&quot;</span> )); </span><br><span class="line"></span><br></pre></td></tr></table></figure></li></ul></li></ul><h5 id="2-3-assign-订阅主题"><a href="#2-3-assign-订阅主题" class="headerlink" title="2.3 assign 订阅主题"></a>2.3 assign 订阅主题</h5><p>消费者不仅可以通过 KafkaConsumer.subscribe() 方法订阅主题,还可直接订阅某些主题的指定分区; </p><ul><li><p>在 KafkaConsumer 中提供了 assign() 方法来实现这些功能,此方法的具体定义如下: </p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">assign</span><span class="params">(Collection&lt;TopicPartition&gt; partitions)</span>;</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">这个方法只接受参数 partitions,用来指定需要订阅的分区集合。</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">consumer.assign(Arrays.asList(<span class="keyword">new</span> <span class="title class_">TopicPartition</span> (<span class="string">&quot;tpc_1&quot;</span> , <span class="number">0</span>),<span class="keyword">new</span> <span class="title class_">TopicPartition</span>(“tpc_2”,<span class="number">1</span>))) ;</span><br><span class="line"></span><br></pre></td></tr></table></figure></li></ul><h5 id="2-4-subscribe-与-assign-的区别"><a href="#2-4-subscribe-与-assign-的区别" class="headerlink" title="2.4  subscribe 与 assign 的区别"></a>2.4  subscribe 与 assign 的区别</h5><ul><li><p>通过 subscribe()方法订阅主题具有消费者自动再均衡功能 ; </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">在多个消费者的情况下可以根据分区分配策略来自动分配各个消费者与分区的关系。 当消费组的消费者增加或减少时,分区分配关系会自动调整,以实现消费负载均衡及故障自动转移。</span><br><span class="line"></span><br></pre></td></tr></table></figure></li><li><p>assign() 方法订阅分区时,是不具备消费者自动均衡的功能的; </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">其实这一点从 assign()方法参数可以看出端倪,两种类型 subscribe()都有 ConsumerRebalanceListener 类型参数的方法,而 assign()方法却没有。</span><br><span class="line"></span><br></pre></td></tr></table></figure></li></ul><h5 id="2-5-取消订阅"><a href="#2-5-取消订阅" class="headerlink" title="2.5  取消订阅"></a>2.5  取消订阅</h5><p>可以使用 KafkaConsumer 中的 unsubscribe()方法采取消主题的订阅,这个方法既可以取消通过subscribe( Collection)方式实现的订阅; 也可以取消通过 subscribe(Pattem)方式实现的订阅,还可以取消通过 assign( Collection)方式实现的订阅。示例码如下: </p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">consumer.unsubscribe(); </span><br><span class="line"></span><br></pre></td></tr></table></figure><p>如果将 subscribe(Collection )或 assign(Collection)集合参数设置为空集合,作用与 unsubscribe()方法相同,如下示例中三行代码的效果相同: </p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">consumer.unsubscribe(); </span><br><span class="line">consumer.subscribe(<span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;String&gt;()) ; </span><br><span class="line">consumer.assign(<span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;TopicPartition&gt;());</span><br><span class="line"></span><br></pre></td></tr></table></figure><h5 id="2-6-消息的消费模式"><a href="#2-6-消息的消费模式" class="headerlink" title="2.6 消息的消费模式"></a>2.6 消息的消费模式</h5><p>Kafka 中的消费是基于拉取模式的。消息的消费一般有两种模式:推送模式和拉取模式。推模式是服务端主动将消息推送给消费者,而拉模式是消费者主动向服务端发起请求来拉取消息。</p><p>对于 poll () 方法而言,如果某些分区中没有可供消费的消息,那么此分区对应的消息拉取的结果就为空如果订阅的所有分区中都没有可供消费的消息,那么 poll()方法返回为空的消息集; poll () 方法具体定义如下:<br>public ConsumerRecords&lt;K, V&gt; poll(final Duration timeout)<br>超时时间参数 timeout , 用来控制 poll() 方法的阻塞时间, 在消费者的缓冲区里没有可用数据时会发生阻塞。如果消费者程序只用来单纯拉取并消费数据,则为了提高吞吐率,可以把 timeout 设置为Long.MAX_VALUE;</p><ul><li><p>消费者消费到的每条消息的类型为 ConsumerRecord</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ConsumerRecord</span>&lt;K, V&gt; &#123; </span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">long</span> <span class="variable">NO_TIMESTAMP</span> <span class="operator">=</span> RecordBatch.NO_TIMESTAMP; </span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">NULL_SIZE</span> <span class="operator">=</span> -<span class="number">1</span>; </span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">NULL_CHECKSUM</span> <span class="operator">=</span> -<span class="number">1</span>; </span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> String topic; </span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> <span class="type">int</span> partition; </span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> <span class="type">long</span> offset;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> <span class="type">long</span> timestamp; </span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> TimestampType timestampType; </span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> <span class="type">int</span> serializedKeySize; </span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> <span class="type">int</span> serializedValueSize; </span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> Headers headers; </span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> K key; </span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> V value; </span><br><span class="line"><span class="keyword">private</span> <span class="keyword">volatile</span> Long checksum;</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">topic partition 这两个字段分别代表消息所属主题的名称和所在分区的编号。</span><br><span class="line"></span><br><span class="line">offsset 表示消息在所属分区的偏移量。</span><br><span class="line"></span><br><span class="line">timestamp 表示时间戳,与此对应的 timestampType 表示时间戳的类型。</span><br><span class="line"></span><br><span class="line">timestampType 有两种类型 CreateTime 和 LogAppendTime , 分别代表消息创建的时间戳和消息追加到日志的时间戳。</span><br><span class="line"></span><br><span class="line">headers 表示消息的头部内容。</span><br><span class="line"></span><br><span class="line">key value 分别表示消息的键和消息的值,一般业务应用要读取的就是 value ; </span><br><span class="line"></span><br><span class="line">serializedKeySize、serializedValueSize 分别表示 key、value 经过序列化之后的大小,如果 key 为空, 则 serializedKeySize 值为 -1,同样,如果 value 为空,则 serializedValueSize 的值也会为 -1; </span><br><span class="line"></span><br><span class="line">checksum 是 CRC32 的校验值。</span><br><span class="line"></span><br></pre></td></tr></table></figure></li></ul><h5 id="2-7-指定位移消费"><a href="#2-7-指定位移消费" class="headerlink" title="2.7 指定位移消费"></a>2.7 指定位移消费</h5><p>有些时候,我们需要一种更细粒度的掌控,可以让我们从特定的位移处开始拉取消息,而KafkaConsumer 中的 seek() 方法正好提供了这个功能,让我们可以追前消费或回溯消费。</p><ul><li><p>seek()方法的具体定义如下: </p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">seek</span><span class="params">(TopicPartiton partition,<span class="type">long</span> offset)</span>;</span><br><span class="line"></span><br></pre></td></tr></table></figure></li></ul><h5 id="2-8-再均衡监听器"><a href="#2-8-再均衡监听器" class="headerlink" title="2.8 再均衡监听器"></a>2.8 再均衡监听器</h5><p>一个消费组中,一旦有消费者的增减发生,会触发消费者组的 rebalance 再均衡; 如果 A 消费者消费掉的一批消息还没来得及提交 offset, 而它所负责的分区在 rebalance 中转移给了 B 消费者,则有可能发生数据的重复消费处理。此情形下,可以通过再均衡监听器做一定程度的补救;</p><h5 id="2-9-自动位移提交"><a href="#2-9-自动位移提交" class="headerlink" title="2.9 自动位移提交"></a>2.9 自动位移提交</h5><p>Kafka 中默认的消费位移的提交方式是自动提交,这个由消费者客户端参数 enable.auto.commit 配置, 默认值为 true 。当然这个默认的自动提交不是每消费一条消息就提交一次,而是定期提交,这个定期的周期时间由客户端参数 auto.commit.interval.ms 配置, 默认值为 5 秒, 此参数生效的前提是 enable.</p><p>auto.commit 参数为 true。</p><p>在默认的方式下,消费者每隔 5 秒会将拉取到的每个分区中最大的消息位移进行提交。自动位移提交的动作是在 poll() 方法的逻辑里完成的,在每次真正向服务端发起拉取请求之前会检查是否可以进行位移提交,如果可以,那么就会提交上一次轮询的位移。</p><p>Kafka 消费的编程逻辑中位移提交是一大难点,自动提交消费位移的方式非常简便,它免去了复杂的位移提交逻辑,让编码更简洁。但随之而来的是重复消费和消息丢失的问题。</p><ul><li><p>重复消费</p><p>假设刚刚提交完一次消费位移,然后拉取一批消息进行消费,在下一次自动提交消费位移之前,消费者崩溃了,那么又得从上一次位移提交的地方重新开始消费,这样便发生了重复消费的现象(对于再均衡的情况同样适用)。我们可以通过减小位移提交的时间间隔来减小重复消息的窗口大小,但这样并不能避免重复消费的发送,而且也会使位移提交更加频繁。</p></li><li><p>丢失消息</p><p>按照一般思维逻辑而言,自动提交是延时提交,重复消费可以理解,那么消息丢失又是在什么情形下会发生的呢?我们来看下图中的情形: 拉取线程不断地拉取消息并存入本地缓存, 比如在 BlockingQueue 中, 另一个处理线程从缓存中读取消息并进行相应的逻辑处理。设目前进行到了第 y+l 次拉取,以及第 m 次位移提交的时候,也就是x+6 之前的位移己经确认提交了, 处理线程却还正在处理 x+3 的消息; 此时如果处理线程发生了异常, 待其恢复之后会从第 m 次位移提交处,也就是 x+6 的位置开始拉取消息,那么 x+3 至 x+6 之间的消息就没有得到相应的处理,这样便发生消息丢失的现象。</p></li></ul><h5 id="2-10-手动位移提交-调用-kafka-api"><a href="#2-10-手动位移提交-调用-kafka-api" class="headerlink" title="2.10 手动位移提交(调用 kafka api)"></a>2.10 手动位移提交(调用 kafka api)</h5><p>自动位移提交的方式在正常情况下不会发生消息丢失或重复消费的现象, 但是在编程的世界里异常无可避免; 同时, 自动位移提交也无法做到精确的位移管理。 在 Kafka 中还提供了手动位移提交的方式, 这样可以使得开发人员对消费位移的管理控制更加灵活。<br>很多时候并不是说拉取到消息就算消费完成,而是需要将消息写入数据库、写入本地缓存,或者是更加复杂的业务处理。在这些场景下,所有的业务处理完成才能认为消息被成功消费; 手动的提交方式可以让开发人员根据程序的逻辑在合适的地方进行位移提交。 开启手动提交功能的前提是消费者客户端参数 enable.auto.commit 配置为 fals ,示例如下:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">props.put(ConsumerConf.ENABLE_AUTO_COMMIT_CONFIG, <span class="literal">false</span>); </span><br><span class="line"></span><br></pre></td></tr></table></figure><p>手动提交可以细分为同步提交和异步提交,对应于 KafkaConsumer 中的 commitSync()和commitAsync()两种类型的方法。</p><h4 id="3、Topic管理-API"><a href="#3、Topic管理-API" class="headerlink" title="3、Topic管理 API"></a>3、Topic管理 API</h4><p>一般情况下,我们都习惯使用 kafka-topic.sh 本来管理主题,如果希望将管理类的功能集成到公司内部的系统中,打造集管理、监控、运维、告警为一体的生态平台,那么就需要以程序调用 API 方式去实现。这种调用 API 方式实现管理主要利用 KafkaAdminClient 工具类KafkaAdminClient 不仅可以用来管理 broker、配置和 ACL (Access Control List),还可用来管理主题)</p><h5 id="3-1-列出主题"><a href="#3-1-列出主题" class="headerlink" title="3.1 列出主题"></a>3.1 列出主题</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">ListTopicsResult</span> <span class="variable">listTopicsResult</span> <span class="operator">=</span> adminClient.listTopics(); </span><br><span class="line"></span><br><span class="line">Set&lt;String&gt; topics = listTopicsResult.names().get(); </span><br><span class="line"></span><br><span class="line">System.out.println(topics);</span><br><span class="line"></span><br></pre></td></tr></table></figure><h5 id="3-2-查看主题信息"><a href="#3-2-查看主题信息" class="headerlink" title="3.2 查看主题信息"></a>3.2 查看主题信息</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">DescribeTopicsResult</span> <span class="variable">describeTopicsResult</span> <span class="operator">=</span> adminClient.describeTopics(Arrays.asList(<span class="string">&quot;tpc_4&quot;</span>, <span class="string">&quot;tpc_3&quot;</span>)); </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Map&lt;String, TopicDescription&gt; res = describeTopicsResult.all().get();</span><br><span class="line"></span><br><span class="line">Set&lt;String&gt; ksets = res.keySet(); </span><br><span class="line"><span class="keyword">for</span> (String k : ksets) &#123; </span><br><span class="line">System.out.println(res.get(k)); </span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h5 id="3-3-创建主题"><a href="#3-3-创建主题" class="headerlink" title="3.3 创建主题"></a>3.3 创建主题</h5><ul><li>代码示例（部分截取）</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 参数配置</span></span><br><span class="line"><span class="type">Properties</span> <span class="variable">props</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>(); </span><br><span class="line">props.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG,<span class="string">&quot;node1:9092,node2:9092,node3:9092&quot;</span>);</span><br><span class="line">props.put(AdminClientConfig.REQUEST_TIMEOUT_MS_CONFIG,<span class="number">3000</span>); </span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建 admin client 对象</span></span><br><span class="line"><span class="type">AdminClient</span> <span class="variable">adminClient</span> <span class="operator">=</span> KafkaAdminClient.create(props); </span><br><span class="line"></span><br><span class="line"><span class="comment">// 由服务端 controller 自行分配分区及副本所在 broker </span></span><br><span class="line"><span class="type">NewTopic</span> <span class="variable">tpc_3</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">NewTopic</span>(<span class="string">&quot;tpc_3&quot;</span>, <span class="number">2</span>, (<span class="type">short</span>) <span class="number">1</span>); </span><br><span class="line"></span><br><span class="line"><span class="comment">// 手动指定分区及副本的 broker 分配</span></span><br><span class="line">HashMap&lt;Integer, List&lt;Integer&gt;&gt; replicaAssignments = <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;&gt;(); </span><br><span class="line"></span><br><span class="line"><span class="comment">// 分区 0,分配到 broker0,broker1 replicaAssignments.put(0,Arrays.asList(0,1)); </span></span><br><span class="line"><span class="comment">// 分区 1,分配到 broker0,broker2 </span></span><br><span class="line">replicaAssignments.put(<span class="number">0</span>,Arrays.asList(<span class="number">0</span>,<span class="number">1</span>));</span><br><span class="line"><span class="type">NewTopic</span> <span class="variable">tpc_4</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">NewTopic</span>(<span class="string">&quot;tpc_4&quot;</span>, replicaAssignments); </span><br><span class="line"><span class="type">CreateTopicsResult</span> <span class="variable">result</span> <span class="operator">=</span> adminClient.createTopics(Arrays.asList(tpc_3,tpc_4)); </span><br><span class="line"></span><br><span class="line"><span class="comment">// 从 future 中等待服务端返回</span></span><br><span class="line"><span class="keyword">try</span> &#123; </span><br><span class="line">result.all().get(); </span><br><span class="line">&#125; <span class="keyword">catch</span> (Exception e) &#123; </span><br><span class="line">e.printStackTrace(); </span><br><span class="line">&#125; </span><br><span class="line">adminClient.close();</span><br><span class="line"></span><br></pre></td></tr></table></figure><h5 id="3-4-删除主题"><a href="#3-4-删除主题" class="headerlink" title="3.4 删除主题"></a>3.4 删除主题</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">DeleteTopicsResult</span> <span class="variable">deleteTopicsResult</span> <span class="operator">=</span> adminClient.deleteTopics(Arrays.asList(<span class="string">&quot;tpc_1&quot;</span>, <span class="string">&quot;tpc_1&quot;</span>)); </span><br><span class="line"></span><br><span class="line">Map&lt;String, KafkaFuture&lt;Void&gt;&gt; values = deleteTopicsResult.values();</span><br><span class="line"></span><br><span class="line">System.out.println(values);</span><br><span class="line"></span><br></pre></td></tr></table></figure><h5 id="3-5-其他管理"><a href="#3-5-其他管理" class="headerlink" title="3.5 其他管理"></a>3.5 其他管理</h5><p>除了进行 topic 管理之外,KafkaAdminClient 也可以进行诸如动态参数管理,分区管理等各类管理操作;</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Kafka命令行操作</title>
      <link href="/2022/06/18/Kafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C/"/>
      <url>/2022/06/18/Kafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C/</url>
      
        <content type="html"><![CDATA[<h4 id="Kafka命令行操作"><a href="#Kafka命令行操作" class="headerlink" title="Kafka命令行操作"></a>Kafka命令行操作</h4><h5 id="1-创建-topic"><a href="#1-创建-topic" class="headerlink" title="1 创建 topic"></a>1 创建 topic</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka-topics.sh --zookeeper node1:2181,node2:2181,node3:2181 --create --replication-factor 3 --partitions 3 --topic test_1</span><br></pre></td></tr></table></figure><span id="more"></span><p><img src="/.%5CKafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C%5C1654748253380.png" class="lazyload" data-srcset="/.%5CKafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C%5C1654748253380.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw=="></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">--replication-factor 副本数量</span><br><span class="line">--partitions 分区数量</span><br><span class="line">--topic topic 名称</span><br></pre></td></tr></table></figure><ul><li><p>手动指定副本的存储位置</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka-topics.sh --create --topic test_2 --zookeeper node1:2181 --replica-assignment 0:1,1:2</span><br></pre></td></tr></table></figure><p><img src="/.%5CKafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C%5C1654748314383.png" class="lazyload" data-srcset="/.%5CKafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C%5C1654748314383.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw=="></p></li></ul><h5 id="2-删除-topic"><a href="#2-删除-topic" class="headerlink" title="2  删除 topic"></a>2  删除 topic</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka-topics.sh --delete --topic tpc_1 --zookeeper node1:2181</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(异步线程去删除)删除 topic,需要一个参数处于启用状态: delete.topic.enable = true,否则删不掉</span><br></pre></td></tr></table></figure><h5 id="3-查看-topic"><a href="#3-查看-topic" class="headerlink" title="3 查看 topic"></a>3 查看 topic</h5><ul><li><p>列出当前系统中的所有 topic </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka-topics.sh --list --zookeeper node1:2181</span><br></pre></td></tr></table></figure><p><img src="/.%5CKafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C%5C1654749244422.png" class="lazyload" data-srcset="/.%5CKafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C%5C1654749244422.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw=="></p></li><li><p>查看 topic 详细信息</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kafka-topics.sh --create --topic tpc_1 --zookeeper node1:2181 --replica-assignment 0:1,1:2</span><br><span class="line"></span><br><span class="line">kafka-topics.sh --describe --topic tpc_1 --zookeeper node1:2181 </span><br></pre></td></tr></table></figure></li></ul><h5 id="4-增加分区数"><a href="#4-增加分区数" class="headerlink" title="4 增加分区数"></a>4 增加分区数</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka-topics.sh --alter --topic test_2 --partitions 3 --zookeeper node1:2181</span><br></pre></td></tr></table></figure><p><img src="/.%5CKafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C%5C1654749419821.png" class="lazyload" data-srcset="/.%5CKafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C%5C1654749419821.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw=="></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Kafka 只支持增加分区,不支持减少分区</span><br></pre></td></tr></table></figure><h5 id="5-动态配置-topic-参数"><a href="#5-动态配置-topic-参数" class="headerlink" title="5 动态配置 topic 参数"></a>5 动态配置 topic 参数</h5><ul><li><p>通过管理命令,可以为已创建的 topic 增加、修改、删除 topic level 参数</p></li><li><p>添加、修改配置参数（开启压缩发送传输种提高kafka消息吞吐量的有效办法(‘gzip’, ‘snappy’, ‘lz4’, ‘zstd’)）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka-configs.sh --zookeeper node1:2181 --entity-type topics --entity-name tpc_1 --alter --add-config compression.type=gzip </span><br></pre></td></tr></table></figure><p><img src="/.%5CKafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C%5C1654749489886.png" class="lazyload" data-srcset="/.%5CKafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C%5C1654749489886.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw=="></p></li><li><p>删除配置参数</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka-configs.sh --zookeeper node1:2181 --entity-type topics --entity-name tpc_1 --alter --delete-config compression.type</span><br></pre></td></tr></table></figure><p><img src="/.%5CKafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C%5C1654749544669.png" class="lazyload" data-srcset="/.%5CKafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C%5C1654749544669.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw=="></p></li></ul><h5 id="6-Kafka命令行生产者与消费者操作"><a href="#6-Kafka命令行生产者与消费者操作" class="headerlink" title="6 Kafka命令行生产者与消费者操作"></a>6 Kafka命令行生产者与消费者操作</h5><ul><li><p>生产者:kafka-console-producer</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka-console-producer.sh --broker-list node1:9092, node2:9092, node3:9092 --topic tpc_1</span><br></pre></td></tr></table></figure><p><img src="/.%5CKafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C%5C1654749699682.png" class="lazyload" data-srcset="/.%5CKafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C%5C1654749699682.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw=="></p></li><li><p>消费者:kafka-console-consumer</p></li><li><p>消费消息</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kafka-console-consumer.sh --bootstrap-server node1:9092, node2:9092, node3:9092 --topic tpc_1 --from-beginning</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="/.%5CKafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C%5C1654749720164.png" class="lazyload" data-srcset="/.%5CKafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C%5C1654749720164.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw=="></p></li><li><p>指定要消费的分区,和要消费的起始 offset </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka-console-consumer.sh --bootstrap-server node1:9092,node2:9092,node3:9092 --topic tpc_1 --offset 2 --partition 0</span><br></pre></td></tr></table></figure></li></ul><h5 id="7-配置管理-kafka-configs"><a href="#7-配置管理-kafka-configs" class="headerlink" title="7 配置管理 kafka-configs"></a>7 配置管理 kafka-configs</h5><ul><li><p>比如查看 topic 的配置可以按如下方式执行:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka-configs.sh zookeeper node1: 2181 --describe --entity-type topics --entity-name tpc_1 </span><br></pre></td></tr></table></figure></li><li><p>比如查看 broker 的动态配置可以按如下方式执行:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kafka-configs.sh zookeeper node1: 2181 --describe --entity-type brokers --entity-name 0 --zookeeper node1:2181</span><br><span class="line"></span><br></pre></td></tr></table></figure></li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Kafka环境配置</title>
      <link href="/2022/06/18/Kafka%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"/>
      <url>/2022/06/18/Kafka%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/</url>
      
        <content type="html"><![CDATA[<h4 id="Kafka-安装配置"><a href="#Kafka-安装配置" class="headerlink" title="Kafka 安装配置"></a>Kafka 安装配置</h4><p>Kafka是一种高吞吐量的分布式发布订阅消息系统，其在大数据开发应用上的目的是通过 Hadoop的并行加载机制来统一线上和离线的消息处理，也是为了通过集群来提供实时的消息。大数据开发需掌握Kafka架构原理及各组件的作用和使用方法及相关功能的实现。</p><span id="more"></span><ul><li><p>上传文件包 到&#x2F;export&#x2F;server&#x2F;</p></li><li><p>解压文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf kafka_2.11-2.0.0.tgz</span><br></pre></td></tr></table></figure></li><li><p>创建软连接</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ln -s kafka_2.11-2.0.0/ kafka</span><br></pre></td></tr></table></figure></li><li><p>进入 &#x2F;export&#x2F;server&#x2F;kafka&#x2F;config  修改 配置文件 server.properties</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /export/server/kafka/config</span><br><span class="line"></span><br><span class="line">vim server.properties </span><br></pre></td></tr></table></figure><ul><li><p>21 行内容 broker.id&#x3D;0 为依次增长的:0、1、2、3、4,集群中唯一 id 从0开始，每台不能重复（注：此处不用修改）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">21 broker.id=0</span><br></pre></td></tr></table></figure></li><li><p>31 行内容 #listeners&#x3D;PLAINTEXT:&#x2F;&#x2F;:9092 取消注释，内容改为：listeners&#x3D;PLAINTEXT:&#x2F;&#x2F;node1:9092 </p><p>PLAINTEXT为通信使用明文（加密ssl）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">31 listeners=PLAINTEXT://node1:9092 </span><br></pre></td></tr></table></figure></li><li><p>59 行内容  log.dirs&#x3D;&#x2F;tmp&#x2F;kafka-logs 为默认日志文件存储的位置，改为 log.dirs&#x3D;&#x2F;export&#x2F;data&#x2F;kafka-logs</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">59 log.dirs=/export/data/kafka-logs</span><br></pre></td></tr></table></figure></li><li><p>63 行内容为  num.partitions&#x3D;1 是默认分区数</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">63 num.partitions=1</span><br></pre></td></tr></table></figure></li><li><p>76 行部分</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">############################ **Log Flush Policy** ###############################</span><br><span class="line"> </span><br><span class="line">数据安全性（持久化之前先放到缓存上，从缓存刷到磁盘上）interval.messages   interval.ms</span><br></pre></td></tr></table></figure></li><li><p>93 行部分</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">########################### **Log Retention Policy** ############################</span><br><span class="line"> </span><br><span class="line">数据保留策略 168/24=7，1073741824/1024=1GB，300000ms = 300s = 5min超过了删掉（最后修改时间还是创建时间--&gt;日志段中最晚的一条消息，维护这个最大的时间戳--&gt;用户无法干预</span><br></pre></td></tr></table></figure></li><li><p>121 行内容 zookeeper.connect&#x3D;localhost:2181 修改为 zookeeper.connect&#x3D;node1:2181,node2:2181,node3:2181</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">121 zookeeper.connect=node1:2181,node2:2181,node3:2181</span><br></pre></td></tr></table></figure></li><li><p>126 行内容 group.initial.rebalance.delay.ms&#x3D;0 修改为 group.initial.rebalance.delay.ms&#x3D;3000</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">133 group.initial.rebalance.delay.ms=3000</span><br></pre></td></tr></table></figure></li></ul></li><li><p>给 node2 和 node3 scp 分发 kafka</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd /export/server/</span><br><span class="line"></span><br><span class="line">scp -r /export/server/kafka_2.11-2.0.0/ node2:$PWD</span><br><span class="line"></span><br><span class="line">scp -r /export/server/kafka_2.11-2.0.0/ node3:$PWD</span><br></pre></td></tr></table></figure></li><li><p>创建软连接</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ln -s /export/server/kafka_2.11-2.0.0/ kafka</span><br></pre></td></tr></table></figure></li><li><p>配置 kafka 环境变量（注：可以一台一台配，也可以在 master 完成后发给 node2 和 node3）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/profile </span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kafka 环境变量</span></span><br><span class="line">export KAFKA_HOME=/export/server/kafka </span><br><span class="line">export PATH=$PATH:$KAFKA_HOME/bin </span><br></pre></td></tr></table></figure></li><li><p>重新加载环境变量</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure></li><li><p>分别在 node2 和 node3 上修改配置文件 路径：&#x2F;export&#x2F;server&#x2F;kafka&#x2F;config</p><ul><li><p>将文件 server.properties  的第 21 行的 broker.id&#x3D;0 修改为 broker.id&#x3D;1 同理 node3 同样操作</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">21 broker.id=1</span><br></pre></td></tr></table></figure></li><li><p>将文件 server.properties  的第 31 行的 listeners&#x3D;PLAINTEXT:&#x2F;&#x2F;node1:9092 修改为 listeners&#x3D;PLAINTEXT:&#x2F;&#x2F;node2:9092 同理 node3 同样操作 </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">31 listeners=PLAINTEXT://node2:9092 </span><br></pre></td></tr></table></figure></li></ul></li><li><p>启停 kafka (注：kafka 启动需要在 zookeeper 启动的情况下才可)</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka-server-start.sh -daemon /export/server/kafka/config/server.properties</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">hadoop，zookeeper，kafka启动</span><br><span class="line">结果显示：</span><br><span class="line">(base) [root@node1 ~]# jps</span><br><span class="line">11793 NodeManager</span><br><span class="line">91699 Kafka</span><br><span class="line">85618 QuorumPeerMain</span><br><span class="line">10697 NameNode</span><br><span class="line">10924 DataNode</span><br><span class="line">11596 ResourceManager</span><br><span class="line">109852 Jps</span><br><span class="line"></span><br><span class="line">[root@node2 ~]# jps</span><br><span class="line">9301 DataNode</span><br><span class="line">9493 SecondaryNameNode</span><br><span class="line">95959 Kafka</span><br><span class="line">102971 Jps</span><br><span class="line">9855 NodeManager</span><br><span class="line">89534 QuorumPeerMain</span><br><span class="line"></span><br><span class="line">[root@node3 ~]# jps</span><br><span class="line">88660 QuorumPeerMain</span><br><span class="line">95204 Kafka</span><br><span class="line">9110 NodeManager</span><br><span class="line">8616 DataNode</span><br><span class="line">102104 Jps</span><br></pre></td></tr></table></figure></li><li><p>关闭 kafka</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka-server-stop.sh stop</span><br></pre></td></tr></table></figure></li><li><p>定制脚本一键启动</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vim kafka-all.sh</span><br><span class="line"></span><br><span class="line">放入 /bin 路径下</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line">if [ $# -eq 0 ];then</span><br><span class="line">        echo &quot;please input param:start stop&quot;</span><br><span class="line">else</span><br><span class="line">        if [ $1 = start ];then</span><br><span class="line">                for i in &#123;1..3&#125;</span><br><span class="line">                do</span><br><span class="line">                        echo &quot;$&#123;1&#125;ing node$&#123;i&#125;&quot;</span><br><span class="line">                        ssh node$&#123;i&#125; &quot;source /etc/profile;/export/server/kafka/bin/</span><br><span class="line">kafka-server-start.sh -daemon /export/server/kafka/config/server.properties&quot;</span><br><span class="line">                done</span><br><span class="line">        fi</span><br><span class="line"></span><br><span class="line">        if [ $1 = stop ];then</span><br><span class="line">                for i in &#123;1..3&#125;</span><br><span class="line">                do</span><br><span class="line">                        echo &quot;$&#123;1&#125;ping node$&#123;i&#125;&quot;        </span><br><span class="line">                        ssh node$&#123;i&#125; &quot;source /etc/profile;/export/server/kafka/bin/</span><br><span class="line">kafka-server-stop.sh -daemon /export/server/kafka/config/server.properties&quot;</span><br><span class="line">                done</span><br><span class="line">        fi</span><br><span class="line">fi</span><br><span class="line"></span><br></pre></td></tr></table></figure></li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Kafka-eagle配置文档</title>
      <link href="/2022/06/16/Kafka-eagle%E9%85%8D%E7%BD%AE%E6%96%87%E6%A1%A3/"/>
      <url>/2022/06/16/Kafka-eagle%E9%85%8D%E7%BD%AE%E6%96%87%E6%A1%A3/</url>
      
        <content type="html"><![CDATA[<p>一、在官网下载安装包：<a href="https://www.kafka-eagle.org/">https://www.kafka-eagle.org/</a><br> <img src="/../img/7.png" class="lazyload" data-srcset="/../img/7.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" alt="7.png"><br>二、下载后上传到node1的&#x2F;export&#x2F;sever&#x2F;目录下，使用命令tar -zxvf kafka-eagle-bin-2.1.0.tar.gz -C &#x2F;export&#x2F;server&#x2F;解压到当前目录<br>然后使用命令ln -s &#x2F;export&#x2F;server&#x2F; kafka-eagle-bin-2.1.0 &#x2F;export&#x2F;server&#x2F;kafka-eagle建立软链接：<br> <img src="/../img/8.png" class="lazyload" data-srcset="/../img/8.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" alt="8.png"><br>cd &#x2F;export&#x2F;server&#x2F;kafka-eagle目录下使用命令tar -zxvf efak-web-2.1.0-bin.tar.gz解压该文件包<br> <img src="/../img/9.png" class="lazyload" data-srcset="/../img/9.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" alt="9.png"><br> cd efak-web-2.1.0&#x2F;进入该文件cd conf然后vi system-config.properties编辑该文件具体改动如下：<br> kafka.eagle.zk.cluster.alias&#x3D;cluster1<br> cluster1.zk.list&#x3D;node1:2181,node2:2181,node3:2181<br> cluster1.kafka.eagle.broker.size&#x3D;3<br> kafka.eagle.url&#x3D;jdbc:sqlite:&#x2F;export&#x2F;data&#x2F;db&#x2F;ke.db<br>启动前需要手动创建&#x2F;export&#x2F;data&#x2F;db目录<br> mkdir &#x2F;export&#x2F;data&#x2F;db<br>进入&#x2F;etc&#x2F;profile目录下配置环境变量：(ps:还需配置java的环境变量)<br> <img src="/../img/10.png" class="lazyload" data-srcset="/../img/10.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" alt="10.png"><br>三、启动Eagle命令为：&#x2F;export&#x2F;server&#x2F;kafka-eagle&#x2F;efak-web-2.1.0&#x2F;bin&#x2F;ke.sh start<br> <img src="/../img/11.png" class="lazyload" data-srcset="/../img/11.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" alt="11.png"><br>然后jps命令查看发现eagle启动成功：<br> <img src="/../img/12.png" class="lazyload" data-srcset="/../img/12.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" alt="12.png"><br>之后进入<a href="http://192.168.117.151:8048，输入Account:admin,Password:123456">http://192.168.117.151:8048，输入Account:admin,Password:123456</a><br> <img src="/../img/13.png" class="lazyload" data-srcset="/../img/13.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" alt="13.png"><br>进入页面为：<br> <img src="/../img/14.png" class="lazyload" data-srcset="/../img/14.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" alt="14.png"><br> <img src="/../img/15.png" class="lazyload" data-srcset="/../img/15.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" alt="15.png"></p>]]></content>
      
      
      
        <tags>
            
            <tag> Kafka-eagle </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2022/05/29/hello-world/"/>
      <url>/2022/05/29/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><div class="story post-story"><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p></div>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>《Spark local&amp; stand-alone配置》</title>
      <link href="/2022/05/22/1/"/>
      <url>/2022/05/22/1/</url>
      
        <content type="html"><![CDATA[<p>Spark-Standalone-HA模式<br>Spark Standalone集群是Master-Slaves架构的集群模式,和大部分的Master-Slaves结构集群一样,存在<br>着Master 单点故障(SPOF)的问题。简单理解为，spark-Standalone 模式下为 master 节点控制其他节<br>点，当 master 节点出现故障时，集群就不可用了。 spark-Standalone-HA 模式下<br>master 节点不固定，当一个宕机时，立即换另一台为 master 保障不出现故障。<br>此处因为先前配置时的 zookeeper 版本和 spark 版本不太兼容，导致此模式有故障，需要重新下<br>载配置新的版本的 zookeeper<br>配置之前需要删除三台主机的 旧版 zookeeper 以及 对应的软连接<br>在 master 节点上重新进行前面配置的 zookeeper 操作</p><blockquote><blockquote><blockquote></blockquote><p>   1.上传apache-zookeeper-3.7.0-bin.tar.gz 到&#x2F;export&#x2F;server&#x2F;目录下 并解压文件 2.在 &#x2F;export&#x2F;server 目录下创建软连接 3.进入 &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;conf&#x2F; 将 zoo_sample.cfg 文件复制为新文件 zoo.cfg 4.接上步给 zoo.cfg 添加内容 5.进入 &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas 目录在此目录下创建 myid 文件，将 1 写入进 去6.将 master 节点中 &#x2F;export&#x2F;server&#x2F;zookeeper-3.7.0 路径下内容推送给slave1 和 slave2 7.推送成功后，分别在 slave1 和 slave2 上创建软连接 8.接上步推送完成后将 slave1 和 slave2 的 &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas&#x2F;文件夹 下的 myid 中的内容分别改为 2 和 3 配置环境变量： 因先前配置 zookeeper 时候创建过软连接且以 ’zookeeper‘ 为路径，所以不用配置环境变量，此 处也是创建软连接的方便之处</p></blockquote></blockquote><p>进入 &#x2F;export&#x2F;server&#x2F;spark&#x2F;conf 文件夹 修改 spark-env.sh 文件内容</p><blockquote><blockquote><blockquote></blockquote><p>   cd &#x2F;export&#x2F;server&#x2F;spark&#x2F;conf<br>    vim spark-env.sh</p></blockquote></blockquote><p>为 83 行内容加上注释，此部分原为指定 某台主机 做 master ，加上注释后即为 任何主机都<br>可以做 master</p><blockquote><blockquote><blockquote></blockquote><p>   结果显示：<br>     ……<br>      82 # 告知Spark的master运行在哪个机器上 83 # export SPARK_MASTER_HOST&#x3D;master<br>     ………</p></blockquote></blockquote><p>文末添加内容</p><blockquote><blockquote><blockquote><p>SPARK_DAEMON_JAVA_OPTS&#x3D;”-Dspark.deploy.recoveryMode&#x3D;ZOOKEEPER -<br>       Dspark.deploy.zookeeper.url&#x3D;master:2181,slave1:2181,slave2:2181 - Dspark.deploy.zookeeper.dir&#x3D;&#x2F;spark-ha”<br>        #spark.deploy.recoveryMode<br>        指定HA模式 基于Zookeeper实现<br>        #指定Zookeeper的连接地址<br>        #指定在Zookeeper中注册临时节点的路径</p></blockquote></blockquote></blockquote><p>分发 spark-env.sh 到 salve1 和 slave2 上</p><blockquote><blockquote><blockquote></blockquote><pre><code>scp spark-env.sh slave1:/export/server/spark/conf/ scp spark-env.sh slave2:/export/server/spark/conf/</code></pre></blockquote></blockquote><p>启动之前确保 Zookeeper 和 HDFS 均已经启动<br>启动集群:</p><blockquote><blockquote><blockquote></blockquote><p>   #在 master 上 启动一个master 和全部worker &#x2F;export&#x2F;server&#x2F;spark&#x2F;sbin&#x2F;start-all.sh # 注意, 下面命令在 slave1 上执行 启动 slave1 上的 master 做备用 master &#x2F;export&#x2F;server&#x2F;spark&#x2F;sbin&#x2F;start-master.sh<br>    结果显示：<br>    (base) [root@master ~]# jps<br>    37328 DataNode<br>    41589 Master<br>    35798 QuorumPeerMain<br>    38521 ResourceManager<br>    46281 Jps<br>    38907 NodeManager<br>    41821 Worker<br>    36958 NameNode (base)<br>    [root@slave1 sbin]# jps<br>    36631 DataNode<br>    48135 Master<br>    35385 QuorumPeerMain<br>    37961 NodeManager<br>    40970 Worker<br>    48282 Jps<br>    37276 SecondaryNameNode</p></blockquote></blockquote><p>访问 WebUI 界面</p><blockquote><blockquote><blockquote></blockquote><pre><code>http://master:8081/</code></pre><p>   <a href="http://slave1:8082/">http://slave1:8082/</a></p></blockquote></blockquote><p>此时 kill 掉 master 上的 master 假设 master 主机宕机掉</p><blockquote><blockquote><blockquote></blockquote><p>   #master主机 master 的进程号 kill -9 41589 结果显示： (base) [root@master ~]# jps 37328 DataNode 90336 Jps 35798 QuorumPeerMain 38521 ResourceManager 38907 NodeManager 41821 Worker 36958 NameNode</p></blockquote></blockquote><p>访问 slave1 的 WebUI</p><blockquote><blockquote><blockquote><p><a href="http://slave1:8082/">http://slave1:8082/</a></p></blockquote></blockquote></blockquote><p>进行主备切换的测试<br>提交一个 spark 任务到当前 活跃的 master上 :</p><blockquote><blockquote><blockquote><p>&#x2F;export&#x2F;server&#x2F;spark&#x2F;bin&#x2F;spark-submit –master spark:&#x2F;&#x2F;master:7077 &#x2F;export&#x2F;server&#x2F;spark&#x2F;examples&#x2F;src&#x2F;main&#x2F;python&#x2F;pi.py 1000</p></blockquote></blockquote></blockquote><p>复制标签 kill 掉 master 的 进程号<br>再次访问 master 的 WebUI</p><blockquote><blockquote><blockquote><p><a href="http://master:8081/">http://master:8081/</a><br>      网页访问不了！</p></blockquote></blockquote></blockquote><p>再次访问 slave1 的 WebUI</p><blockquote><blockquote><blockquote><p><a href="http://slave1:8082/">http://slave1:8082/</a></p></blockquote></blockquote></blockquote><p>可以看到当前活跃的 master 提示信息</p><blockquote><blockquote><blockquote><p>(base) [root@master ~]# &#x2F;export&#x2F;server&#x2F;spark&#x2F;bin&#x2F;spark-submit –master spark:&#x2F;&#x2F;master:7077 &#x2F;export&#x2F;server&#x2F;spark&#x2F;examples&#x2F;src&#x2F;main&#x2F;python&#x2F;pi.py 1000 22&#x2F;03&#x2F;29 16:11:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicable 22&#x2F;03&#x2F;29 16:12:16 WARN StandaloneAppClient$ClientEndpoint: Connection to master:7077 failed; waiting for master to reconnect… 22&#x2F;03&#x2F;29 16:12:16 WARN StandaloneSchedulerBackend: Disconnected from Spark cluster! Waiting for reconnection… 22&#x2F;03&#x2F;29 16:12:16 WARN StandaloneAppClient$ClientEndpoint: Connection to master:7077 failed; waiting for master to reconnect… Pi is roughly 3.140960 (base) [root@master ~]#</p></blockquote></blockquote></blockquote><p>Spark On YARN模式</p><blockquote><blockquote><blockquote><p>在已有YARN集群的前提下在单独准备Spark StandAlone集群,对资源的利用就不高.Spark On YARN, 无</p></blockquote></blockquote></blockquote><p>需部署Spark集群, 只要找一台服务器, 充当Spark的客户端<br>保证 HADOOP_CONF_和 DIR_YARN_CONF_DIR 已经配置在 spark-env.sh 和环境变量中 （注: 前面配置spark-Standlone 时已经配置过此项了）</p><blockquote><blockquote><blockquote><p>spark-env.sh 文件部分显示： …. 77 ## HADOOP软件配置文件目录，读取HDFS上文件和运行YARN集群 78 HADOOP_CONF_DIR&#x3D;&#x2F;export&#x2F;server&#x2F;hadoop&#x2F;etc&#x2F;hadoop 79 YARN_CONF_DIR&#x3D;&#x2F;export&#x2F;server&#x2F;hadoop&#x2F;etc&#x2F;hadoop ….</p></blockquote></blockquote></blockquote><p>链接到 YARN 中（注: 交互式环境 pyspark 和 spark-shell 无法运行 cluster模式）<br>bin&#x2F;pyspark –master yarn –deploy-mode client|cluster # –deploy-mode 选项是指定部署模式, 默认是 客户端模式 # client就是客户端模式 # cluster就是集群模式 # –deploy-mode 仅可以用在YARN模式下<br> bin&#x2F;spark-shell –master yarn –deploy-mode client|cluster<br>bin&#x2F;spark-submit –master yarn –deploy-mode client|cluster &#x2F;xxx&#x2F;xxx&#x2F;xxx.py 参数</p><blockquote><blockquote><blockquote></blockquote><p>  spark-submit 和 spark-shell 和 pyspark的相关参数</p></blockquote></blockquote><ul><li>bin&#x2F;pyspark: pyspark解释器spark环境 - bin&#x2F;spark-shell: scala解释器spark环境 - bin&#x2F;spark-submit: 提交jar包或Python文件执行的工具 - bin&#x2F;spark-sql: sparksql客户端工具<br>  这4个客户端工具的参数基本通用.以spark-submit 为例: bin&#x2F;spark-submit –master spark:&#x2F;&#x2F;master:7077 xxx.py&#96;<blockquote><blockquote><blockquote></blockquote><p>  Usage: spark-submit [options] &lt;app jar | python file | R file&gt; [app arguments]<br>   Usage: spark-submit –kill [submission ID] –master [spark:&#x2F;&#x2F;…]<br>   Usage: spark-submit –status [submission ID] –master [spark:&#x2F;&#x2F;…]<br>   Usage: spark-submit run-example [options] example-class [example args] </p><blockquote></blockquote><p>   Options: –master MASTER_URL spark:&#x2F;&#x2F;host:port, mesos:&#x2F;&#x2F;host:port, yarn, k8s:&#x2F;&#x2F;<a href="https://host:port">https://host:port</a>, or </p><blockquote></blockquote><p>   local (Default: local[*]). –deploy-mode DEPLOY_MODE 部署模式 client 或者 cluster 默认是client –class CLASS_NAME 运行java或者scala class(for Java &#x2F; Scala apps). –name NAME 程序的名字 –jars JARS Comma-separated list of jars to include on the </p><blockquote></blockquote><p>   driver and executor classpaths. –packages Comma-separated list of maven coordinates of </p><blockquote></blockquote><p>   jars to include on the driver and executor classpaths. Will </p><blockquote></blockquote><p>   search the local maven repo, then maven central and any </p><blockquote></blockquote><p>   additional remote repositories given by –repositories. The </p><blockquote></blockquote><p>   format for the coordinates should be </p><blockquote></blockquote><p>   groupId:artifactId:version.<br>   –exclude-packages Comma-separated list of groupId:artifactId, to exclude while resolving the dependencies provided in<br>– packages to avoid dependency conflicts. –repositories Comma-separated list of additional remote repositories to search for the maven coordinates given with<br> – packages.<br> –py-files PY_FILES 指定Python程序依赖的其它python文件<br> –files FILES Comma-separated list of files to be placed in the working directory of each executor. File paths of these files in executors can be accessed via SparkFiles.get(fileName).<br>  –archives ARCHIVES Comma-separated list of archives to be extracted into the working directory of each executor.<br> –conf,<br> -c PROP&#x3D;VALUE 手动指定配置<br> –properties-file FILE Path to a file from which to load extra properties. If not specified, this will look for conf&#x2F;spark- defaults.conf. –driver-memory MEM Driver的可用内存(Default: 1024M). –driver-java-options Driver的一些Java选项 –driver-library-path Extra library path entries to pass to the driver. –driver-class-path Extra class path entries to pass to the driver. Note that jars added with –jars are automatically included in the classpath.<br> –executor-memory MEM Executor的内存 (Default: 1G).<br> –proxy-user NAME User to impersonate when submitting the application. This argument does not work with<br> –principal &#x2F;<br> –keytab.<br> –help,<br> -h 显示帮助文件<br>  –verbose,<br>  -v Print additional debug output. –version, 打印版本 Cluster deploy mode only(集群模式专属):<br>   –driver-cores NUM Driver可用的的CPU核数(Default: 1). Spark standalone or Mesos with cluster deploy mode only:<br>   –supervise 如果给定, 可以尝试重启Driver Spark standalone, Mesos or K8s with cluster deploy mode only:<br>   –kill SUBMISSION_ID 指定程序ID kill –status SUBMISSION_ID 指定程序ID 查看运行状态 Spark standalone, Mesos and Kubernetes only:<br>   –total-executor-cores NUM 整个任务可以给Executor多少个CPU核心用 Spark standalone, YARN and Kubernetes only:<br>    –executor-cores NUM 单个Executor能使用多少CPU核心 Spark on YARN and Kubernetes only(YARN模式下):<br>    –num-executors NUM Executor应该开启几个<br>    –principal PRINCIPAL Principal to be used to login to KDC.<br>    –keytab KEYTAB The full path to the file that contains the keytab for the principal specified above. Spark on YARN only:<br>    –queue QUEUE_NAME 指定运行的YARN队列(Default: “default”)</p></blockquote></blockquote></li></ul><p>启动 YARN 的历史服务器</p><blockquote><blockquote><blockquote><p>cd &#x2F;export&#x2F;server&#x2F;hadoop-3.3.0&#x2F;sbin .&#x2F;mr-jobhistory-daemon.sh start historyserver</p></blockquote></blockquote></blockquote><p>访问WebUI界面</p><blockquote><blockquote><blockquote><p><a href="http://master:19888/">http://master:19888/</a></p></blockquote></blockquote></blockquote><p>client 模式测试</p><blockquote><blockquote><blockquote><p>SPARK_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;spark ${SPARK_HOME}&#x2F;bin&#x2F;spark-submit –master yarn –deploy-mode client – driver-memory 512m –executor-memory 512m –num-executors 1 –total- executor-cores 2 ${SPARK_HOME}&#x2F;examples&#x2F;src&#x2F;main&#x2F;python&#x2F;pi.py</p></blockquote></blockquote></blockquote><p>cluster 模式测试</p><blockquote><blockquote><blockquote><p>SPARK_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;spark ${SPARK_HOME}&#x2F;bin&#x2F;spark-submit –master yarn –deploy-mode cluster –driver- memory 512m –executor-memory 512m –num-executors 1 –total-executor-cores 2 –conf “spark.pyspark.driver.python&#x3D;&#x2F;root&#x2F;anaconda3&#x2F;bin&#x2F;python3” –conf “spark.pyspark.python&#x3D;&#x2F;root&#x2F;anaconda3&#x2F;bin&#x2F;python3” ${SPARK_HOME}&#x2F;examples&#x2F;src&#x2F;main&#x2F;python&#x2F;pi.py 3**</p></blockquote></blockquote></blockquote></details><hr><p>title: ‘《Spark local&amp; stand-alone配置》’<br>toc: false<br>comments: true<br>keywords: ‘’<br>description: ‘’<br>date: 2022-05-22 16:18:57<br>updated: 2022-05-22 16:18:57<br>categories:<br>tags:<br>top:<br>academia: true</p><hr><h1 id="《Spark-local-amp-stand-alone配置》"><a href="#《Spark-local-amp-stand-alone配置》" class="headerlink" title="《Spark local&amp; stand-alone配置》"></a>《Spark local&amp; stand-alone配置》</h1><details><summary>阅读全文</summary><p>**<summary>本地模式(单机) 本地模式就是以一个独立的进程,通过其内部的多个线程来模拟整个Spark运行时环境<br>Anaconda On Linux 安装 (单台服务器脚本安装)<br>安装上传安装包: 资料中提供的Anaconda3-2021.05-Linux-x86_64.sh文件到Linux服务器上安装<br>位置在 &#x2F;export&#x2F;server:</p><blockquote><blockquote><blockquote></blockquote><p>   cd &#x2F;export&#x2F;server<br>   #运行文件 sh Anaconda3-2021.05-Linux-x86_64.sh<br>   过程显示：<br>   …<br>   #出现内容选 yes Please answer ‘yes’ or ‘no’:’ &gt;&gt;&gt; yes …<br>   #出现添加路径：&#x2F;export&#x2F;server&#x2F;anaconda3<br>   …<br>   [&#x2F;root&#x2F;anaconda3] </p><blockquote></blockquote><p>   &#x2F;export&#x2F;server&#x2F;anaconda3 PREFIX&#x3D;&#x2F;export&#x2F;server&#x2F;anaconda3<br>   …</p></blockquote></blockquote><p>安装完成后, 退出终端， 重新进来:</p><blockquote><blockquote><blockquote><p>exit<br>   结果显示：<br>   #看到这个Base开头表明安装好了.base是默认的虚拟环境. Last login: Tue Mar 15 15:28:59 2022 from 192.168.88.1 (base)<br>   [root@node1 ~]#</p></blockquote></blockquote></blockquote><p>创建虚拟环境 pyspark 基于 python3.8</p><blockquote><blockquote><blockquote></blockquote><p>   conda create -n pyspark python&#x3D;3.8</p></blockquote></blockquote><p>切换到虚拟环境内</p><blockquote><blockquote><blockquote></blockquote><p>   conda activate pyspark 结果显示： (base) [root@node1 ~]# conda activate pyspark (pyspark) [root@node1 ~]#</p></blockquote></blockquote><p>在虚拟环境内安装包 （有WARNING不用管）</p><blockquote><blockquote><blockquote></blockquote><p>   pip install pyhive pyspark jieba -i <a href="https://pypi.tuna.tsinghua.edu.cn/simple">https://pypi.tuna.tsinghua.edu.cn/simple</a></p></blockquote></blockquote><p>spark 安装<br>将文件上传到 &#x2F;export&#x2F;server 里面 ，解压</p><blockquote><blockquote><blockquote></blockquote><p>   cd &#x2F;export&#x2F;server # 解压 tar -zxvf spark-3.2.0-bin-hadoop3.2.tgz -C &#x2F;export&#x2F;server&#x2F;</p></blockquote></blockquote><p>建立软连接</p><blockquote><blockquote><blockquote></blockquote><p>   ln -s &#x2F;export&#x2F;server&#x2F;spark-3.2.0-bin-hadoop3.2 &#x2F;export&#x2F;server&#x2F;spark</p></blockquote></blockquote><p>添加环境变量</p><blockquote><blockquote><blockquote></blockquote><p>   SPARK_HOME: 表示Spark安装路径在哪里<br>   PYSPARK_PYTHON: 表示Spark想运行Python程序, 那么去哪里找python执行器<br>   JAVA_HOME: 告知Spark Java在哪里<br>   HADOOP_CONF_DIR: 告知Spark Hadoop的配置文件在哪里<br>   HADOOP_HOME: 告知Spark Hadoop安装在哪里</p></blockquote></blockquote><p>   vim &#x2F;etc&#x2F;profile<br>   内容：<br>   …..<br>   注：此部分之前配置过，此部分不需要在配置<br>   #JAVA_HOME export JAVA_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;jdk1.8.0_241 export PATH&#x3D;$PATH:$JAVA_HOME&#x2F;bin export CLASSPATH&#x3D;.:$JAVA_HOME&#x2F;lib&#x2F;dt.jar:$JAVA_HOME&#x2F;lib&#x2F;tools.jar </p><p>   #HADOOP_HOME export HADOOP_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;hadoop-3.3.0 export PATH&#x3D;$PATH:$HADOOP_HOME&#x2F;bin:$HADOOP_HOME&#x2F;sbin </p><p>   #ZOOKEEPER_HOME export ZOOKEEPER_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;zookeeper export PATH&#x3D;$PATH:$ZOOKEEPER_HOME&#x2F;bin ….. </p><p>   #将以下部分添加进去 #SPARK_HOME export SPARK_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;spark #HADOOP_CONF_DIR export HADOOP_CONF_DIR&#x3D;$HADOOP_HOME&#x2F;etc&#x2F;hadoop #PYSPARK_PYTHON export PYSPARK_PYTHON&#x3D;&#x2F;export&#x2F;server&#x2F;anaconda3&#x2F;envs&#x2F;pyspark&#x2F;bin&#x2F;python<br>   vim .bashrc<br>   内容添加进去： </p><p>   #JAVA_HOME<br>   export JAVA_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;jdk1.8.0_241<br>   #PYSPARK_PYTHON<br>   export PYSPARK_PYTHON&#x3D;&#x2F;export&#x2F;server&#x2F;anaconda3&#x2F;envs&#x2F;pyspark&#x2F;bin&#x2F;python</p><p>重新加载环境变量文件</p><blockquote><blockquote><blockquote></blockquote><p>   source &#x2F;etc&#x2F;profile<br>   source ~&#x2F;.bashrc</p></blockquote></blockquote><p>进入 &#x2F;export&#x2F;server&#x2F;anaconda3&#x2F;envs&#x2F;pyspark&#x2F;bin&#x2F; 文件夹</p><blockquote><blockquote><blockquote></blockquote><p>   cd &#x2F;export&#x2F;server&#x2F;anaconda3&#x2F;envs&#x2F;pyspark&#x2F;bin&#x2F;</p></blockquote></blockquote><p>开启 </p><blockquote><blockquote><blockquote></blockquote><p>  .&#x2F;pyspark 结果显示： (base) [root@master bin]# .&#x2F;pyspark<br>   Python 3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0] :: Anaconda, Inc. on linux<br>   Type “help”, “copyright”, “credits” or “license” for more information.<br>   Setting default log level to “WARN”. To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel). 2022-03-15 20:37:04,612 WARN util.NativeCodeLoader: Unable to load native- hadoop library for your platform… using builtin-java classes where applicable<br>    Welcome to<br>         __              __<br>    __ &#x2F; <strong>&#x2F;</strong> ___ _<em><em><strong>&#x2F; &#x2F;</strong><br>     <em>\ / _ / _ &#96;&#x2F; _<em>&#x2F; ‘</em>&#x2F;<br>     &#x2F;_</em> &#x2F; .</em><em>&#x2F;_,</em>&#x2F;</em>&#x2F; &#x2F;<em>&#x2F;_\ version 3.2.0<br>        &#x2F;</em>&#x2F;<br>    Using Python version 3.8.12 (default, Oct 12 2021 13:49:34) Spark context Web UI available at <a href="http://master:4040/">http://master:4040</a> Spark context available as ‘sc’ (master &#x3D; local[*], app id &#x3D; local- 1647347826262). SparkSession available as ‘spark’. &gt;&gt;&gt;</p></blockquote></blockquote><p>查看WebUI界面</p><blockquote><blockquote><blockquote><p>浏览器访问：<br>    <a href="http://node1:4040/">http://node1:4040/</a></p></blockquote></blockquote></blockquote><p>退出</p><blockquote><blockquote><blockquote><p>conda deactivate</p></blockquote></blockquote></blockquote><p>Standalone模式(集群) Spark中的各个角色以独立进程的形式存在,并组成Spark集群环境 Anaconda On Linux 安装 (单台服务器脚本安装 注：在 slave1 和 slave2 上部署)<br>安装上传安装包: 资料中提供的Anaconda3-2021.05-Linux-x86_64.sh文件到Linux服务器上安装位置在 &#x2F;export&#x2F;server:</p><p>cd &#x2F;export&#x2F;server # 运行文件 sh Anaconda3-2021.05-Linux-x86_64.sh</p><blockquote><blockquote><blockquote><p>过程显示：<br> …<br> #出现内容选 yes<br>  Please answer ‘yes’ or ‘no’:’<br>yes<br>   …<br>   #出现添加路径：&#x2F;export&#x2F;server&#x2F;anaconda3<br>   …<br>   [&#x2F;root&#x2F;anaconda3] &gt;&gt;&gt; &#x2F;export&#x2F;server&#x2F;anaconda3 PREFIX&#x3D;&#x2F;export&#x2F;server&#x2F;anaconda3<br>   …</p></blockquote></blockquote></blockquote><p>安装完成后, 退出终端，</p><blockquote><blockquote><blockquote></blockquote><p>  重新进来:<br>  exit<br>  结果显示：<br>   #看到这个Base开头表明安装好了.base是默认的虚拟环境.<br>    Last login: Tue Mar 15 15:28:59 2022 from 192.168.88.1<br>   …</p></blockquote></blockquote><p>在 master 节点上把 .&#x2F;bashrc 和 profile 分发给 slave1 和 slave2</p><blockquote><blockquote><blockquote></blockquote><p>   #分发 .bashrc : scp <del>&#x2F;.bashrc root@slave1:</del>&#x2F; scp <del>&#x2F;.bashrc root@slave2:</del>&#x2F; #分发 profile : scp &#x2F;etc&#x2F;profile&#x2F; root@slave1:&#x2F;etc&#x2F; scp &#x2F;etc&#x2F;profile&#x2F; root@slave2:&#x2F;etc&#x2F;<br>   …</p></blockquote></blockquote><p>创建虚拟环境 pyspark 基于 python3.8</p><blockquote><blockquote><blockquote></blockquote><p>   conda create -n pyspark python&#x3D;3.8</p></blockquote></blockquote><p>切换到虚拟环境内</p><blockquote><blockquote><blockquote></blockquote><p>   conda activate pyspark 结果显示： (base) [root@node1 ~]# conda activate pyspark (pyspark)<br>在虚拟环境内安装包 （有WARNING不用管）<br>    pip install pyhive pyspark jieba -i <a href="https://pypi.tuna.tsinghua.edu.cn/simple">https://pypi.tuna.tsinghua.edu.cn/simple</a><br>    spark 安装<br>将文件上传到 &#x2F;export&#x2F;server 里面 ，解压</p></blockquote></blockquote><p>master 节点节点进入 &#x2F;export&#x2F;server&#x2F;spark&#x2F;conf 修改以下配置文件</p><blockquote><blockquote><blockquote></blockquote><p>   cd &#x2F;export&#x2F;server&#x2F;spark&#x2F;conf</p></blockquote></blockquote><p>将文件 workers.template 改名为 workers，并配置文件内容</p><blockquote><blockquote><blockquote></blockquote><p>   mv workers.template workers vim workers<br>    # localhost删除，内容追加文末： node1<br>    node2<br>    node3<br>    # 功能: 这个文件就是指示了 当前SparkStandAlone环境下, 有哪些worker</p></blockquote></blockquote><p>将文件 spark-env.sh.template 改名为 spark-env.sh，并配置相关内容</p><blockquote><blockquote><blockquote></blockquote><p>   mv spark-env.sh.template spark-env.sh vim spark-env.sh</p></blockquote></blockquote><blockquote><blockquote><blockquote><p>文末追加内容：<br>   ##设置JAVA安装目录 JAVA_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;jdk<br>   ##HADOOP软件配置文件目录，读取HDFS上文件和运行YARN集群 HADOOP_CONF_DIR&#x3D;&#x2F;export&#x2F;server&#x2F;hadoop&#x2F;etc&#x2F;hadoop YARN_CONF_DIR&#x3D;&#x2F;export&#x2F;server&#x2F;hadoop&#x2F;etc&#x2F;hadoop ## 指定spark老大Master的IP和提交任务的通信端口 # 告知Spark的master运行在哪个机器上 export SPARK_MASTER_HOST&#x3D;master<br>    #告知sparkmaster的通讯端口 export SPARK_MASTER_PORT&#x3D;7077<br>    # 告知spark master的 webui端口 SPARK_MASTER_WEBUI_PORT&#x3D;8080<br>    # worker cpu可用核数 SPARK_WORKER_CORES&#x3D;1<br>    # worker可用内存 SPARK_WORKER_MEMORY&#x3D;1g<br>    # worker的工作通讯地址 SPARK_WORKER_PORT&#x3D;7078<br>    # worker的 webui地址 SPARK_WORKER_WEBUI_PORT&#x3D;8081<br>    ## 设置历史服务器 # 配置的意思是 将spark程序运行的历史日志 存到hdfs的&#x2F;sparklog文件夹中 SPARK_HISTORY_OPTS&#x3D;”- Dspark.history.fs.logDirectory&#x3D;hdfs:&#x2F;&#x2F;master:8020&#x2F;sparklog&#x2F; - Dspark.history.fs.cleaner.enabled&#x3D;true”</p></blockquote></blockquote></blockquote><p>开启 hadoop 的 hdfs 和 yarn 集群</p><blockquote><blockquote><blockquote></blockquote><pre><code>start-dfs.sh start-yarn.sh</code></pre><p>在HDFS上创建程序运行历史记录存放的文件夹，同样 conf 文件目录下:</p><blockquote></blockquote><p>   hadoop fs -mkdir &#x2F;sparklog hadoop fs -chmod 777 &#x2F;sparklog</p></blockquote></blockquote><p>将 spark-defaults.conf.template 改为 spark-defaults.conf 并做相关配置</p><blockquote><blockquote><blockquote></blockquote><p>   mv spark-defaults.conf.template spark-defaults.conf vim spark-defaults.conf 文末追加内容为： # 开启spark的日期记录功能 spark.eventLog.enabled true # 设置spark日志记录的路径 spark.eventLog.dir hdfs:&#x2F;&#x2F;master:8020&#x2F;sparklog&#x2F; # 设置spark日志是否启动压缩 spark.eventLog.compress true</p></blockquote></blockquote><p>配置 log4j.properties 文件 将文件第 19 行的 log4j.rootCategory&#x3D;INFO, console 改为<br>log4j.rootCategory&#x3D;WARN, console （即将INFO 改为 WARN 目的：输出日志, 设置级别为<br>WARN 只输出警告和错误日志，INFO 则为输出所有信息，多数为无用信息）</p><blockquote><blockquote><blockquote></blockquote><p>   mv log4j.properties.template log4j.properties vim log4j.properties 结果显示：<br>    …<br>    18 # Set everything to be logged to the console<br>    19 log4j.rootCategory&#x3D;WARN, console ….</p></blockquote></blockquote><p>master 节点分发 spark 安装文件夹 到 slave1 和 slave2 上</p><blockquote><blockquote><blockquote></blockquote><p>   master 节点分发 spark 安装文件夹 到 slave1 和 slave2 上</p></blockquote></blockquote><p>在slave1 和 slave2 上做软连接</p><blockquote><blockquote><blockquote></blockquote><p>   ln -s &#x2F;export&#x2F;server&#x2F;spark-3.2.0-bin-hadoop3.2 &#x2F;export&#x2F;server&#x2F;spark</p></blockquote></blockquote><p>重新加载环境变量</p><blockquote><blockquote><blockquote></blockquote><p>   source &#x2F;etc&#x2F;profile</p></blockquote></blockquote><p>进入 &#x2F;export&#x2F;server&#x2F;spark&#x2F;sbin 文件目录下 启动 start-history-server.sh</p><blockquote><blockquote><blockquote></blockquote><p>   cd &#x2F;export&#x2F;server&#x2F;spark&#x2F;sbin .&#x2F;start-history-server.sh</p></blockquote></blockquote><p>访问 WebUI 界面</p><blockquote><blockquote><blockquote></blockquote><p>   浏览器访问： <a href="http://master:18080/">http://master:18080/</a></p></blockquote></blockquote>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>《spark基础环境配置》</title>
      <link href="/2022/05/22/2/"/>
      <url>/2022/05/22/2/</url>
      
        <content type="html"><![CDATA[<h1 id="《spark基础环境配置》"><a href="#《spark基础环境配置》" class="headerlink" title="《spark基础环境配置》"></a>《spark基础环境配置》</h1><p>打开一个hosts映射文件,为了保证后续相互关联的虚拟机能够通过主机名进行访问，根据实际需求配置<br>对应的IP和主机名映射，分别将主机名master、slave1、slave2 与IP地址 192.168.88.134、<br>192.168.88.135 和192.168.88.136进行了匹配映射(这里通常要根据实际需要，将要搭建的集群主机都<br>配置主机名和IP映射)。<br>编辑 &#x2F;etc&#x2F;hosts 文件</p><blockquote><blockquote><blockquote><p>vim &#x2F;etc&#x2F;hosts<br>   内容修改为（注：三台主机内容一样）<br>   localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6<br>   192.168.88.135 node1<br>   192.168.88.136 node2<br>   192.168.88.137 node3</p></blockquote></blockquote></blockquote><p>三、集群配置时间同步<br>定义：网络时间服务协议（Network Time Protocol, NTP），是用来使计算机时间同步化的一种协议，它可以使计算机对其服务器做时间同步化。<br>原因：时间同步服务器，顾名思义就是来同步时间的。在集群中同步时间有着十分重要的作用，负载均衡集群或高可用集群如果时间不一致，在服务器之间的数据误差就会很大，寻找数据便会成为一件棘手的事情。若是时间无法同步，那么就算是备份了数据，你也可能无法在正确的时间将正确的数据备份。那损失可就大了。<br>yum 安装 ntp （注：三台主机做同样操作）</p><blockquote><blockquote><blockquote><p>yum install ntp -y<br>   开机自启动ntp<br>   systemctl enable ntpd &amp;&amp; systemctl start ntpd<br>   结果显示： [root@master ~]# systemctl enable ntpd &amp;&amp; systemctl start ntpd Created symlink from &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;multi- user.target.wants&#x2F;ntpd.service to &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;ntpd.service.</p></blockquote></blockquote></blockquote><p>授权 192.168.88.0-192.168.10.255 网段上的所有机器可以从这台机器上查询和同步时间</p><blockquote><blockquote><blockquote></blockquote><p>   #查看ntp配置文件<br>   ls -al &#x2F;etc | grep ‘ntp’<br>   #显示内容<br>   [root@node1 etc]# ls -al &#x2F;etc | grep ‘ntp’<br>   drwxr-xr-x 3 root root 52 3月 10 18:25 ntp<br>   -rw-r–r– 1 root root 2041 3月 10 20:03 ntp.conf<br>    #编辑内容添加 restrict 192.168.88.0 mask 255.255.255.0 （注：在17行左右） vim &#x2F;etc&#x2F;ntp.conf<br>    16 # Hosts on local network are less restricted.<br>    17 restrict 192.168.88.0 mask 255.255.255.0</p></blockquote></blockquote><p>集群在局域网中，不使用其他互联网上的时间</p><blockquote><blockquote><blockquote></blockquote><pre><code>#修改 /etc/ntpd.conf 内容vim vim /etc/ntp.conf # </code></pre></blockquote></blockquote><pre><code>将21-24行内容注释掉（注：原来未注释）   21 #server 0.centos.pool.ntp.org iburst 22 #server 1.centos.pool.ntp.org iburst 23 #server 2.centos.pool.ntp.org iburst 24 #server 3.centos.pool.ntp.org iburst # 在25行添加 server masterIP 即为： server 192.168.88.135</code></pre><p>node 和 node3 相同操作<br>三台主机同时执行</p><blockquote><blockquote><blockquote></blockquote><p>   systemctl enable ntpd &amp;&amp; systemctl start ntpd</p></blockquote></blockquote><p>查看ntp端口</p><blockquote><blockquote><blockquote></blockquote><p>   [root@master etc]# ss -tupln | grep ‘123’ udp UNCONN 0 0 192.168.88.135:123 <em>:</em> users:((“ntpd”,pid&#x3D;54823,fd&#x3D;19)) udp UNCONN 0 0 127.0.0.1:123 <em>:</em> users:((“ntpd”,pid&#x3D;54823,fd&#x3D;18)) udp UNCONN 0 0 <em>:123 <em>:</em> users:((“ntpd”,pid&#x3D;54823,fd&#x3D;16)) udp UNCONN 0 0 [fe80::2832:5f98:5bc0:e621]%ens33:123 [::]:</em> users:((“ntpd”,pid&#x3D;54823,fd&#x3D;23)) udp UNCONN 0 0 [::1]:123 [::]:* users:((“ntpd”,pid&#x3D;54823,fd&#x3D;20)) udp UNCONN 0 0 [::]:123 [::]:* users:((“ntpd”,pid&#x3D;54823,fd&#x3D;17))</p></blockquote></blockquote><p>配置完成后三台主机都需要重启</p><blockquote><blockquote><blockquote></blockquote><p>   shutdown -r 0</p></blockquote></blockquote><p>三台主机同时执行（注：此过程需要5分钟左右）</p><blockquote><blockquote><blockquote></blockquote><p>   ntpstat</p></blockquote></blockquote><p>三、ssh免密钥登陆<br>SSH免密钥登陆可以更加方便的实现不同计算机之间的连接和切换<br>master 生成公钥私钥 (一路回车)<br>ssh-keygen </p><blockquote><blockquote><blockquote></blockquote><p>   #结果显示：<br>    [root@master .ssh]# ssh-keygen Generating public&#x2F;private rsa key pair.<br>    Enter file in which to save the key (&#x2F;root&#x2F;.ssh&#x2F;id_rsa):<br>    Enter passphrase (empty for no passphrase):<br>    Enter same passphrase again:<br>    Your identification has been saved in &#x2F;root&#x2F;.ssh&#x2F;id_rsa.<br>    Your public key has been saved in &#x2F;root&#x2F;.ssh&#x2F;id_rsa.pub. T<br>    he key fingerprint is: SHA256:QUAgFH5KBc&#x2F;Erlf1JWSBbKeEepPJqMBqpWbc02&#x2F;uFj8 root@master The key’s randomart image is:<br>    +—[RSA 2048]—-+<br>    | .&#x3D;++oo+.o+.     |<br>    | . <em>. ..</em>.o .    |<br>    |. o.++ *.+ o     |</p></blockquote></blockquote><pre><code>|.o ++ B ...      | |o.=o.o .S        ||.*oo.. .         | |+ .. . o         | | + E             | | =o .            | +----[SHA256]-----+</code></pre><p>查看隐藏的 .ssh 文件</p><blockquote><blockquote><blockquote></blockquote><p>   la -al .ssh </p></blockquote></blockquote><pre><code># 结果显示 [root@master ~]# ls -al .ssh/ 总用量 16 drwx------ 2 root root 80 3月 10 21:52 . dr-xr-x---. 4 root root 175 3月 10 21:45 .. -rw------- 1 root root 393 3月 10 21:52 authorized_keys -rw------- 1 root root 1675 3月 10 21:48 id_rsa -rw-r--r-- 1 root root 393 3月 10 21:48 id_rsa.pub -rw-r--r-- 1 root root 366 3月 10 21:54 known_hosts</code></pre><p>master 配置免密登录到master slave1 slave2</p><blockquote><blockquote><blockquote></blockquote><p>   ssh-copy-id master<br>    ssh-copy-id slave1<br>    ssh-copy-id slave2</p></blockquote></blockquote><p>四、安装配置 jdk<br>编译环境软件安装目录</p><blockquote><blockquote><blockquote></blockquote><p>   mkdir -p &#x2F;export&#x2F;server</p></blockquote></blockquote><p>JDK 1.8安装 上传 jdk-8u241-linux-x64.tar.gz到&#x2F;export&#x2F;server&#x2F;目录下 并解压文件</p><blockquote><blockquote><blockquote></blockquote><p>   tar -zxvf jdk-8u241-linux-x64.tar.gz</p></blockquote></blockquote><p>配置环境变量</p><blockquote><blockquote><blockquote></blockquote><p>   vim &#x2F;etc&#x2F;profile export JAVA_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;jdk1.8.0_241 export PATH&#x3D;$PATH:$JAVA_HOME&#x2F;bin export CLASSPATH&#x3D;.:$JAVA_HOME&#x2F;lib&#x2F;dt.jar:$JAVA_HOME&#x2F;lib&#x2F;tools.jar</p></blockquote></blockquote><p>重新加载环境变量文件</p><blockquote><blockquote><blockquote></blockquote><p>   source &#x2F;etc&#x2F;profile</p></blockquote></blockquote><p>查看 java 版本号</p><blockquote><blockquote><blockquote><p>java -version 结果显示： [root@master jdk1.8.0_241]# java -version java version “1.8.0_241” Java(TM) SE Runtime Environment (build 1.8.0_241-b07) Java HotSpot(TM) 64-Bit Server VM (build 25.241-b07, mixed mode)</p></blockquote></blockquote></blockquote><p>master 节点将 java 传输到 slave1 和 slave2</p><blockquote><blockquote><blockquote></blockquote><p>   scp -r &#x2F;export&#x2F;server&#x2F;jdk1.8.0_241&#x2F; root@slave1:&#x2F;export&#x2F;server&#x2F; scp -r &#x2F;export&#x2F;server&#x2F;jdk1.8.0_241&#x2F; root@slave2:&#x2F;export&#x2F;server&#x2F;</p></blockquote></blockquote><p>配置 slave1 和 slave2 的 jdk 环境变量（注：和上方 master 的配置方法一样）<br>在 master slave1 和slave2 创建软连接</p><blockquote><blockquote><blockquote></blockquote><p>   cd &#x2F;export&#x2F;server<br>    ln -s jdk1.8.0_241&#x2F; jdk</p></blockquote></blockquote><p>重新加载环境变量文件</p><blockquote><blockquote><blockquote></blockquote><p>   source &#x2F;etc&#x2F;profile</p></blockquote></blockquote><p>zookeeper安装配置<br>配置主机名和IP的映射关系，修改 &#x2F;etc&#x2F;hosts 文件，添加 master.root slave1.root slave2.root</p><blockquote><blockquote><blockquote></blockquote><pre><code>vim /etc/hosts #结果显示 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 </code></pre></blockquote></blockquote><pre><code>192.168.88.135 master master.root 192.168.88.136 slave1 slave1.root 192.168.88.137 slave2 slave2.root</code></pre><p>zookeeper安装 上传 zookeeper-3.4.10.tar.gz到&#x2F;export&#x2F;server&#x2F;目录下 并解压文件</p><blockquote><blockquote><blockquote></blockquote><p>   cd &#x2F;export&#x2F;server&#x2F; tar -zxvf zookeeper-3.4.10.tar.gz</p></blockquote></blockquote><p>在 &#x2F;export&#x2F;server 目录下创建软连接</p><blockquote><blockquote><blockquote><p>cd &#x2F;export&#x2F;server ln -s zookeeper-3.4.10&#x2F; zookeeper</p></blockquote></blockquote></blockquote><p>进入 &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;conf&#x2F; 将 zoo_sample.cfg 文件复制为新文件 zoo.cfg</p><blockquote><blockquote><blockquote><p>cd &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;conf&#x2F; cp zoo_sample.cfg zoo.cfg</p></blockquote></blockquote></blockquote><p>接上步给 zoo.cfg 添加内容</p><blockquote><blockquote><blockquote><p>#Zookeeper的数据存放目录 dataDir&#x3D;&#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas<br>    # 保留多少个快照 autopurge.snapRetainCount&#x3D;3<br>    # 日志多少小时清理一次 autopurge.purgeInterval&#x3D;1<br>    # 集群中服务器地址 server.1&#x3D;master:2888:3888 server.2&#x3D;slave1:2888:3888 server.3&#x3D;slave2:2888:3888</p></blockquote></blockquote></blockquote><p>进入 &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas 目录在此目录下创建 myid 文件，将 1 写入进去</p><blockquote><blockquote><blockquote><p> cd &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdata touch myid echo ‘1’ &gt; myid</p></blockquote></blockquote></blockquote><p>将 master 节点中 &#x2F;export&#x2F;server&#x2F;zookeeper-3.4.10 路径下内容推送给slave1 和 slave2</p><blockquote><blockquote><blockquote><p>scp -r &#x2F;export&#x2F;server&#x2F;zookeeper-3.4.10&#x2F; slave1:$PWD scp -r &#x2F;export&#x2F;server&#x2F;zookeeper-3.4.10&#x2F; slave2:$PWD</p></blockquote></blockquote></blockquote><p>推送成功后，分别在 slave1 和 slave2 上创建软连接</p><blockquote><blockquote><blockquote><p>ln -s zookeeper-3.4.10&#x2F; zookeeper</p></blockquote></blockquote></blockquote><p>接上步推送完成后将 slave1 和 slave2 的 &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas&#x2F; 文件夹下的 myid中的内容分别改为 2 和3</p><blockquote><blockquote><blockquote></blockquote><p>   cd &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas&#x2F; 结果显示： [root@slave1 zkdatas]# vim myid [root@slave1 zkdatas]# more myid 2[root@slave2 zkdatas]# vim myid [root@slave2 zkdatas]# more myid 3</p></blockquote></blockquote><p>配置zookeeper的环境变量（注：三台主机都需要配置）</p><blockquote><blockquote><blockquote><p> vim &#x2F;etc&#x2F;profile # zookeeper 环境变量 export ZOOKEEPER_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;zookeeper export PATH&#x3D;$PATH:$ZOOKEEPER_HOME&#x2F;bin</p></blockquote></blockquote></blockquote><p>重新加载环境变量文件</p><blockquote><blockquote><blockquote><p>source &#x2F;etc&#x2F;profile</p></blockquote></blockquote></blockquote><p>进入 &#x2F;export&#x2F;server&#x2F;zookeeper-3.4.10&#x2F;bin 目录下启动 zkServer.sh 脚本 （注：三台都需要做）</p><blockquote><blockquote><blockquote><p>cd &#x2F;export&#x2F;server&#x2F;zookeeper-3.4.10&#x2F;bin zkServer.sh start<br>    结果显示： [root@master bin]# .&#x2F;zkServer.sh start ZooKeeper JMX enabled by default Using config: &#x2F;export&#x2F;server&#x2F;zookeeper-3.4.10&#x2F;bin&#x2F;..&#x2F;conf&#x2F;zoo.cfg Starting zookeeper … STARTED</p></blockquote></blockquote></blockquote><p>zookeeper 的状态</p><blockquote><blockquote><blockquote></blockquote><pre><code>zkServer.sh status 结果显示： [root@master server]# zkServer.sh status ZooKeeper JMX enabled by default Using config: /export/server/zookeeper-3.4.10/bin/../conf/zoo.cfg Mode: follower [root@slave1 server]# zkServer.sh status ZooKeeper JMX enabled by default Using config: /export/server/zookeeper-3.4.10/bin/../conf/zoo.cfg Mode: follower [root@slave2 conf]# zkServer.sh status ZooKeeper JMX enabled by default Using config: /export/server/zookeeper-3.4.10/bin/../conf/zoo.cfg Mode: leader</code></pre></blockquote></blockquote><p>jps 结果显示： </p><blockquote><blockquote><blockquote></blockquote><p>   [root@master server]# jps 125348 QuorumPeerMain 16311 Jps [root@slave1 server]# jps 126688 QuorumPeerMain 17685 Jps [root@slave2 conf]# jps 126733 QuorumPeerMain 17727 Jps</p></blockquote></blockquote><p>脚本一键启动</p><blockquote><blockquote><blockquote></blockquote><p>   vim zkServer.sh<br>  #!&#x2F;bin&#x2F;bash<br>  if [ $# -eq 0 ] ;<br>  then<br>       echo “please input param:start stop”<br>else<br>if [ $1 &#x3D; start ] ;then<br>   echo “${1}ing master”<br>   ssh master “source &#x2F;etc&#x2F;profile;&#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;bin&#x2F;zkServer.sh start”<br>         for i in {1..2}<br>          do<br>             echo “${1}ping slave${i}” </p></blockquote></blockquote><pre><code>         ssh slave$&#123;i&#125; &quot;source /etc/profile;/export/server/zookeeper/bin/zkServer.sh start&quot;     done </code></pre><p>fi<br>if [ $1 &#x3D; stop ];then<br>    echo “${1}ping master “<br>    ssh master “source &#x2F;etc&#x2F;profile;&#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;bin&#x2F;zkServer.sh stop”<br>    for i in {1..2}<br>    do<br>       echo “${1}ping slave${i}” ssh slave${i} “source &#x2F;etc&#x2F;profile;&#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;bin&#x2F;zkServer.sh stop”<br>    done<br>fi<br>if [ $1 &#x3D; status ];then<br>    echo “${1}ing master”<br>    ssh master “source &#x2F;etc&#x2F;profile;&#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;bin&#x2F;zkServer.sh status”<br>    for i in {1..2}<br>    do<br>       echo “${1}ping slave${i}”<br>        ssh slave${i} “source &#x2F;etc&#x2F;profile;<br>&#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;bin&#x2F;zkServer.sh status”<br>   done<br> fi<br> fi<br> #将文件放在 &#x2F;bin 目录下 chmod +x zkServer-all.sh &amp;&amp; zkServer-all.sh</p><p>Hadoop 安装配置<br>把 hadoop-3.3.0-Centos7-64-with-snappy.tar.gz 上传到 &#x2F;export&#x2F;server 并解压文件</p><blockquote><blockquote><blockquote><p>tar -zxvf hadoop-3.3.0-Centos7-64-with-snappy.tar.gz</p></blockquote></blockquote></blockquote><pre><code>修改配置文件(进入路径 /export/server/hadoop-3.3.0/etc/hadoop)cd /export/server/hadoop-3.3.0/etc/hadoophadoop-env.sh#文件最后添加 export JAVA_HOME=/export/server/jdk1.8.0_241 export HDFS_NAMENODE_USER=root export HDFS_DATANODE_USER=root export HDFS_SECONDARYNAMENODE_USER=root export YARN_RESOURCEMANAGER_USER=root export YARN_NODEMANAGER_USER=rootcore-site.xml&lt;!-- 设置默认使用的文件系统 Hadoop支持file、HDFS、GFS、ali|Amazon云等文件系统 - -&gt;&lt;property&gt;          &lt;name&gt;fs.defaultFS&lt;/name&gt;          &lt;value&gt;hdfs://master:8020&lt;/value&gt;          &lt;/property&gt; &lt;!-- 设置Hadoop本地保存数据路径 --&gt; &lt;property&gt;     &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;     &lt;value&gt;/export/data/hadoop-3.3.0&lt;/value&gt; &lt;/property&gt; &lt;!-- 设置HDFS web UI用户身份 --&gt; &lt;property&gt;      &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt;      &lt;value&gt;root&lt;/value&gt; &lt;/property&gt; </code></pre><!-- 整合hive 用户代理设置 --><property>     <name>hadoop.proxyuser.root.hosts</name>     <value>*</value>     </property>     <property>      <name>hadoop.proxyuser.root.groups</name>      <value>*</value>      </property> <     !-- 文件系统垃圾桶保存时间 --> <property>      <name>fs.trash.interval</name>      <value>1440</value>     </property>    hdfs-site.xml    <!-- 设置SNN进程运行机器位置信息 -->     <property>     <name>dfs.namenode.secondary.http-address</name> <value>slave1:9868</value>     </property>    mapred-site.xml    <!-- 设置MR程序默认运行模式： yarn集群模式 local本地模式 --> <property>     <name>mapreduce.framework.name</name>     <value>yarn</value>     </property>     <!-- MR程序历史服务地址 -->     <property>     <name>mapreduce.jobhistory.address</name>     <value>master:10020</value>     </property>     <!-- MR程序历史服务器web端地址 -->     <property>     <name>mapreduce.jobhistory.webapp.address</name> <value>master:19888</value>     </property>     <property>     <name>yarn.app.mapreduce.am.env</name>     <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>     </property>     <property>     <name>mapreduce.map.env</name>     <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>     </property>     <property>     <name>mapreduce.reduce.env</name>    <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>     </property>yarn-site.xml<!-- 设置YARN集群主角色运行机器位置 --> <property> <name>yarn.resourcemanager.hostname</name> <value>master</value> </property> <property> <name>yarn.nodemanager.aux-services</name>  <value>mapreduce_shuffle</value>  </property>  <!-- 是否将对容器实施物理内存限制 -->   <property>   <name>yarn.nodemanager.pmem-check-enabled</name>    <value>false</value>    </property>     <!-- 是否将对容器实施虚拟内存限制。 -->      <property>      <name>yarn.nodemanager.vmem-check-enabled</name>      <value>false</value>       </property>       <!-- 开启日志聚集 -->        <property> <name>yarn.log-aggregation-enable</name> <value>true</value>        </property>        <!-- 设置yarn历史服务器地址 -->         <property>         <name>yarn.log.server.url</name>         <value>http://master:19888/jobhistory/logs</value>         </property>         <!-- 历史日志保存的时间 7天 -->         <property>         <name>yarn.log-aggregation.retain-seconds</name> <value>604800</value>         </property><pre><code>     node1    node2      node3</code></pre><p>分发同步hadoop安装包</p><blockquote><blockquote><blockquote></blockquote><p>   cd &#x2F;export&#x2F;server<br>     scp -r hadoop-3.3.0 root@slave1:$PWD<br>     scp -r hadoop-3.3.0 root@slave2:$PWD<br>将hadoop添加到环境变量</p><blockquote></blockquote><p>   vim &#x2F;etc&#x2F;profile export HADOOP_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;hadoop-3.3.0 export PATH&#x3D;$PATH:$HADOOP_HOME&#x2F;bin:$HADOOP_HOME&#x2F;sbin</p></blockquote></blockquote><p>重新加载环境变量文件</p><blockquote><blockquote><blockquote></blockquote><p>   source &#x2F;etc&#x2F;profile</p></blockquote></blockquote><pre><code>Hadoop集群启动格式化namenode（只有首次启动需要格式化）hdfs namenode -format</code></pre><p>脚本一键启动<br>    [root@master ~]# start-dfs.sh Starting namenodes on [master] 上一次登录：五 3月 11 21:27:24 CST 2022pts&#x2F;0 上 Starting datanodes 上一次登录：五 3月 11 21:27:32 CST 2022pts&#x2F;0 上 Starting secondary namenodes [slave1] 上一次登录：五 3月 11 21:27:35 CST 2022pts&#x2F;0 上 </p><pre><code>[root@master ~]# start-yarn.sh Starting resourcemanager 上一次登录：五 3月 11 21:27:41 CST 2022pts/0 上 Starting nodemanagers 上一次登录：五 3月 11 21:27:51 CST 2022pts/0 上启动后 输入 jps 查看[root@master ~]# jps 127729 NameNode 127937 DataNode 14105 Jps 128812 NodeManager 128591 ResourceManager [root@slave1 hadoop]# jps 121889 NodeManager 121559 SecondaryNameNode 7014 Jps 121369 DataNode [root@slave2 hadoop]# jps 6673 Jps 121543 NodeManager 121098 DataNode</code></pre><p>WEB页面<br>HDFS集群：</p><blockquote><blockquote><blockquote></blockquote><pre><code>http://master:9870/</code></pre></blockquote></blockquote><p>YARN集群：</p><blockquote><blockquote><blockquote><p><a href="http://master:9870/">http://master:9870/</a></p></blockquote></blockquote></blockquote>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>《Spark HA &amp; Yarn配置》</title>
      <link href="/2022/05/22/3/"/>
      <url>/2022/05/22/3/</url>
      
        <content type="html"><![CDATA[<h1 id="《Spark-HA-amp-Yarn配置》"><a href="#《Spark-HA-amp-Yarn配置》" class="headerlink" title="《Spark HA &amp; Yarn配置》"></a>《Spark HA &amp; Yarn配置》</h1><p>Spark-Standalone-HA模式<br>Spark Standalone集群是Master-Slaves架构的集群模式,和大部分的Master-Slaves结构集群一样,存在<br>着Master 单点故障(SPOF)的问题。简单理解为，spark-Standalone 模式下为 master 节点控制其他节<br>点，当 master 节点出现故障时，集群就不可用了。 spark-Standalone-HA 模式下<br>master 节点不固定，当一个宕机时，立即换另一台为 master 保障不出现故障。<br>此处因为先前配置时的 zookeeper 版本和 spark 版本不太兼容，导致此模式有故障，需要重新下<br>载配置新的版本的 zookeeper<br>配置之前需要删除三台主机的 旧版 zookeeper 以及 对应的软连接<br>在 master 节点上重新进行前面配置的 zookeeper 操作</p><blockquote><blockquote><blockquote></blockquote><p>   1.上传apache-zookeeper-3.7.0-bin.tar.gz 到&#x2F;export&#x2F;server&#x2F;目录下 并解压文件 2.在 &#x2F;export&#x2F;server 目录下创建软连接 3.进入 &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;conf&#x2F; 将 zoo_sample.cfg 文件复制为新文件 zoo.cfg 4.接上步给 zoo.cfg 添加内容 5.进入 &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas 目录在此目录下创建 myid 文件，将 1 写入进 去6.将 master 节点中 &#x2F;export&#x2F;server&#x2F;zookeeper-3.7.0 路径下内容推送给slave1 和 slave2 7.推送成功后，分别在 slave1 和 slave2 上创建软连接 8.接上步推送完成后将 slave1 和 slave2 的 &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas&#x2F;文件夹 下的 myid 中的内容分别改为 2 和 3 配置环境变量： 因先前配置 zookeeper 时候创建过软连接且以 ’zookeeper‘ 为路径，所以不用配置环境变量，此 处也是创建软连接的方便之处</p></blockquote></blockquote><p>进入 &#x2F;export&#x2F;server&#x2F;spark&#x2F;conf 文件夹 修改 spark-env.sh 文件内容</p><blockquote><blockquote><blockquote></blockquote><p>   cd &#x2F;export&#x2F;server&#x2F;spark&#x2F;conf<br>    vim spark-env.sh</p></blockquote></blockquote><p>为 83 行内容加上注释，此部分原为指定 某台主机 做 master ，加上注释后即为 任何主机都<br>可以做 master</p><blockquote><blockquote><blockquote></blockquote><p>   结果显示：<br>     ……<br>      82 # 告知Spark的master运行在哪个机器上 83 # export SPARK_MASTER_HOST&#x3D;master<br>     ………</p></blockquote></blockquote><p>文末添加内容</p><blockquote><blockquote><blockquote><p>SPARK_DAEMON_JAVA_OPTS&#x3D;”-Dspark.deploy.recoveryMode&#x3D;ZOOKEEPER -<br>       Dspark.deploy.zookeeper.url&#x3D;master:2181,slave1:2181,slave2:2181 - Dspark.deploy.zookeeper.dir&#x3D;&#x2F;spark-ha”<br>        #spark.deploy.recoveryMode<br>        指定HA模式 基于Zookeeper实现<br>        #指定Zookeeper的连接地址<br>        #指定在Zookeeper中注册临时节点的路径</p></blockquote></blockquote></blockquote><p>分发 spark-env.sh 到 salve1 和 slave2 上</p><blockquote><blockquote><blockquote></blockquote><pre><code>scp spark-env.sh slave1:/export/server/spark/conf/ scp spark-env.sh slave2:/export/server/spark/conf/</code></pre></blockquote></blockquote><p>启动之前确保 Zookeeper 和 HDFS 均已经启动<br>启动集群:</p><blockquote><blockquote><blockquote></blockquote><p>   #在 master 上 启动一个master 和全部worker &#x2F;export&#x2F;server&#x2F;spark&#x2F;sbin&#x2F;start-all.sh # 注意, 下面命令在 slave1 上执行 启动 slave1 上的 master 做备用 master &#x2F;export&#x2F;server&#x2F;spark&#x2F;sbin&#x2F;start-master.sh<br>    结果显示：<br>    (base) [root@master ~]# jps<br>    37328 DataNode<br>    41589 Master<br>    35798 QuorumPeerMain<br>    38521 ResourceManager<br>    46281 Jps<br>    38907 NodeManager<br>    41821 Worker<br>    36958 NameNode (base)<br>    [root@slave1 sbin]# jps<br>    36631 DataNode<br>    48135 Master<br>    35385 QuorumPeerMain<br>    37961 NodeManager<br>    40970 Worker<br>    48282 Jps<br>    37276 SecondaryNameNode</p></blockquote></blockquote><p>访问 WebUI 界面</p><blockquote><blockquote><blockquote></blockquote><pre><code>http://master:8081/</code></pre><p>   <a href="http://slave1:8082/">http://slave1:8082/</a></p></blockquote></blockquote><p>此时 kill 掉 master 上的 master 假设 master 主机宕机掉</p><blockquote><blockquote><blockquote></blockquote><p>   #master主机 master 的进程号 kill -9 41589 结果显示： (base) [root@master ~]# jps 37328 DataNode 90336 Jps 35798 QuorumPeerMain 38521 ResourceManager 38907 NodeManager 41821 Worker 36958 NameNode</p></blockquote></blockquote><p>访问 slave1 的 WebUI</p><blockquote><blockquote><blockquote><p><a href="http://slave1:8082/">http://slave1:8082/</a></p></blockquote></blockquote></blockquote><p>进行主备切换的测试<br>提交一个 spark 任务到当前 活跃的 master上 :</p><blockquote><blockquote><blockquote><p>&#x2F;export&#x2F;server&#x2F;spark&#x2F;bin&#x2F;spark-submit –master spark:&#x2F;&#x2F;master:7077 &#x2F;export&#x2F;server&#x2F;spark&#x2F;examples&#x2F;src&#x2F;main&#x2F;python&#x2F;pi.py 1000</p></blockquote></blockquote></blockquote><p>复制标签 kill 掉 master 的 进程号<br>再次访问 master 的 WebUI</p><blockquote><blockquote><blockquote><p><a href="http://master:8081/">http://master:8081/</a><br>      网页访问不了！</p></blockquote></blockquote></blockquote><p>再次访问 slave1 的 WebUI</p><blockquote><blockquote><blockquote><p><a href="http://slave1:8082/">http://slave1:8082/</a></p></blockquote></blockquote></blockquote><p>可以看到当前活跃的 master 提示信息</p><blockquote><blockquote><blockquote><p>(base) [root@master ~]# &#x2F;export&#x2F;server&#x2F;spark&#x2F;bin&#x2F;spark-submit –master spark:&#x2F;&#x2F;master:7077 &#x2F;export&#x2F;server&#x2F;spark&#x2F;examples&#x2F;src&#x2F;main&#x2F;python&#x2F;pi.py 1000 22&#x2F;03&#x2F;29 16:11:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicable 22&#x2F;03&#x2F;29 16:12:16 WARN StandaloneAppClient$ClientEndpoint: Connection to master:7077 failed; waiting for master to reconnect… 22&#x2F;03&#x2F;29 16:12:16 WARN StandaloneSchedulerBackend: Disconnected from Spark cluster! Waiting for reconnection… 22&#x2F;03&#x2F;29 16:12:16 WARN StandaloneAppClient$ClientEndpoint: Connection to master:7077 failed; waiting for master to reconnect… Pi is roughly 3.140960 (base) [root@master ~]#</p></blockquote></blockquote></blockquote><p>Spark On YARN模式</p><blockquote><blockquote><blockquote><p>在已有YARN集群的前提下在单独准备Spark StandAlone集群,对资源的利用就不高.Spark On YARN, 无</p></blockquote></blockquote></blockquote><p>需部署Spark集群, 只要找一台服务器, 充当Spark的客户端<br>保证 HADOOP_CONF_和 DIR_YARN_CONF_DIR 已经配置在 spark-env.sh 和环境变量中 （注: 前面配置spark-Standlone 时已经配置过此项了）</p><blockquote><blockquote><blockquote><p>spark-env.sh 文件部分显示： …. 77 ## HADOOP软件配置文件目录，读取HDFS上文件和运行YARN集群 78 HADOOP_CONF_DIR&#x3D;&#x2F;export&#x2F;server&#x2F;hadoop&#x2F;etc&#x2F;hadoop 79 YARN_CONF_DIR&#x3D;&#x2F;export&#x2F;server&#x2F;hadoop&#x2F;etc&#x2F;hadoop ….</p></blockquote></blockquote></blockquote><p>链接到 YARN 中（注: 交互式环境 pyspark 和 spark-shell 无法运行 cluster模式）<br>bin&#x2F;pyspark –master yarn –deploy-mode client|cluster # –deploy-mode 选项是指定部署模式, 默认是 客户端模式 # client就是客户端模式 # cluster就是集群模式 # –deploy-mode 仅可以用在YARN模式下<br> bin&#x2F;spark-shell –master yarn –deploy-mode client|cluster<br>bin&#x2F;spark-submit –master yarn –deploy-mode client|cluster &#x2F;xxx&#x2F;xxx&#x2F;xxx.py 参数</p><blockquote><blockquote><blockquote></blockquote><p>  spark-submit 和 spark-shell 和 pyspark的相关参数</p></blockquote></blockquote><ul><li>bin&#x2F;pyspark: pyspark解释器spark环境 - bin&#x2F;spark-shell: scala解释器spark环境 - bin&#x2F;spark-submit: 提交jar包或Python文件执行的工具 - bin&#x2F;spark-sql: sparksql客户端工具<br>  这4个客户端工具的参数基本通用.以spark-submit 为例: bin&#x2F;spark-submit –master spark:&#x2F;&#x2F;master:7077 xxx.py&#96;<blockquote><blockquote><blockquote></blockquote><p>  Usage: spark-submit [options] &lt;app jar | python file | R file&gt; [app arguments]<br>   Usage: spark-submit –kill [submission ID] –master [spark:&#x2F;&#x2F;…]<br>   Usage: spark-submit –status [submission ID] –master [spark:&#x2F;&#x2F;…]<br>   Usage: spark-submit run-example [options] example-class [example args] </p><blockquote></blockquote><p>   Options: –master MASTER_URL spark:&#x2F;&#x2F;host:port, mesos:&#x2F;&#x2F;host:port, yarn, k8s:&#x2F;&#x2F;<a href="https://host:port">https://host:port</a>, or </p><blockquote></blockquote><p>   local (Default: local[*]). –deploy-mode DEPLOY_MODE 部署模式 client 或者 cluster 默认是client –class CLASS_NAME 运行java或者scala class(for Java &#x2F; Scala apps). –name NAME 程序的名字 –jars JARS Comma-separated list of jars to include on the </p><blockquote></blockquote><p>   driver and executor classpaths. –packages Comma-separated list of maven coordinates of </p><blockquote></blockquote><p>   jars to include on the driver and executor classpaths. Will </p><blockquote></blockquote><p>   search the local maven repo, then maven central and any </p><blockquote></blockquote><p>   additional remote repositories given by –repositories. The </p><blockquote></blockquote><p>   format for the coordinates should be </p><blockquote></blockquote><p>   groupId:artifactId:version.<br>   –exclude-packages Comma-separated list of groupId:artifactId, to exclude while resolving the dependencies provided in<br>– packages to avoid dependency conflicts. –repositories Comma-separated list of additional remote repositories to search for the maven coordinates given with<br> – packages.<br> –py-files PY_FILES 指定Python程序依赖的其它python文件<br> –files FILES Comma-separated list of files to be placed in the working directory of each executor. File paths of these files in executors can be accessed via SparkFiles.get(fileName).<br>  –archives ARCHIVES Comma-separated list of archives to be extracted into the working directory of each executor.<br> –conf,<br> -c PROP&#x3D;VALUE 手动指定配置<br> –properties-file FILE Path to a file from which to load extra properties. If not specified, this will look for conf&#x2F;spark- defaults.conf. –driver-memory MEM Driver的可用内存(Default: 1024M). –driver-java-options Driver的一些Java选项 –driver-library-path Extra library path entries to pass to the driver. –driver-class-path Extra class path entries to pass to the driver. Note that jars added with –jars are automatically included in the classpath.<br> –executor-memory MEM Executor的内存 (Default: 1G).<br> –proxy-user NAME User to impersonate when submitting the application. This argument does not work with<br> –principal &#x2F;<br> –keytab.<br> –help,<br> -h 显示帮助文件<br>  –verbose,<br>  -v Print additional debug output. –version, 打印版本 Cluster deploy mode only(集群模式专属):<br>   –driver-cores NUM Driver可用的的CPU核数(Default: 1). Spark standalone or Mesos with cluster deploy mode only:<br>   –supervise 如果给定, 可以尝试重启Driver Spark standalone, Mesos or K8s with cluster deploy mode only:<br>   –kill SUBMISSION_ID 指定程序ID kill –status SUBMISSION_ID 指定程序ID 查看运行状态 Spark standalone, Mesos and Kubernetes only:<br>   –total-executor-cores NUM 整个任务可以给Executor多少个CPU核心用 Spark standalone, YARN and Kubernetes only:<br>    –executor-cores NUM 单个Executor能使用多少CPU核心 Spark on YARN and Kubernetes only(YARN模式下):<br>    –num-executors NUM Executor应该开启几个<br>    –principal PRINCIPAL Principal to be used to login to KDC.<br>    –keytab KEYTAB The full path to the file that contains the keytab for the principal specified above. Spark on YARN only:<br>    –queue QUEUE_NAME 指定运行的YARN队列(Default: “default”)</p></blockquote></blockquote></li></ul><p>启动 YARN 的历史服务器</p><blockquote><blockquote><blockquote><p>cd &#x2F;export&#x2F;server&#x2F;hadoop-3.3.0&#x2F;sbin .&#x2F;mr-jobhistory-daemon.sh start historyserver</p></blockquote></blockquote></blockquote><p>访问WebUI界面</p><blockquote><blockquote><blockquote><p><a href="http://master:19888/">http://master:19888/</a></p></blockquote></blockquote></blockquote><p>client 模式测试</p><blockquote><blockquote><blockquote><p>SPARK_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;spark ${SPARK_HOME}&#x2F;bin&#x2F;spark-submit –master yarn –deploy-mode client – driver-memory 512m –executor-memory 512m –num-executors 1 –total- executor-cores 2 ${SPARK_HOME}&#x2F;examples&#x2F;src&#x2F;main&#x2F;python&#x2F;pi.py</p></blockquote></blockquote></blockquote><p>cluster 模式测试</p><blockquote><blockquote><blockquote><p>SPARK_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;spark ${SPARK_HOME}&#x2F;bin&#x2F;spark-submit –master yarn –deploy-mode cluster –driver- memory 512m –executor-memory 512m –num-executors 1 –total-executor-cores 2 –conf “spark.pyspark.driver.python&#x3D;&#x2F;root&#x2F;anaconda3&#x2F;bin&#x2F;python3” –conf “spark.pyspark.python&#x3D;&#x2F;root&#x2F;anaconda3&#x2F;bin&#x2F;python3” ${SPARK_HOME}&#x2F;examples&#x2F;src&#x2F;main&#x2F;python&#x2F;pi.py 3**</p></blockquote></blockquote></blockquote>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
