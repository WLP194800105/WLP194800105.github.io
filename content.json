{"meta":{"title":"Hexo","subtitle":"","description":"This is wlp194800105_blog","author":"WLP","url":"http://example.com","root":"/"},"pages":[],"posts":[{"title":"Kafka生产者API示例","slug":"Kafka生产者API示例","date":"2022-06-18T02:33:41.000Z","updated":"2022-06-18T02:35:38.399Z","comments":true,"path":"2022/06/18/Kafka生产者API示例/","link":"","permalink":"http://example.com/2022/06/18/Kafka%E7%94%9F%E4%BA%A7%E8%80%85API%E7%A4%BA%E4%BE%8B/","excerpt":"Kafka生产者API示例1、Kafka生产者API示例1.1 生产者API示例一个正常的生产逻辑需要具备以下几个步骤 1234(1)配置生产者客户端参数及创建相应的生产者实例(2)构建待发送的消息(3)发送消息(4)关闭生产者实例","text":"Kafka生产者API示例1、Kafka生产者API示例1.1 生产者API示例一个正常的生产逻辑需要具备以下几个步骤 1234(1)配置生产者客户端参数及创建相应的生产者实例(2)构建待发送的消息(3)发送消息(4)关闭生产者实例 示例代码（部分截取） 12345678910111213141516171819202122232425262728Properties props = new Properties(); //设置 kafka 集群的地址props.put(&quot;bootstrap.servers&quot;, &quot;node1:9092,node2:9092,node3:9092&quot;);//ack 模式,取值有 0,1,-1(all) , all 是最慢但最安全的，props.put(“acks”, “all”); //失败重试次数-&gt;失败会自动重试（可恢复/不可恢复）--&gt;(有可能会造成数据的乱序)props.put(“retries”, 3); //数据发送的批次大小提高效率/吞吐量太大会数据延迟props.put(“batch.size”, 10); //消息在缓冲区保留的时间,超过设置的值就会被提交到服务端props.put(&quot;linger.ms&quot;, 10000); //数据发送请求的最大缓存数props.put(&quot;max.request.size&quot;,10); //整个 Producer 用到总内存的大小,如果缓冲区满了会提交数据到服务端 buffer.memory 要大于 batch.size,否则会报申请内存不足的错误降低阻塞的可能性props.put(“buffer.memory”, 10240);//key-value序列化器props.put(&quot;key.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;); //字符串最好props.put(“value.serializer”, “org.apache.kafka.common.serialization.StringSerializer”); 消息对象 ProducerRecord,它并不是单纯意义上的消息,它包含了多个属性,原本需要发送的与业务关的消息体只是其中的一个 value 属性 ,比“ Hello, rgzn!”只是 ProducerRecord 对象的一个属性。 123456789ProducerRecord 类的定义如下:public class ProducerRecord&lt;K, V&gt; &#123; private final String topic; private final Integer partition; private final Headers headers; private final K key; private final V value; private final Long timestamp;&#125; 1.2 必要参数配置 在创建真正的生产者实例前需要配置相应的参数,比如需要连接的 Kafka 集群地址。在 Kafka 生产者客户端 KatkaProducer 中有 3 个参数是必填的。 123* bootstrap.servers * key.serializer * value.serializer 为了防止参数名字符串书写错误,可以使用如下方式进行设置: 123props.setProperty(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG,ProducerInterceptorPrefix.class.getName());props.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,&quot;node1:9092,node2:9092&quot;); props.setProperty(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,StringSerializer.class.getName()); props.setProperty(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,StringSerializer.class.getName()); 1.3 生产者API参数发送方式这个客户端经过了生产环境测试并且通常情况它比原来Scals客户端更加快速、功能更加齐全。你可以通过添加以下示例的Maven坐标到客户端依赖中来使用这个新的客户端（你可以修改版本号来使用新的发布版本）： 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; &lt;version&gt;0.10.0.0&lt;/version&gt;&lt;/dependency&gt; 1.3.1 发后即忘( fire-and-forget)发后即忘,它只管往 Kafka 发送,并不关心消息是否正确到达。在大多数情况下,这种发送方式没有问题; 不过在某些时候(比如发生不可重试异常时)会造成消息的丢失。这种发送方式的性能最高,可靠性最差。 1Future&lt;RecordMetadata&gt; send = producer.send(rcd); 1.3.2 同步发送(sync )0.8.x 前,有一个参数 producer.type&#x3D;sycn|asycn 来决定生产者的发送模式;现已失效(新版中,producer 在底层只有异步) 12345try &#123; producer.send(rcd).get(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; 在调用 send 方法后可以接着调用 get() 方法，send 方法的返回值是一个 Future\\对象，RecordMetadata 里面包含了发送消息的主题、分区、偏移量等信息。改写后的代码如下： 123456789101112for (int i = 0; i &lt; 10; i++) &#123; try &#123; ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;&gt;(topicName, &quot;k&quot; + i, &quot;world&quot; + i); /*同步发送消息*/ RecordMetadata metadata = producer.send(record).get(); System.out.printf(&quot;topic=%s, partition=%d, offset=%s \\n&quot;, metadata.topic(), metadata.partition(), metadata.offset()); &#125; catch (InterruptedException | ExecutionException e) &#123; e.printStackTrace(); &#125;&#125; 此时得到的输出如下：偏移量和调用次数有关，所有记录都分配到了 0 分区，这是因为在创建 Hello-Kafka 主题时候，使用 --partitions 指定其分区数为 1，即只有一个分区。 1234567891011topic=Hello-Kafka, partition=0, offset=40 topic=Hello-Kafka, partition=0, offset=41 topic=Hello-Kafka, partition=0, offset=42 topic=Hello-Kafka, partition=0, offset=43 topic=Hello-Kafka, partition=0, offset=44 topic=Hello-Kafka, partition=0, offset=45 topic=Hello-Kafka, partition=0, offset=46 topic=Hello-Kafka, partition=0, offset=47 topic=Hello-Kafka, partition=0, offset=48 topic=Hello-Kafka, partition=0, offset=49 1.3.3 异步发送(async )回调函数会在 producer 收到 ack 时调用,为异步调用,该方法有两个参数,分别是 RecordMetadata 和Exception,如果 Exception 为 null,说明消息发送成功,如果 Exception 不为 null,说明消息发送失败。 12注意:消息发送失败会自动重试,不需要我们在回调函数中手动重试 通常我们并不关心发送成功的情况，更多关注的是失败的情况，因此 Kafka 提供了异步发送和回调函数。 代码如下： 12345678910111213141516for (int i = 0; i &lt; 10; i++) &#123; ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;&gt;(topicName, &quot;k&quot; + i, &quot;world&quot; + i); /*异步发送消息，并监听回调*/ producer.send(record, new Callback() &#123; @Override public void onCompletion(RecordMetadata metadata, Exception exception) &#123; if (exception != null) &#123; System.out.println(&quot;进行异常处理&quot;); &#125; else &#123; System.out.printf(&quot;topic=%s, partition=%d, offset=%s \\n&quot;, metadata.topic(), metadata.partition(), metadata.offset()); &#125; &#125; &#125;);&#125; 1.4 生产者原理解析 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556570 新建kafka生产实例，参数也是放在kafkaProducer里面1 Producerinterceptor拦截器，设置特定的规则对消息进行拦截，可以通过指定的消息2 Serializer序列化器，创建生产者对象时必须指定序列化器，作用就是将key和value转换为二进制3 Partitioner，topic中有分区，如何分发就是通过此处有规划的分发数据4.1 RecordAccumulator消息累加器，其中有多个分区，对于每个分区，都会单独维护主要用来缓存消息以便 Sender 线程可以批量发送, 进而减少网络传输的资源消耗以提升性能。4.2 RecordAccumulator 缓存的大小可以通过生产者客户端参数 buffer.memory 配 置, 默认值为 33554432B ,即 32M。4.3 如果生产者发送消息的速度超过发送到服务器的速度,则会导致生产者空间不足,这个时KafkaProducer.send()方法调用要么被阻塞,要么抛出异常,这个取决于参数max.block.ms 的配置,此参数的默认值为 60000,即 60 秒。（此配置可以理解为阻塞时间，在这个范围内不会抛出异常）4.4 主线程中发送过来的消息都会被迫加到 RecordAccumulator 的某个双端队列(Deque )中,RecordAccumulator 内部为每个分区都维护了一个双端队列,即Deque&lt;ProducerBatch&gt;。消息写入缓存时,追加到双端队列的尾部;4.5 Sender 读取消息时,从双端队列的头部读取。4.6 注意:ProducerBatch 是指一个消息批次; 与此同时,会将较小的 ProducerBatch凑成一个较ProducerBatch ,也可以减少网络请求的次数以提升整体的吞吐量。4.7 ProducerBatch 大小和 batch.size 参数也有着密切的关系。4.8 当一条消息(ProducerRecord ) 流入RecordAccumulator 时,会先寻找与消息分区所对应的双端队列(如果没有则新建),再从这个双端队列的尾部获取一个ProducerBatch (如果没有则新建),查看 ProducerBatch 中是否还可以写入这个ProducerRecord,如果可以写入,如果不可以则需要创建一个新的 Producer Batch。4.9 在新建ProducerBatch 时评估这条消息的大小是否超过 batch.size 参数大小, 如果不超过, 那么就以 batch.size 参数的大小来创建 ProducerBatch。4.10 如果生产者客户端需要向很多分区发送消息, 则可以将 buffer.memory 参数适当调大以增加整体的吞吐量6.1 Sender 从 RecordAccumulator 获取缓存的消息之后,会进一步将&lt;分区,Deque&lt;Producer Batch&gt;&gt;的形式转变成&lt;Node,List&lt;ProducerBatch&gt;的形式,其中 Node 表示 Kafka 集群 broker 节点。6.2 对于网络连接来说,生产者客户端是与具体 broker 节点建立的连接,也就是向具体的 broker 节点发送消息,而并不关心消息属于哪一个分区;6.3 而对于 KafkaProducer 的应用逻辑而言,我们只关注向哪个分区中发送哪些消息,所以在这里需要做一个应用逻辑层面到网络 I/O层面的转换。6.4 在转换成&lt;Node, List&lt;ProducerBatch&gt;&gt;的形式之后, Sender 会进一步封装成&lt;Node,Request&gt; 的形式, 这样就可以将 Request 请求发往各个 Node 了,这里的 Request 是 Kafka 各种协议请求;6.5 下一步直接就可以从Request 发送到 Selector 在转到 kafka集群7.1 缓存操作可以理解为当请求 从 sender 发送给 kafka 集群时候，sender 是不知道是否成功发送，即kafka 是否接收到消息，所以此功能是，当sender只要给 kafka 发送请求，此消息就同步InFlightRequests7.2 请求在从 sender 线程发往 Kafka 之前还会保存到InFlightRequests 中,InFlightRequests 保存对象的具体形式为Map&lt;Nodeld, Deque&lt;request&gt;&gt;,它的主要作用是缓存了已经发出去但还没有收到服务端响应的请求(Nodeld 是一个 String 类型,表示节点的 id 编号)。7.3 与此同时,InFlightRequests 还提供了许多管理类的方法,并且通过配置参数还可以限制每个连接(也就是客户端与 Node 之间的连接) 最多缓存的请求数。7.4 这个配置参数为 max.in.flight.request.per. connection ,默认值为 5,即每个连接最多只能缓存 5 个未响应的请求,超过该数值之后就不能再向这个连接发送更多的请求了,除非有缓存的请求收到了响应( Response )。8 提交到selector 准备发送9 发送到 kafka集群10 当 kafka 集群受到消息 ，集群响应，返回给selector11.1 selector 回复给 InFlightRequests11.2 如果没有受到响应， request 则会在 InFlightRequests 一直缓存11.3 通过比较 Deque&lt;Request&gt; 的 size 与这个参数的大小来判断对应的 Node中是否己经堆积了很多未响应的消息, 如果真是如此, 那么说明这个 Node 节点负载较大或网络连接有问题,再继其发送请求会增大请求超时的可能。 2、消费者API一个正常的消费逻辑需要具备以下几个步骤: 1234567(1)配置消费者客户端参数(2)创建相应的消费者实例; (3)订阅主题; (4)拉取消息并消费; (5)提交消费位移 offset;(6)关闭消费者实例。 消费者API示例代码（部分截取） 1234567891011121314151617181920212223242526272829Properties props = new Properties(); // 定义 kakfa 服务的地址,不需要将所有 broker 指定上props.put(&quot;bootstrap.servers&quot;, &quot;node1:9092&quot;); // 指定 consumer group props.put(&quot;group.id&quot;, &quot;g1&quot;); // 是否自动提交 offset props.put(&quot;enable.auto.commit&quot;, &quot;true&quot;); // 自动提交 offset 的时间间隔props.put(&quot;auto.commit.interval.ms&quot;, &quot;1000&quot;);// key 的反序列化类props.put(&quot;key.deserializer&quot;,&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); // value 的反序列化类props.put(&quot;value.deserializer&quot;,&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); // 如果没有消费偏移量记录,则自动重设为起始 offset:latest, earliest, none//Earliest目前状态下最前面的一条消息（日志在一定保存时间后会自动清空）//none（上次记录的偏移量，如果没有，会抛异常） props.put(&quot;auto.offset.reset&quot;,&quot;earliest&quot;); // 定义 consumer KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props); // 消费者订阅的 topic, 可同时订阅多个consumer.subscribe(Arrays.asList(&quot;first&quot;, &quot;test&quot;,&quot;test1&quot;)); 2.1 Kafka消费者可选属性12345678910111213141516171819202122232425262728291. fetch.min.byte消费者从服务器获取记录的最小字节数。如果可用的数据量小于设置值，broker 会等待有足够的可用数据时才会把它返回给消费者。2. fetch.max.wait.msbroker 返回给消费者数据的等待时间，默认是 500ms。3. max.partition.fetch.bytes该属性指定了服务器从每个分区返回给消费者的最大字节数，默认为 1MB。4. session.timeout.ms消费者在被认为死亡之前可以与服务器断开连接的时间，默认是 3s。5. auto.offset.reset该属性指定了消费者在读取一个没有偏移量的分区或者偏移量无效的情况下该作何处理：latest (默认值) ：在偏移量无效的情况下，消费者将从最新的记录开始读取数据（在消费者启动之后生成的最新记录）;earliest ：在偏移量无效的情况下，消费者将从起始位置读取分区的记录。6. enable.auto.commit是否自动提交偏移量，默认值是 true。为了避免出现重复消费和数据丢失，可以把它设置为 false。7. client.id客户端 id，服务器用来识别消息的来源。8. max.poll.records单次调用 poll() 方法能够返回的记录数量。9. receive.buffer.bytes &amp; send.buffer.byte这两个参数分别指定 TCP socket 接收和发送数据包缓冲区的大小，-1 代表使用操作系统的默认值。 必要参数配置 12345678910Properties props = new Properties(); props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG,StringDeserializer.class.getName());props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,StringDeserializer.class.getName());props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG,brokerList);props.put(ConsumerConfig.GROUP_ID_CONFIG,groupid);props.put(ConsumerConfig.CLIENT_ID_CONFIG,clientid); 2.2 subscribe 订阅主题 subscribe 有如下重载方法: 12345public void subscribe(Collection&lt;String&gt; topics,ConsumerRebalanceListener listener)public void subscribe(Collection&lt;String&gt; topics)public void subscribe(Pattern pattern, ConsumerRebalanceListener listener) public void subscribe(Pattern pattern) 指定集合方式订阅主题 123consumer.subscribe(Arrays.asList(topic1)); consumer subscribe(Arrays.asList(topic2)); 正则方式订阅主题 如果消费者采用的是正则表达式的方式(subscribe(Pattern))订阅, 在之后的过程中,如果有人又创建了新的主题,并且主题名字与正表达式相匹配,那么这个消费者就可以消费到新添加的主题中的消息。如果应用程序需要消费多个主题,并且可以处理不同的类型,那么这种订阅方式就很有效。 正则表达式的方式订阅的示例如下 12consumer.subscribe(Pattern.compile (&quot;topic.*&quot; )); 2.3 assign 订阅主题消费者不仅可以通过 KafkaConsumer.subscribe() 方法订阅主题,还可直接订阅某些主题的指定分区; 在 KafkaConsumer 中提供了 assign() 方法来实现这些功能,此方法的具体定义如下: 12public void assign(Collection&lt;TopicPartition&gt; partitions); 12这个方法只接受参数 partitions,用来指定需要订阅的分区集合。 12consumer.assign(Arrays.asList(new TopicPartition (&quot;tpc_1&quot; , 0),new TopicPartition(“tpc_2”,1))) ; 2.4 subscribe 与 assign 的区别 通过 subscribe()方法订阅主题具有消费者自动再均衡功能 ; 12在多个消费者的情况下可以根据分区分配策略来自动分配各个消费者与分区的关系。 当消费组的消费者增加或减少时,分区分配关系会自动调整,以实现消费负载均衡及故障自动转移。 assign() 方法订阅分区时,是不具备消费者自动均衡的功能的; 12其实这一点从 assign()方法参数可以看出端倪,两种类型 subscribe()都有 ConsumerRebalanceListener 类型参数的方法,而 assign()方法却没有。 2.5 取消订阅可以使用 KafkaConsumer 中的 unsubscribe()方法采取消主题的订阅,这个方法既可以取消通过subscribe( Collection)方式实现的订阅; 也可以取消通过 subscribe(Pattem)方式实现的订阅,还可以取消通过 assign( Collection)方式实现的订阅。示例码如下: 12consumer.unsubscribe(); 如果将 subscribe(Collection )或 assign(Collection)集合参数设置为空集合,作用与 unsubscribe()方法相同,如下示例中三行代码的效果相同: 1234consumer.unsubscribe(); consumer.subscribe(new ArrayList&lt;String&gt;()) ; consumer.assign(new ArrayList&lt;TopicPartition&gt;()); 2.6 消息的消费模式Kafka 中的消费是基于拉取模式的。消息的消费一般有两种模式:推送模式和拉取模式。推模式是服务端主动将消息推送给消费者,而拉模式是消费者主动向服务端发起请求来拉取消息。 对于 poll () 方法而言,如果某些分区中没有可供消费的消息,那么此分区对应的消息拉取的结果就为空如果订阅的所有分区中都没有可供消费的消息,那么 poll()方法返回为空的消息集; poll () 方法具体定义如下:public ConsumerRecords&lt;K, V&gt; poll(final Duration timeout)超时时间参数 timeout , 用来控制 poll() 方法的阻塞时间, 在消费者的缓冲区里没有可用数据时会发生阻塞。如果消费者程序只用来单纯拉取并消费数据,则为了提高吞吐率,可以把 timeout 设置为Long.MAX_VALUE; 消费者消费到的每条消息的类型为 ConsumerRecord 12345678910111213141516public class ConsumerRecord&lt;K, V&gt; &#123; public static final long NO_TIMESTAMP = RecordBatch.NO_TIMESTAMP; public static final int NULL_SIZE = -1; public static final int NULL_CHECKSUM = -1; private final String topic; private final int partition; private final long offset;private final long timestamp; private final TimestampType timestampType; private final int serializedKeySize; private final int serializedValueSize; private final Headers headers; private final K key; private final V value; private volatile Long checksum; 12345678910111213141516topic partition 这两个字段分别代表消息所属主题的名称和所在分区的编号。offsset 表示消息在所属分区的偏移量。timestamp 表示时间戳,与此对应的 timestampType 表示时间戳的类型。timestampType 有两种类型 CreateTime 和 LogAppendTime , 分别代表消息创建的时间戳和消息追加到日志的时间戳。headers 表示消息的头部内容。key value 分别表示消息的键和消息的值,一般业务应用要读取的就是 value ; serializedKeySize、serializedValueSize 分别表示 key、value 经过序列化之后的大小,如果 key 为空, 则 serializedKeySize 值为 -1,同样,如果 value 为空,则 serializedValueSize 的值也会为 -1; checksum 是 CRC32 的校验值。 2.7 指定位移消费有些时候,我们需要一种更细粒度的掌控,可以让我们从特定的位移处开始拉取消息,而KafkaConsumer 中的 seek() 方法正好提供了这个功能,让我们可以追前消费或回溯消费。 seek()方法的具体定义如下: 12public void seek(TopicPartiton partition,long offset); 2.8 再均衡监听器一个消费组中,一旦有消费者的增减发生,会触发消费者组的 rebalance 再均衡; 如果 A 消费者消费掉的一批消息还没来得及提交 offset, 而它所负责的分区在 rebalance 中转移给了 B 消费者,则有可能发生数据的重复消费处理。此情形下,可以通过再均衡监听器做一定程度的补救; 2.9 自动位移提交Kafka 中默认的消费位移的提交方式是自动提交,这个由消费者客户端参数 enable.auto.commit 配置, 默认值为 true 。当然这个默认的自动提交不是每消费一条消息就提交一次,而是定期提交,这个定期的周期时间由客户端参数 auto.commit.interval.ms 配置, 默认值为 5 秒, 此参数生效的前提是 enable. auto.commit 参数为 true。 在默认的方式下,消费者每隔 5 秒会将拉取到的每个分区中最大的消息位移进行提交。自动位移提交的动作是在 poll() 方法的逻辑里完成的,在每次真正向服务端发起拉取请求之前会检查是否可以进行位移提交,如果可以,那么就会提交上一次轮询的位移。 Kafka 消费的编程逻辑中位移提交是一大难点,自动提交消费位移的方式非常简便,它免去了复杂的位移提交逻辑,让编码更简洁。但随之而来的是重复消费和消息丢失的问题。 重复消费 假设刚刚提交完一次消费位移,然后拉取一批消息进行消费,在下一次自动提交消费位移之前,消费者崩溃了,那么又得从上一次位移提交的地方重新开始消费,这样便发生了重复消费的现象(对于再均衡的情况同样适用)。我们可以通过减小位移提交的时间间隔来减小重复消息的窗口大小,但这样并不能避免重复消费的发送,而且也会使位移提交更加频繁。 丢失消息 按照一般思维逻辑而言,自动提交是延时提交,重复消费可以理解,那么消息丢失又是在什么情形下会发生的呢?我们来看下图中的情形: 拉取线程不断地拉取消息并存入本地缓存, 比如在 BlockingQueue 中, 另一个处理线程从缓存中读取消息并进行相应的逻辑处理。设目前进行到了第 y+l 次拉取,以及第 m 次位移提交的时候,也就是x+6 之前的位移己经确认提交了, 处理线程却还正在处理 x+3 的消息; 此时如果处理线程发生了异常, 待其恢复之后会从第 m 次位移提交处,也就是 x+6 的位置开始拉取消息,那么 x+3 至 x+6 之间的消息就没有得到相应的处理,这样便发生消息丢失的现象。 2.10 手动位移提交(调用 kafka api)自动位移提交的方式在正常情况下不会发生消息丢失或重复消费的现象, 但是在编程的世界里异常无可避免; 同时, 自动位移提交也无法做到精确的位移管理。 在 Kafka 中还提供了手动位移提交的方式, 这样可以使得开发人员对消费位移的管理控制更加灵活。很多时候并不是说拉取到消息就算消费完成,而是需要将消息写入数据库、写入本地缓存,或者是更加复杂的业务处理。在这些场景下,所有的业务处理完成才能认为消息被成功消费; 手动的提交方式可以让开发人员根据程序的逻辑在合适的地方进行位移提交。 开启手动提交功能的前提是消费者客户端参数 enable.auto.commit 配置为 fals ,示例如下: 12props.put(ConsumerConf.ENABLE_AUTO_COMMIT_CONFIG, false); 手动提交可以细分为同步提交和异步提交,对应于 KafkaConsumer 中的 commitSync()和commitAsync()两种类型的方法。 3、Topic管理 API一般情况下,我们都习惯使用 kafka-topic.sh 本来管理主题,如果希望将管理类的功能集成到公司内部的系统中,打造集管理、监控、运维、告警为一体的生态平台,那么就需要以程序调用 API 方式去实现。这种调用 API 方式实现管理主要利用 KafkaAdminClient 工具类KafkaAdminClient 不仅可以用来管理 broker、配置和 ACL (Access Control List),还可用来管理主题) 3.1 列出主题123456ListTopicsResult listTopicsResult = adminClient.listTopics(); Set&lt;String&gt; topics = listTopicsResult.names().get(); System.out.println(topics); 3.2 查看主题信息12345678910DescribeTopicsResult describeTopicsResult = adminClient.describeTopics(Arrays.asList(&quot;tpc_4&quot;, &quot;tpc_3&quot;)); Map&lt;String, TopicDescription&gt; res = describeTopicsResult.all().get();Set&lt;String&gt; ksets = res.keySet(); for (String k : ksets) &#123; System.out.println(res.get(k)); &#125; 3.3 创建主题 代码示例（部分截取） 12345678910111213141516171819202122232425262728// 参数配置Properties props = new Properties(); props.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG,&quot;node1:9092,node2:9092,node3:9092&quot;);props.put(AdminClientConfig.REQUEST_TIMEOUT_MS_CONFIG,3000); // 创建 admin client 对象AdminClient adminClient = KafkaAdminClient.create(props); // 由服务端 controller 自行分配分区及副本所在 broker NewTopic tpc_3 = new NewTopic(&quot;tpc_3&quot;, 2, (short) 1); // 手动指定分区及副本的 broker 分配HashMap&lt;Integer, List&lt;Integer&gt;&gt; replicaAssignments = new HashMap&lt;&gt;(); // 分区 0,分配到 broker0,broker1 replicaAssignments.put(0,Arrays.asList(0,1)); // 分区 1,分配到 broker0,broker2 replicaAssignments.put(0,Arrays.asList(0,1));NewTopic tpc_4 = new NewTopic(&quot;tpc_4&quot;, replicaAssignments); CreateTopicsResult result = adminClient.createTopics(Arrays.asList(tpc_3,tpc_4)); // 从 future 中等待服务端返回try &#123; result.all().get(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; adminClient.close(); 3.4 删除主题123456DeleteTopicsResult deleteTopicsResult = adminClient.deleteTopics(Arrays.asList(&quot;tpc_1&quot;, &quot;tpc_1&quot;)); Map&lt;String, KafkaFuture&lt;Void&gt;&gt; values = deleteTopicsResult.values();System.out.println(values); 3.5 其他管理除了进行 topic 管理之外,KafkaAdminClient 也可以进行诸如动态参数管理,分区管理等各类管理操作;","categories":[],"tags":[]},{"title":"Kafka命令行操作","slug":"Kafka命令行操作","date":"2022-06-18T02:24:25.000Z","updated":"2022-06-18T02:31:39.941Z","comments":true,"path":"2022/06/18/Kafka命令行操作/","link":"","permalink":"http://example.com/2022/06/18/Kafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C/","excerpt":"Kafka命令行操作1 创建 topic1kafka-topics.sh --zookeeper node1:2181,node2:2181,node3:2181 --create --replication-factor 3 --partitions 3 --topic test_1","text":"Kafka命令行操作1 创建 topic1kafka-topics.sh --zookeeper node1:2181,node2:2181,node3:2181 --create --replication-factor 3 --partitions 3 --topic test_1 123--replication-factor 副本数量--partitions 分区数量--topic topic 名称 手动指定副本的存储位置 1kafka-topics.sh --create --topic test_2 --zookeeper node1:2181 --replica-assignment 0:1,1:2 2 删除 topic1kafka-topics.sh --delete --topic tpc_1 --zookeeper node1:2181 1(异步线程去删除)删除 topic,需要一个参数处于启用状态: delete.topic.enable = true,否则删不掉 3 查看 topic 列出当前系统中的所有 topic 1kafka-topics.sh --list --zookeeper node1:2181 查看 topic 详细信息 123kafka-topics.sh --create --topic tpc_1 --zookeeper node1:2181 --replica-assignment 0:1,1:2kafka-topics.sh --describe --topic tpc_1 --zookeeper node1:2181 4 增加分区数1kafka-topics.sh --alter --topic test_2 --partitions 3 --zookeeper node1:2181 1Kafka 只支持增加分区,不支持减少分区 5 动态配置 topic 参数 通过管理命令,可以为已创建的 topic 增加、修改、删除 topic level 参数 添加、修改配置参数（开启压缩发送传输种提高kafka消息吞吐量的有效办法(‘gzip’, ‘snappy’, ‘lz4’, ‘zstd’)） 1kafka-configs.sh --zookeeper node1:2181 --entity-type topics --entity-name tpc_1 --alter --add-config compression.type=gzip 删除配置参数 1kafka-configs.sh --zookeeper node1:2181 --entity-type topics --entity-name tpc_1 --alter --delete-config compression.type 6 Kafka命令行生产者与消费者操作 生产者:kafka-console-producer 1kafka-console-producer.sh --broker-list node1:9092, node2:9092, node3:9092 --topic tpc_1 消费者:kafka-console-consumer 消费消息 12kafka-console-consumer.sh --bootstrap-server node1:9092, node2:9092, node3:9092 --topic tpc_1 --from-beginning 指定要消费的分区,和要消费的起始 offset 1kafka-console-consumer.sh --bootstrap-server node1:9092,node2:9092,node3:9092 --topic tpc_1 --offset 2 --partition 0 7 配置管理 kafka-configs 比如查看 topic 的配置可以按如下方式执行: 1kafka-configs.sh zookeeper node1: 2181 --describe --entity-type topics --entity-name tpc_1 比如查看 broker 的动态配置可以按如下方式执行: 12kafka-configs.sh zookeeper node1: 2181 --describe --entity-type brokers --entity-name 0 --zookeeper node1:2181","categories":[],"tags":[]},{"title":"Kafka环境配置","slug":"Kafka环境配置","date":"2022-06-18T02:22:35.000Z","updated":"2022-06-18T02:23:52.819Z","comments":true,"path":"2022/06/18/Kafka环境配置/","link":"","permalink":"http://example.com/2022/06/18/Kafka%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/","excerpt":"Kafka 安装配置Kafka是一种高吞吐量的分布式发布订阅消息系统，其在大数据开发应用上的目的是通过 Hadoop的并行加载机制来统一线上和离线的消息处理，也是为了通过集群来提供实时的消息。大数据开发需掌握Kafka架构原理及各组件的作用和使用方法及相关功能的实现。","text":"Kafka 安装配置Kafka是一种高吞吐量的分布式发布订阅消息系统，其在大数据开发应用上的目的是通过 Hadoop的并行加载机制来统一线上和离线的消息处理，也是为了通过集群来提供实时的消息。大数据开发需掌握Kafka架构原理及各组件的作用和使用方法及相关功能的实现。 上传文件包 到&#x2F;export&#x2F;server&#x2F; 解压文件 1tar -zxvf kafka_2.11-2.0.0.tgz 创建软连接 1ln -s kafka_2.11-2.0.0/ kafka 进入 &#x2F;export&#x2F;server&#x2F;kafka&#x2F;config 修改 配置文件 server.properties 123cd /export/server/kafka/configvim server.properties 21 行内容 broker.id&#x3D;0 为依次增长的:0、1、2、3、4,集群中唯一 id 从0开始，每台不能重复（注：此处不用修改） 121 broker.id=0 31 行内容 #listeners&#x3D;PLAINTEXT:&#x2F;&#x2F;:9092 取消注释，内容改为：listeners&#x3D;PLAINTEXT:&#x2F;&#x2F;node1:9092 PLAINTEXT为通信使用明文（加密ssl） 131 listeners=PLAINTEXT://node1:9092 59 行内容 log.dirs&#x3D;&#x2F;tmp&#x2F;kafka-logs 为默认日志文件存储的位置，改为 log.dirs&#x3D;&#x2F;export&#x2F;data&#x2F;kafka-logs 159 log.dirs=/export/data/kafka-logs 63 行内容为 num.partitions&#x3D;1 是默认分区数 163 num.partitions=1 76 行部分 123############################ **Log Flush Policy** ############################### 数据安全性（持久化之前先放到缓存上，从缓存刷到磁盘上）interval.messages interval.ms 93 行部分 123########################### **Log Retention Policy** ############################ 数据保留策略 168/24=7，1073741824/1024=1GB，300000ms = 300s = 5min超过了删掉（最后修改时间还是创建时间--&gt;日志段中最晚的一条消息，维护这个最大的时间戳--&gt;用户无法干预 121 行内容 zookeeper.connect&#x3D;localhost:2181 修改为 zookeeper.connect&#x3D;node1:2181,node2:2181,node3:2181 1121 zookeeper.connect=node1:2181,node2:2181,node3:2181 126 行内容 group.initial.rebalance.delay.ms&#x3D;0 修改为 group.initial.rebalance.delay.ms&#x3D;3000 1133 group.initial.rebalance.delay.ms=3000 给 node2 和 node3 scp 分发 kafka 12345cd /export/server/scp -r /export/server/kafka_2.11-2.0.0/ node2:$PWDscp -r /export/server/kafka_2.11-2.0.0/ node3:$PWD 创建软连接 1ln -s /export/server/kafka_2.11-2.0.0/ kafka 配置 kafka 环境变量（注：可以一台一台配，也可以在 master 完成后发给 node2 和 node3） 12345vim /etc/profile # kafka 环境变量export KAFKA_HOME=/export/server/kafka export PATH=$PATH:$KAFKA_HOME/bin 重新加载环境变量 1source /etc/profile 分别在 node2 和 node3 上修改配置文件 路径：&#x2F;export&#x2F;server&#x2F;kafka&#x2F;config 将文件 server.properties 的第 21 行的 broker.id&#x3D;0 修改为 broker.id&#x3D;1 同理 node3 同样操作 121 broker.id=1 将文件 server.properties 的第 31 行的 listeners&#x3D;PLAINTEXT:&#x2F;&#x2F;node1:9092 修改为 listeners&#x3D;PLAINTEXT:&#x2F;&#x2F;node2:9092 同理 node3 同样操作 131 listeners=PLAINTEXT://node2:9092 启停 kafka (注：kafka 启动需要在 zookeeper 启动的情况下才可) 1kafka-server-start.sh -daemon /export/server/kafka/config/server.properties 12345678910111213141516171819202122232425hadoop，zookeeper，kafka启动结果显示：(base) [root@node1 ~]# jps11793 NodeManager91699 Kafka85618 QuorumPeerMain10697 NameNode10924 DataNode11596 ResourceManager109852 Jps[root@node2 ~]# jps9301 DataNode9493 SecondaryNameNode95959 Kafka102971 Jps9855 NodeManager89534 QuorumPeerMain[root@node3 ~]# jps88660 QuorumPeerMain95204 Kafka9110 NodeManager8616 DataNode102104 Jps 关闭 kafka 1kafka-server-stop.sh stop 定制脚本一键启动 123vim kafka-all.sh放入 /bin 路径下 1234567891011121314151617181920212223#!/bin/bashif [ $# -eq 0 ];then echo &quot;please input param:start stop&quot;else if [ $1 = start ];then for i in &#123;1..3&#125; do echo &quot;$&#123;1&#125;ing node$&#123;i&#125;&quot; ssh node$&#123;i&#125; &quot;source /etc/profile;/export/server/kafka/bin/kafka-server-start.sh -daemon /export/server/kafka/config/server.properties&quot; done fi if [ $1 = stop ];then for i in &#123;1..3&#125; do echo &quot;$&#123;1&#125;ping node$&#123;i&#125;&quot; ssh node$&#123;i&#125; &quot;source /etc/profile;/export/server/kafka/bin/kafka-server-stop.sh -daemon /export/server/kafka/config/server.properties&quot; done fifi","categories":[],"tags":[]},{"title":"Kafka-eagle配置文档","slug":"Kafka-eagle配置文档","date":"2022-06-16T13:18:32.000Z","updated":"2022-06-18T01:49:20.918Z","comments":true,"path":"2022/06/16/Kafka-eagle配置文档/","link":"","permalink":"http://example.com/2022/06/16/Kafka-eagle%E9%85%8D%E7%BD%AE%E6%96%87%E6%A1%A3/","excerpt":"","text":"一、在官网下载安装包：https://www.kafka-eagle.org/ 二、下载后上传到node1的&#x2F;export&#x2F;sever&#x2F;目录下，使用命令tar -zxvf kafka-eagle-bin-2.1.0.tar.gz -C &#x2F;export&#x2F;server&#x2F;解压到当前目录然后使用命令ln -s &#x2F;export&#x2F;server&#x2F; kafka-eagle-bin-2.1.0 &#x2F;export&#x2F;server&#x2F;kafka-eagle建立软链接： cd &#x2F;export&#x2F;server&#x2F;kafka-eagle目录下使用命令tar -zxvf efak-web-2.1.0-bin.tar.gz解压该文件包 cd efak-web-2.1.0&#x2F;进入该文件cd conf然后vi system-config.properties编辑该文件具体改动如下： kafka.eagle.zk.cluster.alias&#x3D;cluster1 cluster1.zk.list&#x3D;node1:2181,node2:2181,node3:2181 cluster1.kafka.eagle.broker.size&#x3D;3 kafka.eagle.url&#x3D;jdbc:sqlite:&#x2F;export&#x2F;data&#x2F;db&#x2F;ke.db启动前需要手动创建&#x2F;export&#x2F;data&#x2F;db目录 mkdir &#x2F;export&#x2F;data&#x2F;db进入&#x2F;etc&#x2F;profile目录下配置环境变量：(ps:还需配置java的环境变量) 三、启动Eagle命令为：&#x2F;export&#x2F;server&#x2F;kafka-eagle&#x2F;efak-web-2.1.0&#x2F;bin&#x2F;ke.sh start 然后jps命令查看发现eagle启动成功： 之后进入http://192.168.117.151:8048，输入Account:admin,Password:123456 进入页面为：","categories":[],"tags":[{"name":"Kafka-eagle","slug":"Kafka-eagle","permalink":"http://example.com/tags/Kafka-eagle/"}]},{"title":"Hello World","slug":"hello-world","date":"2022-05-29T09:07:18.607Z","updated":"2022-05-29T09:07:18.607Z","comments":true,"path":"2022/05/29/hello-world/","link":"","permalink":"http://example.com/2022/05/29/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]},{"title":"《Spark local& stand-alone配置》","slug":"1","date":"2022-05-22T08:18:57.000Z","updated":"2022-05-22T08:18:57.000Z","comments":true,"path":"2022/05/22/1/","link":"","permalink":"http://example.com/2022/05/22/1/","excerpt":"","text":"Spark-Standalone-HA模式Spark Standalone集群是Master-Slaves架构的集群模式,和大部分的Master-Slaves结构集群一样,存在着Master 单点故障(SPOF)的问题。简单理解为，spark-Standalone 模式下为 master 节点控制其他节点，当 master 节点出现故障时，集群就不可用了。 spark-Standalone-HA 模式下master 节点不固定，当一个宕机时，立即换另一台为 master 保障不出现故障。此处因为先前配置时的 zookeeper 版本和 spark 版本不太兼容，导致此模式有故障，需要重新下载配置新的版本的 zookeeper配置之前需要删除三台主机的 旧版 zookeeper 以及 对应的软连接在 master 节点上重新进行前面配置的 zookeeper 操作 1.上传apache-zookeeper-3.7.0-bin.tar.gz 到&#x2F;export&#x2F;server&#x2F;目录下 并解压文件 2.在 &#x2F;export&#x2F;server 目录下创建软连接 3.进入 &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;conf&#x2F; 将 zoo_sample.cfg 文件复制为新文件 zoo.cfg 4.接上步给 zoo.cfg 添加内容 5.进入 &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas 目录在此目录下创建 myid 文件，将 1 写入进 去6.将 master 节点中 &#x2F;export&#x2F;server&#x2F;zookeeper-3.7.0 路径下内容推送给slave1 和 slave2 7.推送成功后，分别在 slave1 和 slave2 上创建软连接 8.接上步推送完成后将 slave1 和 slave2 的 &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas&#x2F;文件夹 下的 myid 中的内容分别改为 2 和 3 配置环境变量： 因先前配置 zookeeper 时候创建过软连接且以 ’zookeeper‘ 为路径，所以不用配置环境变量，此 处也是创建软连接的方便之处 进入 &#x2F;export&#x2F;server&#x2F;spark&#x2F;conf 文件夹 修改 spark-env.sh 文件内容 cd &#x2F;export&#x2F;server&#x2F;spark&#x2F;conf vim spark-env.sh 为 83 行内容加上注释，此部分原为指定 某台主机 做 master ，加上注释后即为 任何主机都可以做 master 结果显示： …… 82 # 告知Spark的master运行在哪个机器上 83 # export SPARK_MASTER_HOST&#x3D;master ……… 文末添加内容 SPARK_DAEMON_JAVA_OPTS&#x3D;”-Dspark.deploy.recoveryMode&#x3D;ZOOKEEPER - Dspark.deploy.zookeeper.url&#x3D;master:2181,slave1:2181,slave2:2181 - Dspark.deploy.zookeeper.dir&#x3D;&#x2F;spark-ha” #spark.deploy.recoveryMode 指定HA模式 基于Zookeeper实现 #指定Zookeeper的连接地址 #指定在Zookeeper中注册临时节点的路径 分发 spark-env.sh 到 salve1 和 slave2 上 scp spark-env.sh slave1:/export/server/spark/conf/ scp spark-env.sh slave2:/export/server/spark/conf/ 启动之前确保 Zookeeper 和 HDFS 均已经启动启动集群: #在 master 上 启动一个master 和全部worker &#x2F;export&#x2F;server&#x2F;spark&#x2F;sbin&#x2F;start-all.sh # 注意, 下面命令在 slave1 上执行 启动 slave1 上的 master 做备用 master &#x2F;export&#x2F;server&#x2F;spark&#x2F;sbin&#x2F;start-master.sh 结果显示： (base) [root@master ~]# jps 37328 DataNode 41589 Master 35798 QuorumPeerMain 38521 ResourceManager 46281 Jps 38907 NodeManager 41821 Worker 36958 NameNode (base) [root@slave1 sbin]# jps 36631 DataNode 48135 Master 35385 QuorumPeerMain 37961 NodeManager 40970 Worker 48282 Jps 37276 SecondaryNameNode 访问 WebUI 界面 http://master:8081/ http://slave1:8082/ 此时 kill 掉 master 上的 master 假设 master 主机宕机掉 #master主机 master 的进程号 kill -9 41589 结果显示： (base) [root@master ~]# jps 37328 DataNode 90336 Jps 35798 QuorumPeerMain 38521 ResourceManager 38907 NodeManager 41821 Worker 36958 NameNode 访问 slave1 的 WebUI http://slave1:8082/ 进行主备切换的测试提交一个 spark 任务到当前 活跃的 master上 : &#x2F;export&#x2F;server&#x2F;spark&#x2F;bin&#x2F;spark-submit –master spark:&#x2F;&#x2F;master:7077 &#x2F;export&#x2F;server&#x2F;spark&#x2F;examples&#x2F;src&#x2F;main&#x2F;python&#x2F;pi.py 1000 复制标签 kill 掉 master 的 进程号再次访问 master 的 WebUI http://master:8081/ 网页访问不了！ 再次访问 slave1 的 WebUI http://slave1:8082/ 可以看到当前活跃的 master 提示信息 (base) [root@master ~]# &#x2F;export&#x2F;server&#x2F;spark&#x2F;bin&#x2F;spark-submit –master spark:&#x2F;&#x2F;master:7077 &#x2F;export&#x2F;server&#x2F;spark&#x2F;examples&#x2F;src&#x2F;main&#x2F;python&#x2F;pi.py 1000 22&#x2F;03&#x2F;29 16:11:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicable 22&#x2F;03&#x2F;29 16:12:16 WARN StandaloneAppClient$ClientEndpoint: Connection to master:7077 failed; waiting for master to reconnect… 22&#x2F;03&#x2F;29 16:12:16 WARN StandaloneSchedulerBackend: Disconnected from Spark cluster! Waiting for reconnection… 22&#x2F;03&#x2F;29 16:12:16 WARN StandaloneAppClient$ClientEndpoint: Connection to master:7077 failed; waiting for master to reconnect… Pi is roughly 3.140960 (base) [root@master ~]# Spark On YARN模式 在已有YARN集群的前提下在单独准备Spark StandAlone集群,对资源的利用就不高.Spark On YARN, 无 需部署Spark集群, 只要找一台服务器, 充当Spark的客户端保证 HADOOP_CONF_和 DIR_YARN_CONF_DIR 已经配置在 spark-env.sh 和环境变量中 （注: 前面配置spark-Standlone 时已经配置过此项了） spark-env.sh 文件部分显示： …. 77 ## HADOOP软件配置文件目录，读取HDFS上文件和运行YARN集群 78 HADOOP_CONF_DIR&#x3D;&#x2F;export&#x2F;server&#x2F;hadoop&#x2F;etc&#x2F;hadoop 79 YARN_CONF_DIR&#x3D;&#x2F;export&#x2F;server&#x2F;hadoop&#x2F;etc&#x2F;hadoop …. 链接到 YARN 中（注: 交互式环境 pyspark 和 spark-shell 无法运行 cluster模式）bin&#x2F;pyspark –master yarn –deploy-mode client|cluster # –deploy-mode 选项是指定部署模式, 默认是 客户端模式 # client就是客户端模式 # cluster就是集群模式 # –deploy-mode 仅可以用在YARN模式下 bin&#x2F;spark-shell –master yarn –deploy-mode client|clusterbin&#x2F;spark-submit –master yarn –deploy-mode client|cluster &#x2F;xxx&#x2F;xxx&#x2F;xxx.py 参数 spark-submit 和 spark-shell 和 pyspark的相关参数 bin&#x2F;pyspark: pyspark解释器spark环境 - bin&#x2F;spark-shell: scala解释器spark环境 - bin&#x2F;spark-submit: 提交jar包或Python文件执行的工具 - bin&#x2F;spark-sql: sparksql客户端工具 这4个客户端工具的参数基本通用.以spark-submit 为例: bin&#x2F;spark-submit –master spark:&#x2F;&#x2F;master:7077 xxx.py&#96; Usage: spark-submit [options] &lt;app jar | python file | R file&gt; [app arguments] Usage: spark-submit –kill [submission ID] –master [spark:&#x2F;&#x2F;…] Usage: spark-submit –status [submission ID] –master [spark:&#x2F;&#x2F;…] Usage: spark-submit run-example [options] example-class [example args] Options: –master MASTER_URL spark:&#x2F;&#x2F;host:port, mesos:&#x2F;&#x2F;host:port, yarn, k8s:&#x2F;&#x2F;https://host:port, or local (Default: local[*]). –deploy-mode DEPLOY_MODE 部署模式 client 或者 cluster 默认是client –class CLASS_NAME 运行java或者scala class(for Java &#x2F; Scala apps). –name NAME 程序的名字 –jars JARS Comma-separated list of jars to include on the driver and executor classpaths. –packages Comma-separated list of maven coordinates of jars to include on the driver and executor classpaths. Will search the local maven repo, then maven central and any additional remote repositories given by –repositories. The format for the coordinates should be groupId:artifactId:version. –exclude-packages Comma-separated list of groupId:artifactId, to exclude while resolving the dependencies provided in– packages to avoid dependency conflicts. –repositories Comma-separated list of additional remote repositories to search for the maven coordinates given with – packages. –py-files PY_FILES 指定Python程序依赖的其它python文件 –files FILES Comma-separated list of files to be placed in the working directory of each executor. File paths of these files in executors can be accessed via SparkFiles.get(fileName). –archives ARCHIVES Comma-separated list of archives to be extracted into the working directory of each executor. –conf, -c PROP&#x3D;VALUE 手动指定配置 –properties-file FILE Path to a file from which to load extra properties. If not specified, this will look for conf&#x2F;spark- defaults.conf. –driver-memory MEM Driver的可用内存(Default: 1024M). –driver-java-options Driver的一些Java选项 –driver-library-path Extra library path entries to pass to the driver. –driver-class-path Extra class path entries to pass to the driver. Note that jars added with –jars are automatically included in the classpath. –executor-memory MEM Executor的内存 (Default: 1G). –proxy-user NAME User to impersonate when submitting the application. This argument does not work with –principal &#x2F; –keytab. –help, -h 显示帮助文件 –verbose, -v Print additional debug output. –version, 打印版本 Cluster deploy mode only(集群模式专属): –driver-cores NUM Driver可用的的CPU核数(Default: 1). Spark standalone or Mesos with cluster deploy mode only: –supervise 如果给定, 可以尝试重启Driver Spark standalone, Mesos or K8s with cluster deploy mode only: –kill SUBMISSION_ID 指定程序ID kill –status SUBMISSION_ID 指定程序ID 查看运行状态 Spark standalone, Mesos and Kubernetes only: –total-executor-cores NUM 整个任务可以给Executor多少个CPU核心用 Spark standalone, YARN and Kubernetes only: –executor-cores NUM 单个Executor能使用多少CPU核心 Spark on YARN and Kubernetes only(YARN模式下): –num-executors NUM Executor应该开启几个 –principal PRINCIPAL Principal to be used to login to KDC. –keytab KEYTAB The full path to the file that contains the keytab for the principal specified above. Spark on YARN only: –queue QUEUE_NAME 指定运行的YARN队列(Default: “default”) 启动 YARN 的历史服务器 cd &#x2F;export&#x2F;server&#x2F;hadoop-3.3.0&#x2F;sbin .&#x2F;mr-jobhistory-daemon.sh start historyserver 访问WebUI界面 http://master:19888/ client 模式测试 SPARK_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;spark ${SPARK_HOME}&#x2F;bin&#x2F;spark-submit –master yarn –deploy-mode client – driver-memory 512m –executor-memory 512m –num-executors 1 –total- executor-cores 2 ${SPARK_HOME}&#x2F;examples&#x2F;src&#x2F;main&#x2F;python&#x2F;pi.py cluster 模式测试 SPARK_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;spark ${SPARK_HOME}&#x2F;bin&#x2F;spark-submit –master yarn –deploy-mode cluster –driver- memory 512m –executor-memory 512m –num-executors 1 –total-executor-cores 2 –conf “spark.pyspark.driver.python&#x3D;&#x2F;root&#x2F;anaconda3&#x2F;bin&#x2F;python3” –conf “spark.pyspark.python&#x3D;&#x2F;root&#x2F;anaconda3&#x2F;bin&#x2F;python3” ${SPARK_HOME}&#x2F;examples&#x2F;src&#x2F;main&#x2F;python&#x2F;pi.py 3** title: ‘《Spark local&amp; stand-alone配置》’toc: falsecomments: truekeywords: ‘’description: ‘’date: 2022-05-22 16:18:57updated: 2022-05-22 16:18:57categories:tags:top:academia: true 《Spark local&amp; stand-alone配置》 阅读全文 **本地模式(单机) 本地模式就是以一个独立的进程,通过其内部的多个线程来模拟整个Spark运行时环境Anaconda On Linux 安装 (单台服务器脚本安装)安装上传安装包: 资料中提供的Anaconda3-2021.05-Linux-x86_64.sh文件到Linux服务器上安装位置在 &#x2F;export&#x2F;server: cd &#x2F;export&#x2F;server #运行文件 sh Anaconda3-2021.05-Linux-x86_64.sh 过程显示： … #出现内容选 yes Please answer ‘yes’ or ‘no’:’ &gt;&gt;&gt; yes … #出现添加路径：&#x2F;export&#x2F;server&#x2F;anaconda3 … [&#x2F;root&#x2F;anaconda3] &#x2F;export&#x2F;server&#x2F;anaconda3 PREFIX&#x3D;&#x2F;export&#x2F;server&#x2F;anaconda3 … 安装完成后, 退出终端， 重新进来: exit 结果显示： #看到这个Base开头表明安装好了.base是默认的虚拟环境. Last login: Tue Mar 15 15:28:59 2022 from 192.168.88.1 (base) [root@node1 ~]# 创建虚拟环境 pyspark 基于 python3.8 conda create -n pyspark python&#x3D;3.8 切换到虚拟环境内 conda activate pyspark 结果显示： (base) [root@node1 ~]# conda activate pyspark (pyspark) [root@node1 ~]# 在虚拟环境内安装包 （有WARNING不用管） pip install pyhive pyspark jieba -i https://pypi.tuna.tsinghua.edu.cn/simple spark 安装将文件上传到 &#x2F;export&#x2F;server 里面 ，解压 cd &#x2F;export&#x2F;server # 解压 tar -zxvf spark-3.2.0-bin-hadoop3.2.tgz -C &#x2F;export&#x2F;server&#x2F; 建立软连接 ln -s &#x2F;export&#x2F;server&#x2F;spark-3.2.0-bin-hadoop3.2 &#x2F;export&#x2F;server&#x2F;spark 添加环境变量 SPARK_HOME: 表示Spark安装路径在哪里 PYSPARK_PYTHON: 表示Spark想运行Python程序, 那么去哪里找python执行器 JAVA_HOME: 告知Spark Java在哪里 HADOOP_CONF_DIR: 告知Spark Hadoop的配置文件在哪里 HADOOP_HOME: 告知Spark Hadoop安装在哪里 vim &#x2F;etc&#x2F;profile 内容： ….. 注：此部分之前配置过，此部分不需要在配置 #JAVA_HOME export JAVA_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;jdk1.8.0_241 export PATH&#x3D;$PATH:$JAVA_HOME&#x2F;bin export CLASSPATH&#x3D;.:$JAVA_HOME&#x2F;lib&#x2F;dt.jar:$JAVA_HOME&#x2F;lib&#x2F;tools.jar #HADOOP_HOME export HADOOP_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;hadoop-3.3.0 export PATH&#x3D;$PATH:$HADOOP_HOME&#x2F;bin:$HADOOP_HOME&#x2F;sbin #ZOOKEEPER_HOME export ZOOKEEPER_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;zookeeper export PATH&#x3D;$PATH:$ZOOKEEPER_HOME&#x2F;bin ….. #将以下部分添加进去 #SPARK_HOME export SPARK_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;spark #HADOOP_CONF_DIR export HADOOP_CONF_DIR&#x3D;$HADOOP_HOME&#x2F;etc&#x2F;hadoop #PYSPARK_PYTHON export PYSPARK_PYTHON&#x3D;&#x2F;export&#x2F;server&#x2F;anaconda3&#x2F;envs&#x2F;pyspark&#x2F;bin&#x2F;python vim .bashrc 内容添加进去： #JAVA_HOME export JAVA_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;jdk1.8.0_241 #PYSPARK_PYTHON export PYSPARK_PYTHON&#x3D;&#x2F;export&#x2F;server&#x2F;anaconda3&#x2F;envs&#x2F;pyspark&#x2F;bin&#x2F;python 重新加载环境变量文件 source &#x2F;etc&#x2F;profile source ~&#x2F;.bashrc 进入 &#x2F;export&#x2F;server&#x2F;anaconda3&#x2F;envs&#x2F;pyspark&#x2F;bin&#x2F; 文件夹 cd &#x2F;export&#x2F;server&#x2F;anaconda3&#x2F;envs&#x2F;pyspark&#x2F;bin&#x2F; 开启 .&#x2F;pyspark 结果显示： (base) [root@master bin]# .&#x2F;pyspark Python 3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0] :: Anaconda, Inc. on linux Type “help”, “copyright”, “credits” or “license” for more information. Setting default log level to “WARN”. To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel). 2022-03-15 20:37:04,612 WARN util.NativeCodeLoader: Unable to load native- hadoop library for your platform… using builtin-java classes where applicable Welcome to __ __ __ &#x2F; &#x2F; ___ _&#x2F; &#x2F; \\ / _ / _ &#96;&#x2F; _&#x2F; ‘&#x2F; &#x2F;_ &#x2F; .&#x2F;_,&#x2F;&#x2F; &#x2F;&#x2F;_\\ version 3.2.0 &#x2F;&#x2F; Using Python version 3.8.12 (default, Oct 12 2021 13:49:34) Spark context Web UI available at http://master:4040 Spark context available as ‘sc’ (master &#x3D; local[*], app id &#x3D; local- 1647347826262). SparkSession available as ‘spark’. &gt;&gt;&gt; 查看WebUI界面 浏览器访问： http://node1:4040/ 退出 conda deactivate Standalone模式(集群) Spark中的各个角色以独立进程的形式存在,并组成Spark集群环境 Anaconda On Linux 安装 (单台服务器脚本安装 注：在 slave1 和 slave2 上部署)安装上传安装包: 资料中提供的Anaconda3-2021.05-Linux-x86_64.sh文件到Linux服务器上安装位置在 &#x2F;export&#x2F;server: cd &#x2F;export&#x2F;server # 运行文件 sh Anaconda3-2021.05-Linux-x86_64.sh 过程显示： … #出现内容选 yes Please answer ‘yes’ or ‘no’:’yes … #出现添加路径：&#x2F;export&#x2F;server&#x2F;anaconda3 … [&#x2F;root&#x2F;anaconda3] &gt;&gt;&gt; &#x2F;export&#x2F;server&#x2F;anaconda3 PREFIX&#x3D;&#x2F;export&#x2F;server&#x2F;anaconda3 … 安装完成后, 退出终端， 重新进来: exit 结果显示： #看到这个Base开头表明安装好了.base是默认的虚拟环境. Last login: Tue Mar 15 15:28:59 2022 from 192.168.88.1 … 在 master 节点上把 .&#x2F;bashrc 和 profile 分发给 slave1 和 slave2 #分发 .bashrc : scp &#x2F;.bashrc root@slave1:&#x2F; scp &#x2F;.bashrc root@slave2:&#x2F; #分发 profile : scp &#x2F;etc&#x2F;profile&#x2F; root@slave1:&#x2F;etc&#x2F; scp &#x2F;etc&#x2F;profile&#x2F; root@slave2:&#x2F;etc&#x2F; … 创建虚拟环境 pyspark 基于 python3.8 conda create -n pyspark python&#x3D;3.8 切换到虚拟环境内 conda activate pyspark 结果显示： (base) [root@node1 ~]# conda activate pyspark (pyspark)在虚拟环境内安装包 （有WARNING不用管） pip install pyhive pyspark jieba -i https://pypi.tuna.tsinghua.edu.cn/simple spark 安装将文件上传到 &#x2F;export&#x2F;server 里面 ，解压 master 节点节点进入 &#x2F;export&#x2F;server&#x2F;spark&#x2F;conf 修改以下配置文件 cd &#x2F;export&#x2F;server&#x2F;spark&#x2F;conf 将文件 workers.template 改名为 workers，并配置文件内容 mv workers.template workers vim workers # localhost删除，内容追加文末： node1 node2 node3 # 功能: 这个文件就是指示了 当前SparkStandAlone环境下, 有哪些worker 将文件 spark-env.sh.template 改名为 spark-env.sh，并配置相关内容 mv spark-env.sh.template spark-env.sh vim spark-env.sh 文末追加内容： ##设置JAVA安装目录 JAVA_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;jdk ##HADOOP软件配置文件目录，读取HDFS上文件和运行YARN集群 HADOOP_CONF_DIR&#x3D;&#x2F;export&#x2F;server&#x2F;hadoop&#x2F;etc&#x2F;hadoop YARN_CONF_DIR&#x3D;&#x2F;export&#x2F;server&#x2F;hadoop&#x2F;etc&#x2F;hadoop ## 指定spark老大Master的IP和提交任务的通信端口 # 告知Spark的master运行在哪个机器上 export SPARK_MASTER_HOST&#x3D;master #告知sparkmaster的通讯端口 export SPARK_MASTER_PORT&#x3D;7077 # 告知spark master的 webui端口 SPARK_MASTER_WEBUI_PORT&#x3D;8080 # worker cpu可用核数 SPARK_WORKER_CORES&#x3D;1 # worker可用内存 SPARK_WORKER_MEMORY&#x3D;1g # worker的工作通讯地址 SPARK_WORKER_PORT&#x3D;7078 # worker的 webui地址 SPARK_WORKER_WEBUI_PORT&#x3D;8081 ## 设置历史服务器 # 配置的意思是 将spark程序运行的历史日志 存到hdfs的&#x2F;sparklog文件夹中 SPARK_HISTORY_OPTS&#x3D;”- Dspark.history.fs.logDirectory&#x3D;hdfs:&#x2F;&#x2F;master:8020&#x2F;sparklog&#x2F; - Dspark.history.fs.cleaner.enabled&#x3D;true” 开启 hadoop 的 hdfs 和 yarn 集群 start-dfs.sh start-yarn.sh 在HDFS上创建程序运行历史记录存放的文件夹，同样 conf 文件目录下: hadoop fs -mkdir &#x2F;sparklog hadoop fs -chmod 777 &#x2F;sparklog 将 spark-defaults.conf.template 改为 spark-defaults.conf 并做相关配置 mv spark-defaults.conf.template spark-defaults.conf vim spark-defaults.conf 文末追加内容为： # 开启spark的日期记录功能 spark.eventLog.enabled true # 设置spark日志记录的路径 spark.eventLog.dir hdfs:&#x2F;&#x2F;master:8020&#x2F;sparklog&#x2F; # 设置spark日志是否启动压缩 spark.eventLog.compress true 配置 log4j.properties 文件 将文件第 19 行的 log4j.rootCategory&#x3D;INFO, console 改为log4j.rootCategory&#x3D;WARN, console （即将INFO 改为 WARN 目的：输出日志, 设置级别为WARN 只输出警告和错误日志，INFO 则为输出所有信息，多数为无用信息） mv log4j.properties.template log4j.properties vim log4j.properties 结果显示： … 18 # Set everything to be logged to the console 19 log4j.rootCategory&#x3D;WARN, console …. master 节点分发 spark 安装文件夹 到 slave1 和 slave2 上 master 节点分发 spark 安装文件夹 到 slave1 和 slave2 上 在slave1 和 slave2 上做软连接 ln -s &#x2F;export&#x2F;server&#x2F;spark-3.2.0-bin-hadoop3.2 &#x2F;export&#x2F;server&#x2F;spark 重新加载环境变量 source &#x2F;etc&#x2F;profile 进入 &#x2F;export&#x2F;server&#x2F;spark&#x2F;sbin 文件目录下 启动 start-history-server.sh cd &#x2F;export&#x2F;server&#x2F;spark&#x2F;sbin .&#x2F;start-history-server.sh 访问 WebUI 界面 浏览器访问： http://master:18080/","categories":[],"tags":[]},{"title":"《spark基础环境配置》","slug":"2","date":"2022-05-22T08:18:57.000Z","updated":"2022-05-22T08:18:57.000Z","comments":true,"path":"2022/05/22/2/","link":"","permalink":"http://example.com/2022/05/22/2/","excerpt":"","text":"《spark基础环境配置》打开一个hosts映射文件,为了保证后续相互关联的虚拟机能够通过主机名进行访问，根据实际需求配置对应的IP和主机名映射，分别将主机名master、slave1、slave2 与IP地址 192.168.88.134、192.168.88.135 和192.168.88.136进行了匹配映射(这里通常要根据实际需要，将要搭建的集群主机都配置主机名和IP映射)。编辑 &#x2F;etc&#x2F;hosts 文件 vim &#x2F;etc&#x2F;hosts 内容修改为（注：三台主机内容一样） localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 192.168.88.135 node1 192.168.88.136 node2 192.168.88.137 node3 三、集群配置时间同步定义：网络时间服务协议（Network Time Protocol, NTP），是用来使计算机时间同步化的一种协议，它可以使计算机对其服务器做时间同步化。原因：时间同步服务器，顾名思义就是来同步时间的。在集群中同步时间有着十分重要的作用，负载均衡集群或高可用集群如果时间不一致，在服务器之间的数据误差就会很大，寻找数据便会成为一件棘手的事情。若是时间无法同步，那么就算是备份了数据，你也可能无法在正确的时间将正确的数据备份。那损失可就大了。yum 安装 ntp （注：三台主机做同样操作） yum install ntp -y 开机自启动ntp systemctl enable ntpd &amp;&amp; systemctl start ntpd 结果显示： [root@master ~]# systemctl enable ntpd &amp;&amp; systemctl start ntpd Created symlink from &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;multi- user.target.wants&#x2F;ntpd.service to &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;ntpd.service. 授权 192.168.88.0-192.168.10.255 网段上的所有机器可以从这台机器上查询和同步时间 #查看ntp配置文件 ls -al &#x2F;etc | grep ‘ntp’ #显示内容 [root@node1 etc]# ls -al &#x2F;etc | grep ‘ntp’ drwxr-xr-x 3 root root 52 3月 10 18:25 ntp -rw-r–r– 1 root root 2041 3月 10 20:03 ntp.conf #编辑内容添加 restrict 192.168.88.0 mask 255.255.255.0 （注：在17行左右） vim &#x2F;etc&#x2F;ntp.conf 16 # Hosts on local network are less restricted. 17 restrict 192.168.88.0 mask 255.255.255.0 集群在局域网中，不使用其他互联网上的时间 #修改 /etc/ntpd.conf 内容 vim vim /etc/ntp.conf # 将21-24行内容注释掉（注：原来未注释） 21 #server 0.centos.pool.ntp.org iburst 22 #server 1.centos.pool.ntp.org iburst 23 #server 2.centos.pool.ntp.org iburst 24 #server 3.centos.pool.ntp.org iburst # 在25行添加 server masterIP 即为： server 192.168.88.135 node 和 node3 相同操作三台主机同时执行 systemctl enable ntpd &amp;&amp; systemctl start ntpd 查看ntp端口 [root@master etc]# ss -tupln | grep ‘123’ udp UNCONN 0 0 192.168.88.135:123 : users:((“ntpd”,pid&#x3D;54823,fd&#x3D;19)) udp UNCONN 0 0 127.0.0.1:123 : users:((“ntpd”,pid&#x3D;54823,fd&#x3D;18)) udp UNCONN 0 0 :123 : users:((“ntpd”,pid&#x3D;54823,fd&#x3D;16)) udp UNCONN 0 0 [fe80::2832:5f98:5bc0:e621]%ens33:123 [::]: users:((“ntpd”,pid&#x3D;54823,fd&#x3D;23)) udp UNCONN 0 0 [::1]:123 [::]:* users:((“ntpd”,pid&#x3D;54823,fd&#x3D;20)) udp UNCONN 0 0 [::]:123 [::]:* users:((“ntpd”,pid&#x3D;54823,fd&#x3D;17)) 配置完成后三台主机都需要重启 shutdown -r 0 三台主机同时执行（注：此过程需要5分钟左右） ntpstat 三、ssh免密钥登陆SSH免密钥登陆可以更加方便的实现不同计算机之间的连接和切换master 生成公钥私钥 (一路回车)ssh-keygen #结果显示： [root@master .ssh]# ssh-keygen Generating public&#x2F;private rsa key pair. Enter file in which to save the key (&#x2F;root&#x2F;.ssh&#x2F;id_rsa): Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in &#x2F;root&#x2F;.ssh&#x2F;id_rsa. Your public key has been saved in &#x2F;root&#x2F;.ssh&#x2F;id_rsa.pub. T he key fingerprint is: SHA256:QUAgFH5KBc&#x2F;Erlf1JWSBbKeEepPJqMBqpWbc02&#x2F;uFj8 root@master The key’s randomart image is: +—[RSA 2048]—-+ | .&#x3D;++oo+.o+. | | . . ...o . | |. o.++ *.+ o | |.o ++ B ... | |o.=o.o .S | |.*oo.. . | |+ .. . o | | + E | | =o . | +----[SHA256]-----+ 查看隐藏的 .ssh 文件 la -al .ssh # 结果显示 [root@master ~]# ls -al .ssh/ 总用量 16 drwx------ 2 root root 80 3月 10 21:52 . dr-xr-x---. 4 root root 175 3月 10 21:45 .. -rw------- 1 root root 393 3月 10 21:52 authorized_keys -rw------- 1 root root 1675 3月 10 21:48 id_rsa -rw-r--r-- 1 root root 393 3月 10 21:48 id_rsa.pub -rw-r--r-- 1 root root 366 3月 10 21:54 known_hosts master 配置免密登录到master slave1 slave2 ssh-copy-id master ssh-copy-id slave1 ssh-copy-id slave2 四、安装配置 jdk编译环境软件安装目录 mkdir -p &#x2F;export&#x2F;server JDK 1.8安装 上传 jdk-8u241-linux-x64.tar.gz到&#x2F;export&#x2F;server&#x2F;目录下 并解压文件 tar -zxvf jdk-8u241-linux-x64.tar.gz 配置环境变量 vim &#x2F;etc&#x2F;profile export JAVA_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;jdk1.8.0_241 export PATH&#x3D;$PATH:$JAVA_HOME&#x2F;bin export CLASSPATH&#x3D;.:$JAVA_HOME&#x2F;lib&#x2F;dt.jar:$JAVA_HOME&#x2F;lib&#x2F;tools.jar 重新加载环境变量文件 source &#x2F;etc&#x2F;profile 查看 java 版本号 java -version 结果显示： [root@master jdk1.8.0_241]# java -version java version “1.8.0_241” Java(TM) SE Runtime Environment (build 1.8.0_241-b07) Java HotSpot(TM) 64-Bit Server VM (build 25.241-b07, mixed mode) master 节点将 java 传输到 slave1 和 slave2 scp -r &#x2F;export&#x2F;server&#x2F;jdk1.8.0_241&#x2F; root@slave1:&#x2F;export&#x2F;server&#x2F; scp -r &#x2F;export&#x2F;server&#x2F;jdk1.8.0_241&#x2F; root@slave2:&#x2F;export&#x2F;server&#x2F; 配置 slave1 和 slave2 的 jdk 环境变量（注：和上方 master 的配置方法一样）在 master slave1 和slave2 创建软连接 cd &#x2F;export&#x2F;server ln -s jdk1.8.0_241&#x2F; jdk 重新加载环境变量文件 source &#x2F;etc&#x2F;profile zookeeper安装配置配置主机名和IP的映射关系，修改 &#x2F;etc&#x2F;hosts 文件，添加 master.root slave1.root slave2.root vim /etc/hosts #结果显示 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 192.168.88.135 master master.root 192.168.88.136 slave1 slave1.root 192.168.88.137 slave2 slave2.root zookeeper安装 上传 zookeeper-3.4.10.tar.gz到&#x2F;export&#x2F;server&#x2F;目录下 并解压文件 cd &#x2F;export&#x2F;server&#x2F; tar -zxvf zookeeper-3.4.10.tar.gz 在 &#x2F;export&#x2F;server 目录下创建软连接 cd &#x2F;export&#x2F;server ln -s zookeeper-3.4.10&#x2F; zookeeper 进入 &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;conf&#x2F; 将 zoo_sample.cfg 文件复制为新文件 zoo.cfg cd &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;conf&#x2F; cp zoo_sample.cfg zoo.cfg 接上步给 zoo.cfg 添加内容 #Zookeeper的数据存放目录 dataDir&#x3D;&#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas # 保留多少个快照 autopurge.snapRetainCount&#x3D;3 # 日志多少小时清理一次 autopurge.purgeInterval&#x3D;1 # 集群中服务器地址 server.1&#x3D;master:2888:3888 server.2&#x3D;slave1:2888:3888 server.3&#x3D;slave2:2888:3888 进入 &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas 目录在此目录下创建 myid 文件，将 1 写入进去 cd &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdata touch myid echo ‘1’ &gt; myid 将 master 节点中 &#x2F;export&#x2F;server&#x2F;zookeeper-3.4.10 路径下内容推送给slave1 和 slave2 scp -r &#x2F;export&#x2F;server&#x2F;zookeeper-3.4.10&#x2F; slave1:$PWD scp -r &#x2F;export&#x2F;server&#x2F;zookeeper-3.4.10&#x2F; slave2:$PWD 推送成功后，分别在 slave1 和 slave2 上创建软连接 ln -s zookeeper-3.4.10&#x2F; zookeeper 接上步推送完成后将 slave1 和 slave2 的 &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas&#x2F; 文件夹下的 myid中的内容分别改为 2 和3 cd &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas&#x2F; 结果显示： [root@slave1 zkdatas]# vim myid [root@slave1 zkdatas]# more myid 2[root@slave2 zkdatas]# vim myid [root@slave2 zkdatas]# more myid 3 配置zookeeper的环境变量（注：三台主机都需要配置） vim &#x2F;etc&#x2F;profile # zookeeper 环境变量 export ZOOKEEPER_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;zookeeper export PATH&#x3D;$PATH:$ZOOKEEPER_HOME&#x2F;bin 重新加载环境变量文件 source &#x2F;etc&#x2F;profile 进入 &#x2F;export&#x2F;server&#x2F;zookeeper-3.4.10&#x2F;bin 目录下启动 zkServer.sh 脚本 （注：三台都需要做） cd &#x2F;export&#x2F;server&#x2F;zookeeper-3.4.10&#x2F;bin zkServer.sh start 结果显示： [root@master bin]# .&#x2F;zkServer.sh start ZooKeeper JMX enabled by default Using config: &#x2F;export&#x2F;server&#x2F;zookeeper-3.4.10&#x2F;bin&#x2F;..&#x2F;conf&#x2F;zoo.cfg Starting zookeeper … STARTED zookeeper 的状态 zkServer.sh status 结果显示： [root@master server]# zkServer.sh status ZooKeeper JMX enabled by default Using config: /export/server/zookeeper-3.4.10/bin/../conf/zoo.cfg Mode: follower [root@slave1 server]# zkServer.sh status ZooKeeper JMX enabled by default Using config: /export/server/zookeeper-3.4.10/bin/../conf/zoo.cfg Mode: follower [root@slave2 conf]# zkServer.sh status ZooKeeper JMX enabled by default Using config: /export/server/zookeeper-3.4.10/bin/../conf/zoo.cfg Mode: leader jps 结果显示： [root@master server]# jps 125348 QuorumPeerMain 16311 Jps [root@slave1 server]# jps 126688 QuorumPeerMain 17685 Jps [root@slave2 conf]# jps 126733 QuorumPeerMain 17727 Jps 脚本一键启动 vim zkServer.sh #!&#x2F;bin&#x2F;bash if [ $# -eq 0 ] ; then echo “please input param:start stop”elseif [ $1 &#x3D; start ] ;then echo “${1}ing master” ssh master “source &#x2F;etc&#x2F;profile;&#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;bin&#x2F;zkServer.sh start” for i in {1..2} do echo “${1}ping slave${i}” ssh slave$&#123;i&#125; &quot;source /etc/profile;/export/server/zookeeper/bin/zkServer.sh start&quot; done fiif [ $1 &#x3D; stop ];then echo “${1}ping master “ ssh master “source &#x2F;etc&#x2F;profile;&#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;bin&#x2F;zkServer.sh stop” for i in {1..2} do echo “${1}ping slave${i}” ssh slave${i} “source &#x2F;etc&#x2F;profile;&#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;bin&#x2F;zkServer.sh stop” donefiif [ $1 &#x3D; status ];then echo “${1}ing master” ssh master “source &#x2F;etc&#x2F;profile;&#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;bin&#x2F;zkServer.sh status” for i in {1..2} do echo “${1}ping slave${i}” ssh slave${i} “source &#x2F;etc&#x2F;profile;&#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;bin&#x2F;zkServer.sh status” done fi fi #将文件放在 &#x2F;bin 目录下 chmod +x zkServer-all.sh &amp;&amp; zkServer-all.sh Hadoop 安装配置把 hadoop-3.3.0-Centos7-64-with-snappy.tar.gz 上传到 &#x2F;export&#x2F;server 并解压文件 tar -zxvf hadoop-3.3.0-Centos7-64-with-snappy.tar.gz 修改配置文件(进入路径 /export/server/hadoop-3.3.0/etc/hadoop) cd /export/server/hadoop-3.3.0/etc/hadoop hadoop-env.sh #文件最后添加 export JAVA_HOME=/export/server/jdk1.8.0_241 export HDFS_NAMENODE_USER=root export HDFS_DATANODE_USER=root export HDFS_SECONDARYNAMENODE_USER=root export YARN_RESOURCEMANAGER_USER=root export YARN_NODEMANAGER_USER=root core-site.xml &lt;!-- 设置默认使用的文件系统 Hadoop支持file、HDFS、GFS、ali|Amazon云等文件系统 - -&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://master:8020&lt;/value&gt; &lt;/property&gt; &lt;!-- 设置Hadoop本地保存数据路径 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/export/data/hadoop-3.3.0&lt;/value&gt; &lt;/property&gt; &lt;!-- 设置HDFS web UI用户身份 --&gt; &lt;property&gt; &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt; hadoop.proxyuser.root.hosts * hadoop.proxyuser.root.groups * < !-- 文件系统垃圾桶保存时间 --> fs.trash.interval 1440 hdfs-site.xml dfs.namenode.secondary.http-address slave1:9868 mapred-site.xml mapreduce.framework.name yarn mapreduce.jobhistory.address master:10020 mapreduce.jobhistory.webapp.address master:19888 yarn.app.mapreduce.am.env HADOOP_MAPRED_HOME=${HADOOP_HOME} mapreduce.map.env HADOOP_MAPRED_HOME=${HADOOP_HOME} mapreduce.reduce.env HADOOP_MAPRED_HOME=${HADOOP_HOME} yarn-site.xml yarn.resourcemanager.hostname master yarn.nodemanager.aux-services mapreduce_shuffle yarn.nodemanager.pmem-check-enabled false yarn.nodemanager.vmem-check-enabled false yarn.log-aggregation-enable true yarn.log.server.url http://master:19888/jobhistory/logs yarn.log-aggregation.retain-seconds 604800 node1 node2 node3 分发同步hadoop安装包 cd &#x2F;export&#x2F;server scp -r hadoop-3.3.0 root@slave1:$PWD scp -r hadoop-3.3.0 root@slave2:$PWD将hadoop添加到环境变量 vim &#x2F;etc&#x2F;profile export HADOOP_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;hadoop-3.3.0 export PATH&#x3D;$PATH:$HADOOP_HOME&#x2F;bin:$HADOOP_HOME&#x2F;sbin 重新加载环境变量文件 source &#x2F;etc&#x2F;profile Hadoop集群启动 格式化namenode（只有首次启动需要格式化） hdfs namenode -format 脚本一键启动 [root@master ~]# start-dfs.sh Starting namenodes on [master] 上一次登录：五 3月 11 21:27:24 CST 2022pts&#x2F;0 上 Starting datanodes 上一次登录：五 3月 11 21:27:32 CST 2022pts&#x2F;0 上 Starting secondary namenodes [slave1] 上一次登录：五 3月 11 21:27:35 CST 2022pts&#x2F;0 上 [root@master ~]# start-yarn.sh Starting resourcemanager 上一次登录：五 3月 11 21:27:41 CST 2022pts/0 上 Starting nodemanagers 上一次登录：五 3月 11 21:27:51 CST 2022pts/0 上 启动后 输入 jps 查看 [root@master ~]# jps 127729 NameNode 127937 DataNode 14105 Jps 128812 NodeManager 128591 ResourceManager [root@slave1 hadoop]# jps 121889 NodeManager 121559 SecondaryNameNode 7014 Jps 121369 DataNode [root@slave2 hadoop]# jps 6673 Jps 121543 NodeManager 121098 DataNode WEB页面HDFS集群： http://master:9870/ YARN集群： http://master:9870/","categories":[],"tags":[]},{"title":"《Spark HA & Yarn配置》","slug":"3","date":"2022-05-22T08:18:57.000Z","updated":"2022-05-22T08:18:57.000Z","comments":true,"path":"2022/05/22/3/","link":"","permalink":"http://example.com/2022/05/22/3/","excerpt":"","text":"《Spark HA &amp; Yarn配置》Spark-Standalone-HA模式Spark Standalone集群是Master-Slaves架构的集群模式,和大部分的Master-Slaves结构集群一样,存在着Master 单点故障(SPOF)的问题。简单理解为，spark-Standalone 模式下为 master 节点控制其他节点，当 master 节点出现故障时，集群就不可用了。 spark-Standalone-HA 模式下master 节点不固定，当一个宕机时，立即换另一台为 master 保障不出现故障。此处因为先前配置时的 zookeeper 版本和 spark 版本不太兼容，导致此模式有故障，需要重新下载配置新的版本的 zookeeper配置之前需要删除三台主机的 旧版 zookeeper 以及 对应的软连接在 master 节点上重新进行前面配置的 zookeeper 操作 1.上传apache-zookeeper-3.7.0-bin.tar.gz 到&#x2F;export&#x2F;server&#x2F;目录下 并解压文件 2.在 &#x2F;export&#x2F;server 目录下创建软连接 3.进入 &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;conf&#x2F; 将 zoo_sample.cfg 文件复制为新文件 zoo.cfg 4.接上步给 zoo.cfg 添加内容 5.进入 &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas 目录在此目录下创建 myid 文件，将 1 写入进 去6.将 master 节点中 &#x2F;export&#x2F;server&#x2F;zookeeper-3.7.0 路径下内容推送给slave1 和 slave2 7.推送成功后，分别在 slave1 和 slave2 上创建软连接 8.接上步推送完成后将 slave1 和 slave2 的 &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas&#x2F;文件夹 下的 myid 中的内容分别改为 2 和 3 配置环境变量： 因先前配置 zookeeper 时候创建过软连接且以 ’zookeeper‘ 为路径，所以不用配置环境变量，此 处也是创建软连接的方便之处 进入 &#x2F;export&#x2F;server&#x2F;spark&#x2F;conf 文件夹 修改 spark-env.sh 文件内容 cd &#x2F;export&#x2F;server&#x2F;spark&#x2F;conf vim spark-env.sh 为 83 行内容加上注释，此部分原为指定 某台主机 做 master ，加上注释后即为 任何主机都可以做 master 结果显示： …… 82 # 告知Spark的master运行在哪个机器上 83 # export SPARK_MASTER_HOST&#x3D;master ……… 文末添加内容 SPARK_DAEMON_JAVA_OPTS&#x3D;”-Dspark.deploy.recoveryMode&#x3D;ZOOKEEPER - Dspark.deploy.zookeeper.url&#x3D;master:2181,slave1:2181,slave2:2181 - Dspark.deploy.zookeeper.dir&#x3D;&#x2F;spark-ha” #spark.deploy.recoveryMode 指定HA模式 基于Zookeeper实现 #指定Zookeeper的连接地址 #指定在Zookeeper中注册临时节点的路径 分发 spark-env.sh 到 salve1 和 slave2 上 scp spark-env.sh slave1:/export/server/spark/conf/ scp spark-env.sh slave2:/export/server/spark/conf/ 启动之前确保 Zookeeper 和 HDFS 均已经启动启动集群: #在 master 上 启动一个master 和全部worker &#x2F;export&#x2F;server&#x2F;spark&#x2F;sbin&#x2F;start-all.sh # 注意, 下面命令在 slave1 上执行 启动 slave1 上的 master 做备用 master &#x2F;export&#x2F;server&#x2F;spark&#x2F;sbin&#x2F;start-master.sh 结果显示： (base) [root@master ~]# jps 37328 DataNode 41589 Master 35798 QuorumPeerMain 38521 ResourceManager 46281 Jps 38907 NodeManager 41821 Worker 36958 NameNode (base) [root@slave1 sbin]# jps 36631 DataNode 48135 Master 35385 QuorumPeerMain 37961 NodeManager 40970 Worker 48282 Jps 37276 SecondaryNameNode 访问 WebUI 界面 http://master:8081/ http://slave1:8082/ 此时 kill 掉 master 上的 master 假设 master 主机宕机掉 #master主机 master 的进程号 kill -9 41589 结果显示： (base) [root@master ~]# jps 37328 DataNode 90336 Jps 35798 QuorumPeerMain 38521 ResourceManager 38907 NodeManager 41821 Worker 36958 NameNode 访问 slave1 的 WebUI http://slave1:8082/ 进行主备切换的测试提交一个 spark 任务到当前 活跃的 master上 : &#x2F;export&#x2F;server&#x2F;spark&#x2F;bin&#x2F;spark-submit –master spark:&#x2F;&#x2F;master:7077 &#x2F;export&#x2F;server&#x2F;spark&#x2F;examples&#x2F;src&#x2F;main&#x2F;python&#x2F;pi.py 1000 复制标签 kill 掉 master 的 进程号再次访问 master 的 WebUI http://master:8081/ 网页访问不了！ 再次访问 slave1 的 WebUI http://slave1:8082/ 可以看到当前活跃的 master 提示信息 (base) [root@master ~]# &#x2F;export&#x2F;server&#x2F;spark&#x2F;bin&#x2F;spark-submit –master spark:&#x2F;&#x2F;master:7077 &#x2F;export&#x2F;server&#x2F;spark&#x2F;examples&#x2F;src&#x2F;main&#x2F;python&#x2F;pi.py 1000 22&#x2F;03&#x2F;29 16:11:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicable 22&#x2F;03&#x2F;29 16:12:16 WARN StandaloneAppClient$ClientEndpoint: Connection to master:7077 failed; waiting for master to reconnect… 22&#x2F;03&#x2F;29 16:12:16 WARN StandaloneSchedulerBackend: Disconnected from Spark cluster! Waiting for reconnection… 22&#x2F;03&#x2F;29 16:12:16 WARN StandaloneAppClient$ClientEndpoint: Connection to master:7077 failed; waiting for master to reconnect… Pi is roughly 3.140960 (base) [root@master ~]# Spark On YARN模式 在已有YARN集群的前提下在单独准备Spark StandAlone集群,对资源的利用就不高.Spark On YARN, 无 需部署Spark集群, 只要找一台服务器, 充当Spark的客户端保证 HADOOP_CONF_和 DIR_YARN_CONF_DIR 已经配置在 spark-env.sh 和环境变量中 （注: 前面配置spark-Standlone 时已经配置过此项了） spark-env.sh 文件部分显示： …. 77 ## HADOOP软件配置文件目录，读取HDFS上文件和运行YARN集群 78 HADOOP_CONF_DIR&#x3D;&#x2F;export&#x2F;server&#x2F;hadoop&#x2F;etc&#x2F;hadoop 79 YARN_CONF_DIR&#x3D;&#x2F;export&#x2F;server&#x2F;hadoop&#x2F;etc&#x2F;hadoop …. 链接到 YARN 中（注: 交互式环境 pyspark 和 spark-shell 无法运行 cluster模式）bin&#x2F;pyspark –master yarn –deploy-mode client|cluster # –deploy-mode 选项是指定部署模式, 默认是 客户端模式 # client就是客户端模式 # cluster就是集群模式 # –deploy-mode 仅可以用在YARN模式下 bin&#x2F;spark-shell –master yarn –deploy-mode client|clusterbin&#x2F;spark-submit –master yarn –deploy-mode client|cluster &#x2F;xxx&#x2F;xxx&#x2F;xxx.py 参数 spark-submit 和 spark-shell 和 pyspark的相关参数 bin&#x2F;pyspark: pyspark解释器spark环境 - bin&#x2F;spark-shell: scala解释器spark环境 - bin&#x2F;spark-submit: 提交jar包或Python文件执行的工具 - bin&#x2F;spark-sql: sparksql客户端工具 这4个客户端工具的参数基本通用.以spark-submit 为例: bin&#x2F;spark-submit –master spark:&#x2F;&#x2F;master:7077 xxx.py&#96; Usage: spark-submit [options] &lt;app jar | python file | R file&gt; [app arguments] Usage: spark-submit –kill [submission ID] –master [spark:&#x2F;&#x2F;…] Usage: spark-submit –status [submission ID] –master [spark:&#x2F;&#x2F;…] Usage: spark-submit run-example [options] example-class [example args] Options: –master MASTER_URL spark:&#x2F;&#x2F;host:port, mesos:&#x2F;&#x2F;host:port, yarn, k8s:&#x2F;&#x2F;https://host:port, or local (Default: local[*]). –deploy-mode DEPLOY_MODE 部署模式 client 或者 cluster 默认是client –class CLASS_NAME 运行java或者scala class(for Java &#x2F; Scala apps). –name NAME 程序的名字 –jars JARS Comma-separated list of jars to include on the driver and executor classpaths. –packages Comma-separated list of maven coordinates of jars to include on the driver and executor classpaths. Will search the local maven repo, then maven central and any additional remote repositories given by –repositories. The format for the coordinates should be groupId:artifactId:version. –exclude-packages Comma-separated list of groupId:artifactId, to exclude while resolving the dependencies provided in– packages to avoid dependency conflicts. –repositories Comma-separated list of additional remote repositories to search for the maven coordinates given with – packages. –py-files PY_FILES 指定Python程序依赖的其它python文件 –files FILES Comma-separated list of files to be placed in the working directory of each executor. File paths of these files in executors can be accessed via SparkFiles.get(fileName). –archives ARCHIVES Comma-separated list of archives to be extracted into the working directory of each executor. –conf, -c PROP&#x3D;VALUE 手动指定配置 –properties-file FILE Path to a file from which to load extra properties. If not specified, this will look for conf&#x2F;spark- defaults.conf. –driver-memory MEM Driver的可用内存(Default: 1024M). –driver-java-options Driver的一些Java选项 –driver-library-path Extra library path entries to pass to the driver. –driver-class-path Extra class path entries to pass to the driver. Note that jars added with –jars are automatically included in the classpath. –executor-memory MEM Executor的内存 (Default: 1G). –proxy-user NAME User to impersonate when submitting the application. This argument does not work with –principal &#x2F; –keytab. –help, -h 显示帮助文件 –verbose, -v Print additional debug output. –version, 打印版本 Cluster deploy mode only(集群模式专属): –driver-cores NUM Driver可用的的CPU核数(Default: 1). Spark standalone or Mesos with cluster deploy mode only: –supervise 如果给定, 可以尝试重启Driver Spark standalone, Mesos or K8s with cluster deploy mode only: –kill SUBMISSION_ID 指定程序ID kill –status SUBMISSION_ID 指定程序ID 查看运行状态 Spark standalone, Mesos and Kubernetes only: –total-executor-cores NUM 整个任务可以给Executor多少个CPU核心用 Spark standalone, YARN and Kubernetes only: –executor-cores NUM 单个Executor能使用多少CPU核心 Spark on YARN and Kubernetes only(YARN模式下): –num-executors NUM Executor应该开启几个 –principal PRINCIPAL Principal to be used to login to KDC. –keytab KEYTAB The full path to the file that contains the keytab for the principal specified above. Spark on YARN only: –queue QUEUE_NAME 指定运行的YARN队列(Default: “default”) 启动 YARN 的历史服务器 cd &#x2F;export&#x2F;server&#x2F;hadoop-3.3.0&#x2F;sbin .&#x2F;mr-jobhistory-daemon.sh start historyserver 访问WebUI界面 http://master:19888/ client 模式测试 SPARK_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;spark ${SPARK_HOME}&#x2F;bin&#x2F;spark-submit –master yarn –deploy-mode client – driver-memory 512m –executor-memory 512m –num-executors 1 –total- executor-cores 2 ${SPARK_HOME}&#x2F;examples&#x2F;src&#x2F;main&#x2F;python&#x2F;pi.py cluster 模式测试 SPARK_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;spark ${SPARK_HOME}&#x2F;bin&#x2F;spark-submit –master yarn –deploy-mode cluster –driver- memory 512m –executor-memory 512m –num-executors 1 –total-executor-cores 2 –conf “spark.pyspark.driver.python&#x3D;&#x2F;root&#x2F;anaconda3&#x2F;bin&#x2F;python3” –conf “spark.pyspark.python&#x3D;&#x2F;root&#x2F;anaconda3&#x2F;bin&#x2F;python3” ${SPARK_HOME}&#x2F;examples&#x2F;src&#x2F;main&#x2F;python&#x2F;pi.py 3**","categories":[],"tags":[]}],"categories":[],"tags":[{"name":"Kafka-eagle","slug":"Kafka-eagle","permalink":"http://example.com/tags/Kafka-eagle/"}]}